{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411e98e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elifyilmaz\\onedrive - enpal b.v\\desktop\\new folder\\project\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Restart with claude\n",
    "%pip install --upgrade scikit-learn imbalanced-learn\n",
    "%pip install seaborn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d82e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad00a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb7cf5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18004 entries, 0 to 18003\n",
      "Data columns (total 60 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       18004 non-null  int64  \n",
      " 1   LEAD_CREATION_DATE       18004 non-null  object \n",
      " 2   SC1_SCHEDULED            11736 non-null  object \n",
      " 3   SC1_APPOINTMENT          11736 non-null  object \n",
      " 4   LAST_FU                  10836 non-null  object \n",
      " 5   NEXT_FU                  2425 non-null   object \n",
      " 6   CONTRACT_SIGNED          680 non-null    object \n",
      " 7   WITHDRAWAL               130 non-null    object \n",
      " 8   IPA_SCHEDULED            583 non-null    object \n",
      " 9   IPA_DAY                  510 non-null    object \n",
      " 10  MOVED2OPS                423 non-null    object \n",
      " 11  NET_TBK                  248 non-null    object \n",
      " 12  POST_IPA_CALL            509 non-null    object \n",
      " 13  SELF_IPA_PV              0 non-null      float64\n",
      " 14  SELF_IPA_HP              0 non-null      float64\n",
      " 15  PRE_ACCEPTANCE_LOAN      2839 non-null   object \n",
      " 16  INSTALL_SCHEDULED        324 non-null    object \n",
      " 17  INSTALL_COMPLETED        213 non-null    object \n",
      " 18  BANKABLE                 228 non-null    object \n",
      " 19  SUBJECTID                18000 non-null  float64\n",
      " 20  BC_REACHED               16996 non-null  object \n",
      " 21  BC_1                     17067 non-null  object \n",
      " 22  BC_2                     9849 non-null   object \n",
      " 23  BC_3                     6041 non-null   object \n",
      " 24  BC_4                     3750 non-null   object \n",
      " 25  BC_5                     2364 non-null   object \n",
      " 26  BC_6                     1547 non-null   object \n",
      " 27  BC_7                     959 non-null    object \n",
      " 28  BC_8                     588 non-null    object \n",
      " 29  BC_1_OUTCOME             17067 non-null  object \n",
      " 30  BC_2_OUTCOME             9849 non-null   object \n",
      " 31  BC_3_OUTCOME             6041 non-null   object \n",
      " 32  BC_4_OUTCOME             3750 non-null   object \n",
      " 33  BC_5_OUTCOME             2364 non-null   object \n",
      " 34  BC_6_OUTCOME             1547 non-null   object \n",
      " 35  BC_7_OUTCOME             959 non-null    object \n",
      " 36  BC_8_OUTCOME             588 non-null    object \n",
      " 37  BC_9_OUTCOME             380 non-null    object \n",
      " 38  BC_10_OUTCOME            261 non-null    object \n",
      " 39  BC_11_OUTCOME            157 non-null    object \n",
      " 40  BC_12_OUTCOME            101 non-null    object \n",
      " 41  BC_12                    101 non-null    object \n",
      " 42  BC_11                    157 non-null    object \n",
      " 43  BC_10                    261 non-null    object \n",
      " 44  BC_9                     380 non-null    object \n",
      " 45  LAST_OUTCOME             17067 non-null  object \n",
      " 46  LAST_UNQUALIFIED_REASON  62 non-null     object \n",
      " 47  LEAD_STATUS              17067 non-null  object \n",
      " 48  BC_13                    73 non-null     object \n",
      " 49  BC_14                    47 non-null     object \n",
      " 50  BC_15                    40 non-null     object \n",
      " 51  BC_16                    29 non-null     object \n",
      " 52  BC_17                    26 non-null     object \n",
      " 53  BC_18                    19 non-null     object \n",
      " 54  BC_13_OUTCOME            73 non-null     object \n",
      " 55  BC_14_OUTCOME            47 non-null     object \n",
      " 56  BC_15_OUTCOME            40 non-null     object \n",
      " 57  BC_16_OUTCOME            29 non-null     object \n",
      " 58  BC_17_OUTCOME            26 non-null     object \n",
      " 59  BC_18_OUTCOME            19 non-null     object \n",
      "dtypes: float64(3), int64(1), object(56)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# all after march --> nb here we dont take all 100k+ columns because we do calcuations on them prior to selecting. so this way we select prior\n",
    "file_path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\lead_time_after_march.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e6a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2108 entries, 0 to 2107\n",
      "Data columns (total 62 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       2108 non-null   int64  \n",
      " 1   LEAD_CREATION_DATE       2108 non-null   object \n",
      " 2   SC1_SCHEDULED            1270 non-null   object \n",
      " 3   SC1_APPOINTMENT          1270 non-null   object \n",
      " 4   LAST_FU                  1289 non-null   object \n",
      " 5   NEXT_FU                  28 non-null     object \n",
      " 6   CONTRACT_SIGNED          2108 non-null   object \n",
      " 7   WITHDRAWAL               755 non-null    object \n",
      " 8   IPA_SCHEDULED            1642 non-null   object \n",
      " 9   IPA_DAY                  1540 non-null   object \n",
      " 10  MOVED2OPS                1273 non-null   object \n",
      " 11  NET_TBK                  182 non-null    object \n",
      " 12  POST_IPA_CALL            1486 non-null   object \n",
      " 13  SELF_IPA_PV              0 non-null      float64\n",
      " 14  SELF_IPA_HP              0 non-null      float64\n",
      " 15  PRE_ACCEPTANCE_LOAN      199 non-null    object \n",
      " 16  INSTALL_SCHEDULED        1137 non-null   object \n",
      " 17  INSTALL_COMPLETED        1036 non-null   object \n",
      " 18  BANKABLE                 1043 non-null   object \n",
      " 19  SUBJECTID                2108 non-null   int64  \n",
      " 20  BC_REACHED               1413 non-null   object \n",
      " 21  BC_1                     1482 non-null   object \n",
      " 22  BC_2                     859 non-null    object \n",
      " 23  BC_3                     533 non-null    object \n",
      " 24  BC_4                     343 non-null    object \n",
      " 25  BC_5                     213 non-null    object \n",
      " 26  BC_6                     136 non-null    object \n",
      " 27  BC_7                     87 non-null     object \n",
      " 28  BC_8                     56 non-null     object \n",
      " 29  BC_1_OUTCOME             1482 non-null   object \n",
      " 30  BC_2_OUTCOME             859 non-null    object \n",
      " 31  BC_3_OUTCOME             533 non-null    object \n",
      " 32  BC_4_OUTCOME             343 non-null    object \n",
      " 33  BC_5_OUTCOME             213 non-null    object \n",
      " 34  BC_6_OUTCOME             136 non-null    object \n",
      " 35  BC_7_OUTCOME             87 non-null     object \n",
      " 36  BC_8_OUTCOME             56 non-null     object \n",
      " 37  BC_9_OUTCOME             29 non-null     object \n",
      " 38  BC_10_OUTCOME            18 non-null     object \n",
      " 39  BC_11_OUTCOME            11 non-null     object \n",
      " 40  BC_12_OUTCOME            9 non-null      object \n",
      " 41  BC_12                    9 non-null      object \n",
      " 42  BC_11                    11 non-null     object \n",
      " 43  BC_10                    18 non-null     object \n",
      " 44  BC_9                     29 non-null     object \n",
      " 45  LAST_OUTCOME             1482 non-null   object \n",
      " 46  LAST_UNQUALIFIED_REASON  22 non-null     object \n",
      " 47  LEAD_STATUS              1482 non-null   object \n",
      " 48  BC_13                    6 non-null      object \n",
      " 49  BC_14                    5 non-null      object \n",
      " 50  BC_15                    2 non-null      object \n",
      " 51  BC_16                    2 non-null      object \n",
      " 52  BC_17                    2 non-null      object \n",
      " 53  BC_18                    1 non-null      object \n",
      " 54  BC_13_OUTCOME            6 non-null      object \n",
      " 55  BC_14_OUTCOME            5 non-null      object \n",
      " 56  BC_15_OUTCOME            2 non-null      object \n",
      " 57  BC_16_OUTCOME            2 non-null      object \n",
      " 58  BC_17_OUTCOME            2 non-null      object \n",
      " 59  BC_18_OUTCOME            1 non-null      object \n",
      " 60  REQUESTID                2108 non-null   int64  \n",
      " 61  LEADID                   2108 non-null   int64  \n",
      "dtypes: float64(2), int64(4), object(56)\n",
      "memory usage: 1021.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# positives also before march\n",
    "path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\lead_time_older_contracts.csv\"\n",
    "df2 = pd.read_csv(path)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c47e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "older = df2[\"REQUESTID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f398ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "older = list(set(older))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ab56d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2108"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(older)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c96395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Found 102 overlapping IDs:\n",
      "{137733, 29702, 145416, 140300, 144909, 144911, 143892, 144933, 138289, 141874, 145470, 140353, 137796, 135752, 62538, 146002, 138843, 143459, 138852, 88165, 145003, 146547, 143477, 142455, 139387, 141437, 139913, 142987, 138897, 139417, 144542, 146595, 143526, 140463, 141999, 145591, 143544, 140478, 146624, 139457, 139980, 138960, 146134, 127194, 146146, 144620, 142061, 139507, 139004, 145665, 141575, 139015, 138506, 140557, 135438, 140559, 139027, 145180, 138535, 138547, 142135, 142647, 146246, 146247, 140105, 144717, 141137, 144211, 138593, 142183, 138600, 144747, 139629, 140655, 146288, 145778, 140148, 146817, 138628, 146824, 145291, 141203, 143258, 144802, 145836, 138160, 138162, 142775, 139193, 142779, 142281, 141774, 143321, 139738, 141275, 139749, 137710, 144884, 145396, 142327, 144889, 144892}\n"
     ]
    }
   ],
   "source": [
    "# Check for overlapping IDs\n",
    "overlapping_ids = set(df['ID']).intersection(set(df2['ID']))\n",
    "\n",
    "if len(overlapping_ids) > 0:\n",
    "    print(f\"WARNING: Found {len(overlapping_ids)} overlapping IDs:\")\n",
    "    print(overlapping_ids)\n",
    "else:\n",
    "    print(\"No overlapping IDs found - safe to concatenate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc04d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Option 2 - Keep first (df priority):\n",
      "Combined shape: (19981, 62)\n"
     ]
    }
   ],
   "source": [
    "# since theyre the same table overlapping is ok ill just drop duplicate ids \n",
    "combined_df_first = pd.concat([df, df2], ignore_index=True).drop_duplicates(subset=['ID'], keep='first')\n",
    "print(f\"\\nOption 2 - Keep first (df priority):\")\n",
    "print(f\"Combined shape: {combined_df_first.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a2a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost 20k entries\n",
    "df = combined_df_first.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5ed915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def engineer_lead_conversion_features(df):\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering for lead conversion prediction\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with lead data following the specified structure\n",
    "        \n",
    "    Returns:\n",
    "        df_features: DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['LEAD_CREATION_DATE', 'SC1_SCHEDULED', 'SC1_APPOINTMENT', \n",
    "                   'LAST_FU', 'NEXT_FU', 'CONTRACT_SIGNED', 'WITHDRAWAL', \n",
    "                   'IPA_SCHEDULED', 'IPA_DAY']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = pd.to_datetime(df_features[col], errors='coerce')\n",
    "    \n",
    "    # BC date columns\n",
    "    bc_date_cols = [f'BC_{i}' for i in range(1, 19)]\n",
    "    for col in bc_date_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = pd.to_datetime(df_features[col], errors='coerce')\n",
    "    \n",
    "    # BC outcome columns\n",
    "    bc_outcome_cols = [f'BC_{i}_OUTCOME' for i in range(1, 19)]\n",
    "    \n",
    "    print(\"Creating booking call efficiency features...\")\n",
    "    \n",
    "    # 1. BOOKING CALL EFFICIENCY FEATURES\n",
    "    # Count total BC attempts (non-null BC dates)\n",
    "    bc_dates = df_features[bc_date_cols].copy()\n",
    "    df_features['total_bc_attempts'] = bc_dates.notna().sum(axis=1)\n",
    "    \n",
    "    # Count total BC outcomes (non-null outcomes)\n",
    "    bc_outcomes = df_features[bc_outcome_cols].copy()\n",
    "    df_features['total_bc_outcomes'] = bc_outcomes.notna().sum(axis=1)\n",
    "    \n",
    "    # First and last BC attempt dates\n",
    "    df_features['first_bc_date'] = bc_dates.min(axis=1)\n",
    "    df_features['last_bc_date'] = bc_dates.max(axis=1)\n",
    "    \n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # 2. TEMPORAL FEATURES\n",
    "    # Days from lead creation to first BC\n",
    "    df_features['lead_to_first_bc_days'] = (\n",
    "        df_features['first_bc_date'] - df_features['LEAD_CREATION_DATE']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Days from first BC to last BC (booking call duration)\n",
    "    df_features['bc_duration_days'] = (\n",
    "        df_features['last_bc_date'] - df_features['first_bc_date']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Days from lead creation to SC1 scheduled\n",
    "    df_features['lead_to_sc1_days'] = (\n",
    "        df_features['SC1_SCHEDULED'] - df_features['LEAD_CREATION_DATE']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Days from SC1 scheduled to appointment\n",
    "    df_features['sc1_schedule_to_appointment_days'] = (\n",
    "        df_features['SC1_APPOINTMENT'] - df_features['SC1_SCHEDULED']\n",
    "    ).dt.days\n",
    "    \n",
    "    # BC call frequency (attempts per day during BC period)\n",
    "    df_features['bc_frequency'] = np.where(\n",
    "        df_features['bc_duration_days'] > 0,\n",
    "        df_features['total_bc_attempts'] / (df_features['bc_duration_days'] + 1),\n",
    "        df_features['total_bc_attempts']  # If same day, frequency = total attempts\n",
    "    )\n",
    "    \n",
    "    print(\"Creating engagement quality features...\")\n",
    "    \n",
    "    # 3. ENGAGEMENT QUALITY FEATURES\n",
    "    # Define outcome categories\n",
    "    positive_outcomes = ['VDTOPLAN', 'REACHED', 'CONFIRMED', 'INTERESTED']\n",
    "    negative_outcomes = ['LOST', 'NOTINTERESTED', 'UNQUALIFIED']\n",
    "    neutral_outcomes = ['TOBERECALLED', 'RESCHEDULE']\n",
    "    unreachable_outcomes = ['NOTREACHED', 'NOANSWER', 'VOICEMAIL']\n",
    "    noshow_outcomes = ['NOSHOW', 'NOSHOWNOTREACHED', 'NOSHOWLOST', 'NOSHOWVDTOPLAN', 'NOSHOWTOBERECALLED']\n",
    "    \n",
    "    # Count outcomes by category\n",
    "    def count_outcome_category(row, category_list):\n",
    "        count = 0\n",
    "        for col in bc_outcome_cols:\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                if any(cat.upper() in str(row[col]).upper() for cat in category_list):\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    df_features['positive_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, positive_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    df_features['negative_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, negative_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    df_features['noshow_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, noshow_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    df_features['unreachable_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, unreachable_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate ratios (avoid division by zero)\n",
    "    total_outcomes = df_features['total_bc_outcomes']\n",
    "    df_features['positive_outcome_ratio'] = np.where(\n",
    "        total_outcomes > 0, \n",
    "        df_features['positive_outcomes_count'] / total_outcomes, \n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['negative_outcome_ratio'] = np.where(\n",
    "        total_outcomes > 0,\n",
    "        df_features['negative_outcomes_count'] / total_outcomes,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['noshow_outcome_ratio'] = np.where(\n",
    "        total_outcomes > 0,\n",
    "        df_features['noshow_outcomes_count'] / total_outcomes,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['reachability_score'] = np.where(\n",
    "        total_outcomes > 0,\n",
    "        1 - (df_features['unreachable_outcomes_count'] / total_outcomes),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    print(\"Creating behavioral pattern features...\")\n",
    "    \n",
    "    # 4. BEHAVIORAL PATTERN FEATURES\n",
    "    # Last BC outcome (most recent attempt result)\n",
    "    def get_last_bc_outcome(row):\n",
    "        for i in range(18, 0, -1):  # Check from BC_18 down to BC_1\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                return row[col]\n",
    "        return 'NO_OUTCOME'\n",
    "    \n",
    "    df_features['last_bc_outcome'] = df_features.apply(get_last_bc_outcome, axis=1)\n",
    "    \n",
    "    # First BC outcome\n",
    "    def get_first_bc_outcome(row):\n",
    "        for i in range(1, 19):  # Check from BC_1 to BC_18\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                return row[col]\n",
    "        return 'NO_OUTCOME'\n",
    "    \n",
    "    df_features['first_bc_outcome'] = df_features.apply(get_first_bc_outcome, axis=1)\n",
    "    \n",
    "    # Outcome improvement/deterioration pattern\n",
    "    def outcome_trend(row):\n",
    "        outcomes = []\n",
    "        for i in range(1, 19):\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                # Score outcomes (higher = better)\n",
    "                if any(pos.upper() in str(row[col]).upper() for pos in positive_outcomes):\n",
    "                    outcomes.append(3)\n",
    "                elif any(neu.upper() in str(row[col]).upper() for neu in neutral_outcomes):\n",
    "                    outcomes.append(2)\n",
    "                elif any(unr.upper() in str(row[col]).upper() for unr in unreachable_outcomes):\n",
    "                    outcomes.append(1)\n",
    "                else:\n",
    "                    outcomes.append(0)  # negative outcomes\n",
    "        \n",
    "        if len(outcomes) < 2:\n",
    "            return 0  # No trend\n",
    "        \n",
    "        # Calculate trend (positive = improving, negative = deteriorating)\n",
    "        return outcomes[-1] - outcomes[0]\n",
    "    \n",
    "    df_features['outcome_trend'] = df_features.apply(outcome_trend, axis=1)\n",
    "    \n",
    "    # Persistence score (attempts after first negative outcome)\n",
    "    def persistence_score(row):\n",
    "        first_negative_pos = None\n",
    "        total_after_negative = 0\n",
    "        \n",
    "        for i in range(1, 19):\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                if first_negative_pos is None and any(neg.upper() in str(row[col]).upper() for neg in negative_outcomes):\n",
    "                    first_negative_pos = i\n",
    "                elif first_negative_pos is not None:\n",
    "                    total_after_negative += 1\n",
    "        \n",
    "        return total_after_negative\n",
    "    \n",
    "    df_features['persistence_after_negative'] = df_features.apply(persistence_score, axis=1)\n",
    "    \n",
    "    print(\"Creating conversion-related features...\")\n",
    "    \n",
    "    # 5. CONVERSION-RELATED FEATURES\n",
    "    # Binary target\n",
    "    df_features['converted'] = df_features['CONTRACT_SIGNED'].notna().astype(int)\n",
    "    \n",
    " \n",
    "    # Show-up for SC1 appointment\n",
    "    df_features['showed_up_sc1'] = df_features['SC1_APPOINTMENT'].notna().astype(int)\n",
    "    \n",
    "    print(\"Creating composite scores...\")\n",
    "    \n",
    "    # 6. COMPOSITE SCORES\n",
    "    # Overall engagement score (weighted combination)\n",
    "    df_features['engagement_score'] = (\n",
    "        0.3 * df_features['positive_outcome_ratio'] + \n",
    "        0.2 * df_features['reachability_score'] + \n",
    "        0.2 * (1 - df_features['negative_outcome_ratio']) +\n",
    "        0.1 * (1 - df_features['noshow_outcome_ratio']) +\n",
    "        0.1 * np.clip(df_features['bc_frequency'], 0, 5) / 5 +  # Cap frequency at 5\n",
    "        0.1 * np.clip(df_features['outcome_trend'], -3, 3) / 6  # Normalize trend\n",
    "    )\n",
    "    \n",
    "    # Efficiency score (quick to schedule with few attempts)\n",
    "    df_features['efficiency_score'] = np.where(\n",
    "        df_features['total_bc_attempts'] > 0,\n",
    "        1 / (1 + df_features['total_bc_attempts']),  # Inverse of attempts\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    print(\"Cleaning up features...\")\n",
    "    \n",
    "    # Fill NaN values in temporal features\n",
    "    temporal_features = [\n",
    "        'lead_to_first_bc_days', 'bc_duration_days', 'lead_to_sc1_days',\n",
    "        'sc1_schedule_to_appointment_days', 'bc_frequency'\n",
    "    ]\n",
    "    \n",
    "    for feature in temporal_features:\n",
    "        if feature in df_features.columns:\n",
    "            df_features[feature] = df_features[feature].fillna(0)\n",
    "    \n",
    "    print(\"Feature engineering completed!\")\n",
    "    \n",
    "    # Return only the engineered features (remove original columns for cleaner output)\n",
    "    feature_columns = [\n",
    "        'ID', 'converted',  # Keep ID and target\n",
    "        # Booking call efficiency\n",
    "        'total_bc_attempts', 'total_bc_outcomes',\n",
    "        # Temporal features\n",
    "        'lead_to_first_bc_days', 'bc_duration_days', 'lead_to_sc1_days',\n",
    "        'sc1_schedule_to_appointment_days', 'bc_frequency',\n",
    "        # Engagement quality\n",
    "        'positive_outcomes_count', 'negative_outcomes_count', 'noshow_outcomes_count',\n",
    "        'positive_outcome_ratio', 'negative_outcome_ratio', 'noshow_outcome_ratio', 'reachability_score',\n",
    "        # Behavioral patterns\n",
    "        'last_bc_outcome', 'first_bc_outcome', 'outcome_trend', 'persistence_after_negative',\n",
    "        # Conversion related\n",
    "        'showed_up_sc1',\n",
    "        # Composite scores\n",
    "        'engagement_score', 'efficiency_score'\n",
    "    ]\n",
    "    \n",
    "    return df_features[feature_columns]\n",
    "\n",
    "# Example usage:\n",
    "# df_features = engineer_lead_conversion_features(df_raw)\n",
    "# print(f\"Created {df_features.shape[1]} features for {df_features.shape[0]} leads\")\n",
    "# print(f\"Conversion rate: {df_features['converted'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2276dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating booking call efficiency features...\n",
      "Creating temporal features...\n",
      "Creating engagement quality features...\n",
      "Creating behavioral pattern features...\n",
      "Creating conversion-related features...\n",
      "Creating composite scores...\n",
      "Cleaning up features...\n",
      "Feature engineering completed!\n",
      "Created 23 features for 19981 leads\n",
      "Conversion rate: 0.133\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df_features = engineer_lead_conversion_features(df)\n",
    "print(f\"Created {df_features.shape[1]} features for {df_features.shape[0]} leads\")\n",
    "print(f\"Conversion rate: {df_features['converted'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce9c018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (19981, 23)\n",
      "Conversion rate: 0.1330\n",
      "\n",
      "Feature columns:\n",
      "['ID', 'converted', 'total_bc_attempts', 'total_bc_outcomes', 'lead_to_first_bc_days', 'bc_duration_days', 'lead_to_sc1_days', 'sc1_schedule_to_appointment_days', 'bc_frequency', 'positive_outcomes_count', 'negative_outcomes_count', 'noshow_outcomes_count', 'positive_outcome_ratio', 'negative_outcome_ratio', 'noshow_outcome_ratio', 'reachability_score', 'last_bc_outcome', 'first_bc_outcome', 'outcome_trend', 'persistence_after_negative', 'showed_up_sc1', 'engagement_score', 'efficiency_score']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic info about what we created\n",
    "print(f\"Shape: {df_features.shape}\")\n",
    "print(f\"Conversion rate: {df_features['converted'].mean():.4f}\")\n",
    "print(\"\\nFeature columns:\")\n",
    "print(df_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "217088e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic statistics for key features:\n",
      "       total_bc_attempts  total_bc_outcomes  lead_to_sc1_days  \\\n",
      "count       19981.000000       19981.000000      19981.000000   \n",
      "mean            2.341424           2.341424          9.332316   \n",
      "std             2.144654           2.144654         40.246629   \n",
      "min             0.000000           0.000000       -133.000000   \n",
      "25%             1.000000           1.000000          0.000000   \n",
      "50%             2.000000           2.000000          0.000000   \n",
      "75%             3.000000           3.000000          1.000000   \n",
      "max            18.000000          18.000000        615.000000   \n",
      "\n",
      "       engagement_score  \n",
      "count      19981.000000  \n",
      "mean           0.708924  \n",
      "std            0.140242  \n",
      "min            0.300000  \n",
      "25%            0.673333  \n",
      "50%            0.740000  \n",
      "75%            0.820000  \n",
      "max            0.900000  \n",
      "\n",
      "Conversions: 2657 out of 19981\n"
     ]
    }
   ],
   "source": [
    "# Look at basic stats for a few key features\n",
    "key_features = ['total_bc_attempts', 'total_bc_outcomes', 'lead_to_sc1_days', 'engagement_score']\n",
    "\n",
    "print(\"Basic statistics for key features:\")\n",
    "print(df_features[key_features].describe())\n",
    "\n",
    "print(f\"\\nConversions: {df_features['converted'].sum()} out of {len(df_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5583f5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converters vs Non-converters:\n",
      "           total_bc_attempts  engagement_score  lead_to_sc1_days\n",
      "converted                                                       \n",
      "0                   2.414165          0.721576          4.730028\n",
      "1                   1.867143          0.626433         39.339857\n"
     ]
    }
   ],
   "source": [
    "# Compare converters vs non-converters on a few key features\n",
    "print(\"Converters vs Non-converters:\")\n",
    "comparison = df_features.groupby('converted')[['total_bc_attempts', 'engagement_score', 'lead_to_sc1_days']].mean()\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ca21355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Features with extreme values to check:\n",
      "Total BC attempts > 15: 31\n",
      "Lead to SC1 days > 30: 1181\n",
      "Engagement score = 0.3 (minimum): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values and extreme outliers\n",
    "print(\"Missing values:\")\n",
    "print(df_features.isnull().sum()[df_features.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nFeatures with extreme values to check:\")\n",
    "print(\"Total BC attempts > 15:\", (df_features['total_bc_attempts'] > 15).sum())\n",
    "print(\"Lead to SC1 days > 30:\", (df_features['lead_to_sc1_days'] > 30).sum())\n",
    "print(\"Engagement score = 0.3 (minimum):\", (df_features['engagement_score'] == 0.3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "359d7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last BC outcome categories:\n",
      "last_bc_outcome\n",
      "VDTOPLAN               16915\n",
      "NO_OUTCOME              1553\n",
      "NOTREACHED               697\n",
      "PLANNERTOASSIGNNOVD      315\n",
      "NOTINTERESTED            184\n",
      "LOST                     145\n",
      "TOBERECALLED              94\n",
      "NOMANAGINGAFTERCALL       78\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique outcomes: 8\n"
     ]
    }
   ],
   "source": [
    "# Check the categorical feature\n",
    "print(\"Last BC outcome categories:\")\n",
    "print(df_features['last_bc_outcome'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nTotal unique outcomes: {df_features['last_bc_outcome'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b667839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding mapping:\n",
      "0: LOST\n",
      "1: NOMANAGINGAFTERCALL\n",
      "2: NOTINTERESTED\n",
      "3: NOTREACHED\n",
      "4: NO_OUTCOME\n",
      "5: PLANNERTOASSIGNNOVD\n",
      "6: TOBERECALLED\n",
      "7: VDTOPLAN\n",
      "\n",
      "Final dataset shape: (19981, 23)\n",
      "Columns: ['ID', 'converted', 'total_bc_attempts', 'total_bc_outcomes', 'lead_to_first_bc_days', 'bc_duration_days', 'lead_to_sc1_days', 'sc1_schedule_to_appointment_days', 'bc_frequency', 'positive_outcomes_count', 'negative_outcomes_count', 'noshow_outcomes_count', 'positive_outcome_ratio', 'negative_outcome_ratio', 'noshow_outcome_ratio', 'reachability_score', 'first_bc_outcome', 'outcome_trend', 'persistence_after_negative', 'showed_up_sc1', 'engagement_score', 'efficiency_score', 'last_bc_outcome_encoded']\n"
     ]
    }
   ],
   "source": [
    "# Simple approach - let's use label encoding for now\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy to preserve original\n",
    "df_model = df_features.copy()\n",
    "\n",
    "# Label encode the categorical feature\n",
    "le = LabelEncoder()\n",
    "df_model['last_bc_outcome_encoded'] = le.fit_transform(df_model['last_bc_outcome'])\n",
    "\n",
    "# Drop the original categorical column\n",
    "df_model = df_model.drop('last_bc_outcome', axis=1)\n",
    "\n",
    "print(\"Label encoding mapping:\")\n",
    "for i, category in enumerate(le.classes_):\n",
    "    print(f\"{i}: {category}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_model.shape}\")\n",
    "print(\"Columns:\", df_model.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa965167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First BC outcome encoding mapping:\n",
      "0: 1\n",
      "1: LOST\n",
      "2: NEW\n",
      "3: NOMANAGINGAFTERCALL\n",
      "4: NOTINTERESTED\n",
      "5: NOTREACHED\n",
      "6: NO_OUTCOME\n",
      "7: PLANNERTOASSIGNNOVD\n",
      "8: TOBERECALLED\n",
      "9: VDTOPLAN\n",
      "\n",
      "Final dataset shape: (19981, 23)\n"
     ]
    }
   ],
   "source": [
    "# Encode first_bc_outcome\n",
    "le_first = LabelEncoder()\n",
    "df_model['first_bc_outcome_encoded'] = le_first.fit_transform(df_model['first_bc_outcome'])\n",
    "\n",
    "# Drop the original categorical column\n",
    "df_model = df_model.drop('first_bc_outcome', axis=1)\n",
    "\n",
    "print(\"First BC outcome encoding mapping:\")\n",
    "for i, category in enumerate(le_first.classes_):\n",
    "    print(f\"{i}: {category}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b72df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19981 entries, 0 to 20111\n",
      "Data columns (total 23 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   ID                                19981 non-null  int64  \n",
      " 1   converted                         19981 non-null  int64  \n",
      " 2   total_bc_attempts                 19981 non-null  int64  \n",
      " 3   total_bc_outcomes                 19981 non-null  int64  \n",
      " 4   lead_to_first_bc_days             19981 non-null  float64\n",
      " 5   bc_duration_days                  19981 non-null  float64\n",
      " 6   lead_to_sc1_days                  19981 non-null  float64\n",
      " 7   sc1_schedule_to_appointment_days  19981 non-null  float64\n",
      " 8   bc_frequency                      19981 non-null  float64\n",
      " 9   positive_outcomes_count           19981 non-null  int64  \n",
      " 10  negative_outcomes_count           19981 non-null  int64  \n",
      " 11  noshow_outcomes_count             19981 non-null  int64  \n",
      " 12  positive_outcome_ratio            19981 non-null  float64\n",
      " 13  negative_outcome_ratio            19981 non-null  float64\n",
      " 14  noshow_outcome_ratio              19981 non-null  float64\n",
      " 15  reachability_score                19981 non-null  float64\n",
      " 16  outcome_trend                     19981 non-null  int64  \n",
      " 17  persistence_after_negative        19981 non-null  int64  \n",
      " 18  showed_up_sc1                     19981 non-null  int64  \n",
      " 19  engagement_score                  19981 non-null  float64\n",
      " 20  efficiency_score                  19981 non-null  float64\n",
      " 21  last_bc_outcome_encoded           19981 non-null  int64  \n",
      " 22  first_bc_outcome_encoded          19981 non-null  int64  \n",
      "dtypes: float64(11), int64(12)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32209ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\n"
     ]
    }
   ],
   "source": [
    "# save into pre-processed folder\n",
    "import os\n",
    "\n",
    "# Check your current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "df_model.to_csv('processed_data/df_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980b372",
   "metadata": {},
   "source": [
    "### save the results in csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9db652",
   "metadata": {},
   "source": [
    "This basic approach focuses on:\n",
    "1. Class Imbalance Handling (1.2% conversion rate)\n",
    "\n",
    "SMOTE: Synthetic oversampling of minority class\n",
    "SMOTE-Tomek: SMOTE + removes borderline cases\n",
    "Undersampling: Reduces majority class size\n",
    "\n",
    "2. Simple Neural Network\n",
    "\n",
    "4-layer architecture optimized for imbalanced data\n",
    "BatchNormalization and Dropout for regularization\n",
    "Early stopping to prevent overfitting\n",
    "\n",
    "3. Comparison Framework\n",
    "\n",
    "Tests all three imbalance methods\n",
    "Compares ROC-AUC scores\n",
    "Shows classification reports\n",
    "\n",
    "Key Benefits for Your 1.2% Conversion Rate:\n",
    "\n",
    "SMOTE will likely work best - creates synthetic minority samples\n",
    "Model focuses on precision/recall balance, not just accuracy\n",
    "ROC-AUC is the right metric for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27395534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Preparing Data...\n",
      "Features: 22 columns\n",
      "Samples: 19981\n",
      "Conversions: 2657 (13.298%)\n",
      "\n",
      "🚀 Training Final Model with SMOTE\n",
      "============================================================\n",
      "🔧 Handling Class Imbalance...\n",
      "Original distribution: Counter({0: 13859, 1: 2125})\n",
      "Conversion rate: 13.295%\n",
      "Resampled distribution: Counter({0: 13859, 1: 13859})\n",
      "New conversion rate: 50.000%\n",
      "Dataset size change: 15984 → 27718\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m2,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,593</span> (57.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,593\u001b[0m (57.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,209</span> (55.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,209\u001b[0m (55.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7471 - loss: 0.5283 - precision: 0.8011 - recall: 0.6573 - val_accuracy: 0.8704 - val_loss: 0.4493 - val_precision: 0.5102 - val_recall: 0.6598\n",
      "Epoch 2/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7730 - loss: 0.4932 - precision: 0.8422 - recall: 0.6718 - val_accuracy: 0.8609 - val_loss: 0.4380 - val_precision: 0.4847 - val_recall: 0.7143\n",
      "Epoch 3/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7801 - loss: 0.4854 - precision: 0.8438 - recall: 0.6876 - val_accuracy: 0.8656 - val_loss: 0.4646 - val_precision: 0.4967 - val_recall: 0.7049\n",
      "Epoch 4/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7805 - loss: 0.4803 - precision: 0.8456 - recall: 0.6864 - val_accuracy: 0.8747 - val_loss: 0.4448 - val_precision: 0.5217 - val_recall: 0.6992\n",
      "Epoch 5/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7827 - loss: 0.4765 - precision: 0.8466 - recall: 0.6906 - val_accuracy: 0.8584 - val_loss: 0.4304 - val_precision: 0.4785 - val_recall: 0.7124\n",
      "Epoch 6/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.4749 - precision: 0.8411 - recall: 0.7011 - val_accuracy: 0.8664 - val_loss: 0.4425 - val_precision: 0.4987 - val_recall: 0.7143\n",
      "Epoch 7/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.4736 - precision: 0.8449 - recall: 0.6963 - val_accuracy: 0.8604 - val_loss: 0.4549 - val_precision: 0.4835 - val_recall: 0.7180\n",
      "Epoch 8/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7850 - loss: 0.4754 - precision: 0.8429 - recall: 0.7006 - val_accuracy: 0.8549 - val_loss: 0.4451 - val_precision: 0.4705 - val_recall: 0.7199\n",
      "Epoch 9/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7854 - loss: 0.4734 - precision: 0.8404 - recall: 0.7045 - val_accuracy: 0.8569 - val_loss: 0.4208 - val_precision: 0.4755 - val_recall: 0.7293\n",
      "Epoch 10/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7863 - loss: 0.4705 - precision: 0.8421 - recall: 0.7047 - val_accuracy: 0.8569 - val_loss: 0.4408 - val_precision: 0.4756 - val_recall: 0.7331\n",
      "Epoch 11/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7877 - loss: 0.4686 - precision: 0.8434 - recall: 0.7066 - val_accuracy: 0.8501 - val_loss: 0.4194 - val_precision: 0.4602 - val_recall: 0.7274\n",
      "Epoch 12/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7880 - loss: 0.4709 - precision: 0.8410 - recall: 0.7102 - val_accuracy: 0.8551 - val_loss: 0.4157 - val_precision: 0.4712 - val_recall: 0.7237\n",
      "Epoch 13/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7886 - loss: 0.4673 - precision: 0.8419 - recall: 0.7107 - val_accuracy: 0.8614 - val_loss: 0.4239 - val_precision: 0.4862 - val_recall: 0.7293\n",
      "Epoch 14/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7873 - loss: 0.4694 - precision: 0.8397 - recall: 0.7101 - val_accuracy: 0.8536 - val_loss: 0.4442 - val_precision: 0.4680 - val_recall: 0.7293\n",
      "Epoch 15/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7883 - loss: 0.4667 - precision: 0.8420 - recall: 0.7099 - val_accuracy: 0.8529 - val_loss: 0.4130 - val_precision: 0.4661 - val_recall: 0.7237\n",
      "Epoch 16/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7889 - loss: 0.4657 - precision: 0.8395 - recall: 0.7146 - val_accuracy: 0.8606 - val_loss: 0.4321 - val_precision: 0.4841 - val_recall: 0.7162\n",
      "Epoch 17/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7890 - loss: 0.4656 - precision: 0.8441 - recall: 0.7089 - val_accuracy: 0.8591 - val_loss: 0.4111 - val_precision: 0.4806 - val_recall: 0.7237\n",
      "Epoch 18/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7899 - loss: 0.4646 - precision: 0.8402 - recall: 0.7161 - val_accuracy: 0.8509 - val_loss: 0.4261 - val_precision: 0.4617 - val_recall: 0.7256\n",
      "Epoch 19/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7894 - loss: 0.4646 - precision: 0.8380 - recall: 0.7175 - val_accuracy: 0.8566 - val_loss: 0.4366 - val_precision: 0.4748 - val_recall: 0.7274\n",
      "Epoch 20/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7906 - loss: 0.4635 - precision: 0.8412 - recall: 0.7164 - val_accuracy: 0.8476 - val_loss: 0.4668 - val_precision: 0.4556 - val_recall: 0.7425\n",
      "Epoch 21/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7907 - loss: 0.4643 - precision: 0.8400 - recall: 0.7181 - val_accuracy: 0.8644 - val_loss: 0.4188 - val_precision: 0.4936 - val_recall: 0.7218\n",
      "Epoch 22/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7905 - loss: 0.4638 - precision: 0.8404 - recall: 0.7171 - val_accuracy: 0.8526 - val_loss: 0.4462 - val_precision: 0.4659 - val_recall: 0.7331\n",
      "Epoch 23/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7891 - loss: 0.4614 - precision: 0.8376 - recall: 0.7173 - val_accuracy: 0.8677 - val_loss: 0.4238 - val_precision: 0.5020 - val_recall: 0.7237\n",
      "Epoch 24/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7896 - loss: 0.4622 - precision: 0.8376 - recall: 0.7185 - val_accuracy: 0.8531 - val_loss: 0.4458 - val_precision: 0.4667 - val_recall: 0.7256\n",
      "Epoch 25/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7908 - loss: 0.4608 - precision: 0.8427 - recall: 0.7150 - val_accuracy: 0.8506 - val_loss: 0.4457 - val_precision: 0.4612 - val_recall: 0.7256\n",
      "Epoch 26/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7905 - loss: 0.4631 - precision: 0.8406 - recall: 0.7170 - val_accuracy: 0.8556 - val_loss: 0.4307 - val_precision: 0.4725 - val_recall: 0.7256\n",
      "Epoch 27/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7915 - loss: 0.4604 - precision: 0.8409 - recall: 0.7192 - val_accuracy: 0.8629 - val_loss: 0.4493 - val_precision: 0.4899 - val_recall: 0.7312\n",
      "Epoch 28/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7900 - loss: 0.4596 - precision: 0.8391 - recall: 0.7175 - val_accuracy: 0.8639 - val_loss: 0.4353 - val_precision: 0.4923 - val_recall: 0.7237\n",
      "Epoch 29/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7916 - loss: 0.4604 - precision: 0.8418 - recall: 0.7182 - val_accuracy: 0.8561 - val_loss: 0.4411 - val_precision: 0.4736 - val_recall: 0.7237\n",
      "Epoch 30/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7912 - loss: 0.4595 - precision: 0.8387 - recall: 0.7211 - val_accuracy: 0.8666 - val_loss: 0.4125 - val_precision: 0.4993 - val_recall: 0.7124\n",
      "Epoch 31/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7934 - loss: 0.4592 - precision: 0.8401 - recall: 0.7248 - val_accuracy: 0.8659 - val_loss: 0.4207 - val_precision: 0.4974 - val_recall: 0.7143\n",
      "Epoch 32/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7926 - loss: 0.4596 - precision: 0.8395 - recall: 0.7235 - val_accuracy: 0.8591 - val_loss: 0.4176 - val_precision: 0.4806 - val_recall: 0.7237\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "🎯 FINAL RESULTS:\n",
      "ROC-AUC Score: 0.8439\n",
      "Test Set Size: 3997 leads\n",
      "Actual Conversions: 532\n",
      "Predicted Conversions: 801\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.92      3465\n",
      "           1       0.48      0.72      0.58       532\n",
      "\n",
      "    accuracy                           0.86      3997\n",
      "   macro avg       0.72      0.80      0.75      3997\n",
      "weighted avg       0.89      0.86      0.87      3997\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 231\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred_binary))\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Step 4: Plot results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[43mplot_results\u001b[49m(history, y_test, predictions)\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Step 5: Feature importance (optional)\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_feature_importance\u001b[39m(model, feature_names, X_test, y_test, n_top=\u001b[32m10\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_results' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming your dataframe is called 'df' with features and 'converted' target\n",
    "# second chat \n",
    "\n",
    "df = df_model\n",
    "\n",
    "def handle_class_imbalance(X_train, y_train, method='smote'):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using different techniques\n",
    "    \"\"\"\n",
    "    print(\"🔧 Handling Class Imbalance...\")\n",
    "    print(f\"Original distribution: {Counter(y_train)}\")\n",
    "    print(f\"Conversion rate: {y_train.mean():.3%}\")\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # SMOTE - Synthetic Minority Oversampling\n",
    "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "    elif method == 'smote_tomek':\n",
    "        # SMOTE + Tomek links (removes borderline cases)\n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "        \n",
    "    elif method == 'undersampling':\n",
    "        # Random undersampling of majority class\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler(random_state=42, sampling_strategy=0.3)  # 30% minority class\n",
    "        X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "        \n",
    "    print(f\"Resampled distribution: {Counter(y_resampled)}\")\n",
    "    print(f\"New conversion rate: {y_resampled.mean():.3%}\")\n",
    "    print(f\"Dataset size change: {len(X_train)} → {len(X_resampled)}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def prepare_data(df, target_col='converted'):\n",
    "    \"\"\"\n",
    "    Basic data preparation and train/test split\n",
    "    \"\"\"\n",
    "    print(\"📊 Preparing Data...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    print(f\"Features: {X.shape[1]} columns\")\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    print(f\"Conversions: {y.sum()} ({y.mean():.3%})\")\n",
    "    \n",
    "    # Train/test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def compare_imbalance_methods(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Compare different imbalance handling methods\n",
    "    \"\"\"\n",
    "    methods = ['smote', 'smote_tomek', 'undersampling']\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Handle imbalance\n",
    "        X_train_resampled, y_train_resampled = handle_class_imbalance(\n",
    "            X_train, y_train, method=method\n",
    "        )\n",
    "        \n",
    "        # Build simple model\n",
    "        model = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_resampled, y_train_resampled,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=50,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_proba = model.predict(X_test, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[method] = {\n",
    "            'auc': auc_score,\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'predictions': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"ROC-AUC: {auc_score:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    return results\n",
    "\n",
    "def build_neural_network(input_dim, use_class_weights=False, y_train=None):\n",
    "    \"\"\"\n",
    "    Build neural network architecture optimized for imbalanced data\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer with batch normalization\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Hidden layers with decreasing size\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Calculate class weights if needed\n",
    "    if use_class_weights and y_train is not None:\n",
    "        class_weights = {\n",
    "            0: 1.0,\n",
    "            1: (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "        }\n",
    "        print(f\"Class weights: {class_weights}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model, class_weights\n",
    "\n",
    "def train_final_model(X_train, X_test, y_train, y_test, imbalance_method='smote'):\n",
    "    \"\"\"\n",
    "    Train the final model with best performing imbalance handling\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Training Final Model with {imbalance_method.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Handle imbalance\n",
    "    X_train_resampled, y_train_resampled = handle_class_imbalance(\n",
    "        X_train, y_train, method=imbalance_method\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model, class_weights = build_neural_network(\n",
    "        input_dim=X_train.shape[1],\n",
    "        use_class_weights=False  # Already handled by resampling\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            patience=15, \n",
    "            restore_best_weights=True, \n",
    "            monitor='val_loss'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_resampled, y_train_resampled,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# STEP-BY-STEP EXECUTION:\n",
    "\n",
    "# Step 1: Prepare your data (assuming df is your dataframe with 24 features + CONTRACT_SIGNED)\n",
    "X_train, X_test, y_train, y_test, scaler = prepare_data(df)\n",
    "\n",
    "# Step 2: Train model with SMOTE (recommended for 1.2% conversion rate)\n",
    "final_model, history = train_final_model(X_train, X_test, y_train, y_test, 'smote')\n",
    "\n",
    "# Step 3: Make predictions and evaluate\n",
    "predictions = final_model.predict(X_test)\n",
    "y_pred_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\n🎯 FINAL RESULTS:\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, predictions):.4f}\")\n",
    "print(f\"Test Set Size: {len(y_test)} leads\")\n",
    "print(f\"Actual Conversions: {y_test.sum()}\")\n",
    "print(f\"Predicted Conversions: {y_pred_binary.sum()}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "# Step 4: Plot results\n",
    "plot_results(history, y_test, predictions)\n",
    "\n",
    "# Step 5: Feature importance (optional)\n",
    "def get_feature_importance(model, feature_names, X_test, y_test, n_top=10):\n",
    "    \"\"\"\n",
    "    Get feature importance using permutation method\n",
    "    \"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    # Convert to sklearn format for permutation importance\n",
    "    def keras_predict_proba(X):\n",
    "        return model.predict(X, verbose=0).ravel()\n",
    "    \n",
    "    # Calculate baseline score\n",
    "    baseline_score = roc_auc_score(y_test, keras_predict_proba(X_test))\n",
    "    \n",
    "    # Simple feature importance by correlation with target\n",
    "    feature_corr = []\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        corr = np.corrcoef(X_test[:, i], y_test)[0, 1]\n",
    "        feature_corr.append((feature, abs(corr)))\n",
    "    \n",
    "    feature_corr.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n📊 TOP {n_top} MOST IMPORTANT FEATURES:\")\n",
    "    for i, (feature, corr) in enumerate(feature_corr[:n_top]):\n",
    "        print(f\"{i+1:2d}. {feature}: {corr:.4f}\")\n",
    "    \n",
    "    return feature_corr\n",
    "\n",
    "# Get feature importance\n",
    "feature_names = df.drop('converted', axis=1).columns.tolist()\n",
    "feature_importance = get_feature_importance(final_model, feature_names, X_test, y_test)\n",
    "\n",
    "def plot_results(history, y_test, predictions):\n",
    "    \"\"\"\n",
    "    Plot training history and evaluation metrics\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0,0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0,0].set_title('Model Loss')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0,1].set_title('Model Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # ROC Curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    axes[1,0].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "    axes[1,0].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[1,0].set_xlabel('False Positive Rate')\n",
    "    axes[1,0].set_ylabel('True Positive Rate')\n",
    "    axes[1,0].set_title('ROC Curve')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "    axes[1,1].plot(recall, precision)\n",
    "    axes[1,1].set_xlabel('Recall')\n",
    "    axes[1,1].set_ylabel('Precision')\n",
    "    axes[1,1].set_title('Precision-Recall Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Lead Conversion Prediction Pipeline Ready!\")\n",
    "print(\"\\nKey Functions:\")\n",
    "print(\"1. prepare_data() - Data prep and train/test split\")\n",
    "print(\"2. compare_imbalance_methods() - Test SMOTE, SMOTE-Tomek, undersampling\")\n",
    "print(\"3. train_final_model() - Train final model with best method\")\n",
    "print(\"4. plot_results() - Visualize training and evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65975589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CHECKING NaN FEATURES:\n",
      "noshow_outcomes_count unique values: noshow_outcomes_count\n",
      "0    19981\n",
      "Name: count, dtype: int64\n",
      "noshow_outcome_ratio unique values: noshow_outcome_ratio\n",
      "0.0    19981\n",
      "Name: count, dtype: int64\n",
      "noshow_outcomes_count variance: 0.0\n",
      "noshow_outcome_ratio variance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fix the NaN values in feature importance\n",
    "print(\"🔧 CHECKING NaN FEATURES:\")\n",
    "print(f\"noshow_outcomes_count unique values: {df['noshow_outcomes_count'].value_counts()}\")\n",
    "print(f\"noshow_outcome_ratio unique values: {df['noshow_outcome_ratio'].value_counts()}\")\n",
    "\n",
    "# These might be all zeros or have very little variance\n",
    "print(f\"noshow_outcomes_count variance: {df['noshow_outcomes_count'].var()}\")\n",
    "print(f\"noshow_outcome_ratio variance: {df['noshow_outcome_ratio'].var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the threshold analysis we discussed\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,    # ← add this\n",
    "    recall_score,       # ← and this\n",
    "    f1_score            # ← and this\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "print(\"\\n🎯 OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "print(\"Threshold | Precision | Recall | F1 | Leads to Call | Efficiency Gain\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for threshold in thresholds:\n",
    "    y_pred = (predictions > threshold).astype(int)\n",
    "    \n",
    "    if y_pred.sum() > 0:\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        leads_to_call = y_pred.sum()\n",
    "        efficiency = len(y_test) / leads_to_call\n",
    "        \n",
    "        print(f\"   {threshold:.1f}    |   {precision:.3f}   |  {recall:.3f}  | {f1:.3f} |     {leads_to_call:4d}      |     {efficiency:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_new_leads(new_leads_df, model, scaler, threshold=0.4):\n",
    "    \"\"\"\n",
    "    Score new leads and prioritize high-probability ones\n",
    "    \"\"\"\n",
    "    # Remove the useless no-show features\n",
    "    features_to_drop = ['noshow_outcomes_count', 'noshow_outcome_ratio']\n",
    "    X_new = new_leads_df.drop(columns=features_to_drop + ['converted'], errors='ignore')\n",
    "    \n",
    "    # Scale features\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    # Get predictions\n",
    "    probabilities = model.predict(X_new_scaled).ravel()\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = new_leads_df.copy()\n",
    "    results['conversion_probability'] = probabilities\n",
    "    results['high_priority'] = (probabilities >= threshold).astype(int)\n",
    "    results['priority_rank'] = probabilities.rank(method='dense', ascending=False)\n",
    "    \n",
    "    return results.sort_values('conversion_probability', ascending=False)\n",
    "\n",
    "# Example usage:\n",
    "# new_scored_leads = score_new_leads(new_leads_df, final_model, scaler, threshold=0.4)\n",
    "# high_priority_leads = new_scored_leads[new_scored_leads['high_priority'] == 1]\n",
    "# print(f\"High priority leads to call: {len(high_priority_leads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d469bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used with new leads \n",
    "\n",
    "new_scored_leads = score_new_leads(new_leads_df, final_model, scaler, threshold=0.4)\n",
    "high_priority_leads = new_scored_leads[new_scored_leads['high_priority'] == 1]\n",
    "print(f\"High priority leads to call: {len(high_priority_leads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c57dc0",
   "metadata": {},
   "source": [
    "end of model that includes sc\n",
    "\n",
    "now attempt without sc, bcs only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset df\n",
    "# now we will drop info of sc and fu\n",
    "\n",
    "df2 = pd.read_csv(file_path)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea61d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def engineer_lead_conversion_features(df):\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering for lead conversion prediction\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with lead data following the specified structure\n",
    "        \n",
    "    Returns:\n",
    "        df_features: DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['LEAD_CREATION_DATE', \n",
    "                  'CONTRACT_SIGNED']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = pd.to_datetime(df_features[col], errors='coerce')\n",
    "    \n",
    "    # BC date columns\n",
    "    bc_date_cols = [f'BC_{i}' for i in range(1, 19)]\n",
    "    for col in bc_date_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = pd.to_datetime(df_features[col], errors='coerce')\n",
    "    \n",
    "    # BC outcome columns\n",
    "    bc_outcome_cols = [f'BC_{i}_OUTCOME' for i in range(1, 19)]\n",
    "    \n",
    "    print(\"Creating booking call efficiency features...\")\n",
    "    \n",
    "    # 1. BOOKING CALL EFFICIENCY FEATURES\n",
    "    # Count total BC attempts (non-null BC dates)\n",
    "    bc_dates = df_features[bc_date_cols].copy()\n",
    "    df_features['total_bc_attempts'] = bc_dates.notna().sum(axis=1)\n",
    "    \n",
    "    # Count total BC outcomes (non-null outcomes)\n",
    "    bc_outcomes = df_features[bc_outcome_cols].copy()\n",
    "    df_features['total_bc_outcomes'] = bc_outcomes.notna().sum(axis=1)\n",
    "    \n",
    "    # First and last BC attempt dates\n",
    "    df_features['first_bc_date'] = bc_dates.min(axis=1)\n",
    "    df_features['last_bc_date'] = bc_dates.max(axis=1)\n",
    "    \n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # 2. TEMPORAL FEATURES\n",
    "    # Days from lead creation to first BC\n",
    "    df_features['lead_to_first_bc_days'] = (\n",
    "        df_features['first_bc_date'] - df_features['LEAD_CREATION_DATE']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Days from first BC to last BC (booking call duration)\n",
    "    df_features['bc_duration_days'] = (\n",
    "        df_features['last_bc_date'] - df_features['first_bc_date']\n",
    "    ).dt.days\n",
    "    \n",
    "\n",
    "\n",
    "    # BC call frequency (attempts per day during BC period)\n",
    "    df_features['bc_frequency'] = np.where(\n",
    "        df_features['bc_duration_days'] > 0,\n",
    "        df_features['total_bc_attempts'] / (df_features['bc_duration_days'] + 1),\n",
    "        df_features['total_bc_attempts']  # If same day, frequency = total attempts\n",
    "    )\n",
    "    \n",
    "    print(\"Creating engagement quality features...\")\n",
    "    \n",
    "    # 3. ENGAGEMENT QUALITY FEATURES\n",
    "    # Define outcome categories\n",
    "    positive_outcomes = ['VDTOPLAN', 'REACHED', 'CONFIRMED', 'INTERESTED']\n",
    "    negative_outcomes = ['LOST', 'NOTINTERESTED', 'UNQUALIFIED']\n",
    "    neutral_outcomes = ['TOBERECALLED', 'RESCHEDULE']\n",
    "    unreachable_outcomes = ['NOTREACHED', 'NOANSWER', 'VOICEMAIL']\n",
    "    noshow_outcomes = ['NOSHOW', 'NOSHOWNOTREACHED', 'NOSHOWLOST', 'NOSHOWVDTOPLAN', 'NOSHOWTOBERECALLED']\n",
    "    \n",
    "    # Count outcomes by category\n",
    "    def count_outcome_category(row, category_list):\n",
    "        count = 0\n",
    "        for col in bc_outcome_cols:\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                if any(cat.upper() in str(row[col]).upper() for cat in category_list):\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    df_features['positive_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, positive_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    df_features['negative_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, negative_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    df_features['noshow_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, noshow_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    df_features['unreachable_outcomes_count'] = df_features.apply(\n",
    "        lambda row: count_outcome_category(row, unreachable_outcomes), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate ratios (avoid division by zero)\n",
    "    total_outcomes = df_features['total_bc_outcomes']\n",
    "    df_features['positive_outcome_ratio'] = np.where(\n",
    "        total_outcomes > 0, \n",
    "        df_features['positive_outcomes_count'] / total_outcomes, \n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['negative_outcome_ratio'] = np.where(\n",
    "        total_outcomes > 0,\n",
    "        df_features['negative_outcomes_count'] / total_outcomes,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['noshow_outcome_ratio'] = np.where(\n",
    "        total_outcomes > 0,\n",
    "        df_features['noshow_outcomes_count'] / total_outcomes,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    df_features['reachability_score'] = np.where(\n",
    "        total_outcomes > 0,\n",
    "        1 - (df_features['unreachable_outcomes_count'] / total_outcomes),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    print(\"Creating behavioral pattern features...\")\n",
    "    \n",
    "    # 4. BEHAVIORAL PATTERN FEATURES\n",
    "    # Last BC outcome (most recent attempt result)\n",
    "    def get_last_bc_outcome(row):\n",
    "        for i in range(18, 0, -1):  # Check from BC_18 down to BC_1\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                return row[col]\n",
    "        return 'NO_OUTCOME'\n",
    "    \n",
    "    df_features['last_bc_outcome'] = df_features.apply(get_last_bc_outcome, axis=1)\n",
    "    \n",
    "    # First BC outcome\n",
    "    def get_first_bc_outcome(row):\n",
    "        for i in range(1, 19):  # Check from BC_1 to BC_18\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                return row[col]\n",
    "        return 'NO_OUTCOME'\n",
    "    \n",
    "    df_features['first_bc_outcome'] = df_features.apply(get_first_bc_outcome, axis=1)\n",
    "    \n",
    "    # Outcome improvement/deterioration pattern\n",
    "    def outcome_trend(row):\n",
    "        outcomes = []\n",
    "        for i in range(1, 19):\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                # Score outcomes (higher = better)\n",
    "                if any(pos.upper() in str(row[col]).upper() for pos in positive_outcomes):\n",
    "                    outcomes.append(3)\n",
    "                elif any(neu.upper() in str(row[col]).upper() for neu in neutral_outcomes):\n",
    "                    outcomes.append(2)\n",
    "                elif any(unr.upper() in str(row[col]).upper() for unr in unreachable_outcomes):\n",
    "                    outcomes.append(1)\n",
    "                else:\n",
    "                    outcomes.append(0)  # negative outcomes\n",
    "        \n",
    "        if len(outcomes) < 2:\n",
    "            return 0  # No trend\n",
    "        \n",
    "        # Calculate trend (positive = improving, negative = deteriorating)\n",
    "        return outcomes[-1] - outcomes[0]\n",
    "    \n",
    "    df_features['outcome_trend'] = df_features.apply(outcome_trend, axis=1)\n",
    "    \n",
    "    # Persistence score (attempts after first negative outcome)\n",
    "    def persistence_score(row):\n",
    "        first_negative_pos = None\n",
    "        total_after_negative = 0\n",
    "        \n",
    "        for i in range(1, 19):\n",
    "            col = f'BC_{i}_OUTCOME'\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                if first_negative_pos is None and any(neg.upper() in str(row[col]).upper() for neg in negative_outcomes):\n",
    "                    first_negative_pos = i\n",
    "                elif first_negative_pos is not None:\n",
    "                    total_after_negative += 1\n",
    "        \n",
    "        return total_after_negative\n",
    "    \n",
    "    df_features['persistence_after_negative'] = df_features.apply(persistence_score, axis=1)\n",
    "    \n",
    "    print(\"Creating conversion-related features...\")\n",
    "    \n",
    "    # 5. CONVERSION-RELATED FEATURES\n",
    "    # Binary target\n",
    "    df_features['converted'] = df_features['CONTRACT_SIGNED'].notna().astype(int)\n",
    "    \n",
    " \n",
    "  \n",
    "    print(\"Creating composite scores...\")\n",
    "    \n",
    "    # 6. COMPOSITE SCORES\n",
    "    # Overall engagement score (weighted combination)\n",
    "    df_features['engagement_score'] = (\n",
    "        0.3 * df_features['positive_outcome_ratio'] + \n",
    "        0.2 * df_features['reachability_score'] + \n",
    "        0.2 * (1 - df_features['negative_outcome_ratio']) +\n",
    "        0.1 * (1 - df_features['noshow_outcome_ratio']) +\n",
    "        0.1 * np.clip(df_features['bc_frequency'], 0, 5) / 5 +  # Cap frequency at 5\n",
    "        0.1 * np.clip(df_features['outcome_trend'], -3, 3) / 6  # Normalize trend\n",
    "    )\n",
    "    \n",
    "    # Efficiency score (quick to schedule with few attempts)\n",
    "    df_features['efficiency_score'] = np.where(\n",
    "        df_features['total_bc_attempts'] > 0,\n",
    "        1 / (1 + df_features['total_bc_attempts']),  # Inverse of attempts\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    print(\"Cleaning up features...\")\n",
    "    \n",
    "    # Fill NaN values in temporal features\n",
    "    temporal_features = [\n",
    "        'lead_to_first_bc_days', 'bc_duration_days', \n",
    "        'bc_frequency'\n",
    "    ]\n",
    "    \n",
    "    for feature in temporal_features:\n",
    "        if feature in df_features.columns:\n",
    "            df_features[feature] = df_features[feature].fillna(0)\n",
    "    \n",
    "    print(\"Feature engineering completed!\")\n",
    "    \n",
    "    # Return only the engineered features (remove original columns for cleaner output)\n",
    "    feature_columns = [\n",
    "        'ID', 'converted',  # Keep ID and target\n",
    "        # Booking call efficiency\n",
    "        'total_bc_attempts', 'total_bc_outcomes',\n",
    "        # Temporal features\n",
    "        'lead_to_first_bc_days', 'bc_duration_days', \n",
    "        'bc_frequency',\n",
    "        # Engagement quality\n",
    "        'positive_outcomes_count', 'negative_outcomes_count', 'noshow_outcomes_count',\n",
    "        'positive_outcome_ratio', 'negative_outcome_ratio', 'noshow_outcome_ratio', 'reachability_score',\n",
    "        # Behavioral patterns\n",
    "        'last_bc_outcome', 'first_bc_outcome', 'outcome_trend', 'persistence_after_negative',\n",
    "        # Conversion related\n",
    "    \n",
    "        # Composite scores\n",
    "        'engagement_score', 'efficiency_score'\n",
    "    ]\n",
    "    \n",
    "    return df_features[feature_columns]\n",
    "\n",
    "# Example usage:\n",
    "# df_features = engineer_lead_conversion_features(df_raw)\n",
    "# print(f\"Created {df_features.shape[1]} features for {df_features.shape[0]} leads\")\n",
    "# print(f\"Conversion rate: {df_features['converted'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df_features = engineer_lead_conversion_features(df2)\n",
    "print(f\"Created {df_features.shape[1]} features for {df_features.shape[0]} leads\")\n",
    "print(f\"Conversion rate: {df_features['converted'].mean():.3f}\")\n",
    "\n",
    "# Basic info about what we created\n",
    "print(f\"Shape: {df_features.shape}\")\n",
    "print(f\"Conversion rate: {df_features['converted'].mean():.4f}\")\n",
    "print(\"\\nFeature columns:\")\n",
    "print(df_features.columns.tolist())\n",
    "\n",
    "# Look at basic stats for a few key features\n",
    "key_features = ['total_bc_attempts', 'total_bc_outcomes', 'engagement_score']\n",
    "\n",
    "print(\"Basic statistics for key features:\")\n",
    "print(df_features[key_features].describe())\n",
    "\n",
    "print(f\"\\nConversions: {df_features['converted'].sum()} out of {len(df_features)}\")\n",
    "\n",
    "# Compare converters vs non-converters on a few key features\n",
    "print(\"Converters vs Non-converters:\")\n",
    "comparison = df_features.groupby('converted')[['total_bc_attempts', 'engagement_score']].mean()\n",
    "print(comparison)\n",
    "\n",
    "# Check for missing values and extreme outliers\n",
    "print(\"Missing values:\")\n",
    "print(df_features.isnull().sum()[df_features.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nFeatures with extreme values to check:\")\n",
    "print(\"Total BC attempts > 15:\", (df_features['total_bc_attempts'] > 15).sum())\n",
    "print(\"Engagement score = 0.3 (minimum):\", (df_features['engagement_score'] == 0.3).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1997f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the categorical feature\n",
    "print(\"Last BC outcome categories:\")\n",
    "print(df_features['last_bc_outcome'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nTotal unique outcomes: {df_features['last_bc_outcome'].nunique()}\")\n",
    "\n",
    "# Simple approach - let's use label encoding for now\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy to preserve original\n",
    "df_model = df_features.copy()\n",
    "\n",
    "# Label encode the categorical feature\n",
    "le = LabelEncoder()\n",
    "df_model['last_bc_outcome_encoded'] = le.fit_transform(df_model['last_bc_outcome'])\n",
    "\n",
    "# Drop the original categorical column\n",
    "df_model = df_model.drop('last_bc_outcome', axis=1)\n",
    "\n",
    "print(\"Label encoding mapping:\")\n",
    "for i, category in enumerate(le.classes_):\n",
    "    print(f\"{i}: {category}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_model.shape}\")\n",
    "print(\"Columns:\", df_model.columns.tolist())\n",
    "\n",
    "# Encode first_bc_outcome\n",
    "le_first = LabelEncoder()\n",
    "df_model['first_bc_outcome_encoded'] = le_first.fit_transform(df_model['first_bc_outcome'])\n",
    "\n",
    "# Drop the original categorical column\n",
    "df_model = df_model.drop('first_bc_outcome', axis=1)\n",
    "\n",
    "print(\"First BC outcome encoding mapping:\")\n",
    "for i, category in enumerate(le_first.classes_):\n",
    "    print(f\"{i}: {category}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9604af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming your dataframe is called 'df' with features and 'converted' target\n",
    "# second chat \n",
    "\n",
    "df = df_model\n",
    "\n",
    "def handle_class_imbalance(X_train, y_train, method='smote'):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using different techniques\n",
    "    \"\"\"\n",
    "    print(\"🔧 Handling Class Imbalance...\")\n",
    "    print(f\"Original distribution: {Counter(y_train)}\")\n",
    "    print(f\"Conversion rate: {y_train.mean():.3%}\")\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # SMOTE - Synthetic Minority Oversampling\n",
    "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "    elif method == 'smote_tomek':\n",
    "        # SMOTE + Tomek links (removes borderline cases)\n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "        \n",
    "    elif method == 'undersampling':\n",
    "        # Random undersampling of majority class\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler(random_state=42, sampling_strategy=0.3)  # 30% minority class\n",
    "        X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "        \n",
    "    print(f\"Resampled distribution: {Counter(y_resampled)}\")\n",
    "    print(f\"New conversion rate: {y_resampled.mean():.3%}\")\n",
    "    print(f\"Dataset size change: {len(X_train)} → {len(X_resampled)}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def prepare_data(df, target_col='converted'):\n",
    "    \"\"\"\n",
    "    Basic data preparation and train/test split\n",
    "    \"\"\"\n",
    "    print(\"📊 Preparing Data...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    print(f\"Features: {X.shape[1]} columns\")\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    print(f\"Conversions: {y.sum()} ({y.mean():.3%})\")\n",
    "    \n",
    "    # Train/test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def compare_imbalance_methods(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Compare different imbalance handling methods\n",
    "    \"\"\"\n",
    "    methods = ['smote', 'smote_tomek', 'undersampling']\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {method.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Handle imbalance\n",
    "        X_train_resampled, y_train_resampled = handle_class_imbalance(\n",
    "            X_train, y_train, method=method\n",
    "        )\n",
    "        \n",
    "        # Build simple model\n",
    "        model = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_resampled, y_train_resampled,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=50,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_proba = model.predict(X_test, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[method] = {\n",
    "            'auc': auc_score,\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'predictions': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"ROC-AUC: {auc_score:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    return results\n",
    "\n",
    "def build_neural_network(input_dim, use_class_weights=False, y_train=None):\n",
    "    \"\"\"\n",
    "    Build neural network architecture optimized for imbalanced data\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer with batch normalization\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Hidden layers with decreasing size\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Calculate class weights if needed\n",
    "    if use_class_weights and y_train is not None:\n",
    "        class_weights = {\n",
    "            0: 1.0,\n",
    "            1: (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "        }\n",
    "        print(f\"Class weights: {class_weights}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model, class_weights\n",
    "\n",
    "def train_final_model(X_train, X_test, y_train, y_test, imbalance_method='smote'):\n",
    "    \"\"\"\n",
    "    Train the final model with best performing imbalance handling\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Training Final Model with {imbalance_method.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Handle imbalance\n",
    "    X_train_resampled, y_train_resampled = handle_class_imbalance(\n",
    "        X_train, y_train, method=imbalance_method\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model, class_weights = build_neural_network(\n",
    "        input_dim=X_train.shape[1],\n",
    "        use_class_weights=False  # Already handled by resampling\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            patience=15, \n",
    "            restore_best_weights=True, \n",
    "            monitor='val_loss'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_resampled, y_train_resampled,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# STEP-BY-STEP EXECUTION:\n",
    "\n",
    "# Step 1: Prepare your data (assuming df is your dataframe with 24 features + CONTRACT_SIGNED)\n",
    "X_train, X_test, y_train, y_test, scaler = prepare_data(df)\n",
    "\n",
    "# Step 2: Train model with SMOTE (recommended for 1.2% conversion rate)\n",
    "final_model, history = train_final_model(X_train, X_test, y_train, y_test, 'smote')\n",
    "\n",
    "# Step 3: Make predictions and evaluate\n",
    "predictions = final_model.predict(X_test)\n",
    "y_pred_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\n🎯 FINAL RESULTS:\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, predictions):.4f}\")\n",
    "print(f\"Test Set Size: {len(y_test)} leads\")\n",
    "print(f\"Actual Conversions: {y_test.sum()}\")\n",
    "print(f\"Predicted Conversions: {y_pred_binary.sum()}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "# Step 4: Plot results\n",
    "plot_results(history, y_test, predictions)\n",
    "\n",
    "# Step 5: Feature importance (optional)\n",
    "def get_feature_importance(model, feature_names, X_test, y_test, n_top=10):\n",
    "    \"\"\"\n",
    "    Get feature importance using permutation method\n",
    "    \"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    # Convert to sklearn format for permutation importance\n",
    "    def keras_predict_proba(X):\n",
    "        return model.predict(X, verbose=0).ravel()\n",
    "    \n",
    "    # Calculate baseline score\n",
    "    baseline_score = roc_auc_score(y_test, keras_predict_proba(X_test))\n",
    "    \n",
    "    # Simple feature importance by correlation with target\n",
    "    feature_corr = []\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        corr = np.corrcoef(X_test[:, i], y_test)[0, 1]\n",
    "        feature_corr.append((feature, abs(corr)))\n",
    "    \n",
    "    feature_corr.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n📊 TOP {n_top} MOST IMPORTANT FEATURES:\")\n",
    "    for i, (feature, corr) in enumerate(feature_corr[:n_top]):\n",
    "        print(f\"{i+1:2d}. {feature}: {corr:.4f}\")\n",
    "    \n",
    "    return feature_corr\n",
    "\n",
    "# Get feature importance\n",
    "feature_names = df.drop('converted', axis=1).columns.tolist()\n",
    "feature_importance = get_feature_importance(final_model, feature_names, X_test, y_test)\n",
    "\n",
    "def plot_results(history, y_test, predictions):\n",
    "    \"\"\"\n",
    "    Plot training history and evaluation metrics\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0,0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0,0].set_title('Model Loss')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0,1].set_title('Model Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # ROC Curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    axes[1,0].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "    axes[1,0].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[1,0].set_xlabel('False Positive Rate')\n",
    "    axes[1,0].set_ylabel('True Positive Rate')\n",
    "    axes[1,0].set_title('ROC Curve')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "    axes[1,1].plot(recall, precision)\n",
    "    axes[1,1].set_xlabel('Recall')\n",
    "    axes[1,1].set_ylabel('Precision')\n",
    "    axes[1,1].set_title('Precision-Recall Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Lead Conversion Prediction Pipeline Ready!\")\n",
    "print(\"\\nKey Functions:\")\n",
    "print(\"1. prepare_data() - Data prep and train/test split\")\n",
    "print(\"2. compare_imbalance_methods() - Test SMOTE, SMOTE-Tomek, undersampling\")\n",
    "print(\"3. train_final_model() - Train final model with best method\")\n",
    "print(\"4. plot_results() - Visualize training and evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b268d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the NaN values in feature importance\n",
    "print(\"🔧 CHECKING NaN FEATURES:\")\n",
    "print(f\"noshow_outcomes_count unique values: {df['noshow_outcomes_count'].value_counts()}\")\n",
    "print(f\"noshow_outcome_ratio unique values: {df['noshow_outcome_ratio'].value_counts()}\")\n",
    "\n",
    "# These might be all zeros or have very little variance\n",
    "print(f\"noshow_outcomes_count variance: {df['noshow_outcomes_count'].var()}\")\n",
    "print(f\"noshow_outcome_ratio variance: {df['noshow_outcome_ratio'].var()}\")\n",
    "\n",
    "# Let's run the threshold analysis we discussed\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,    # ← add this\n",
    "    recall_score,       # ← and this\n",
    "    f1_score            # ← and this\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "print(\"\\n🎯 OPTIMAL THRESHOLD ANALYSIS:\")\n",
    "print(\"Threshold | Precision | Recall | F1 | Leads to Call | Efficiency Gain\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for threshold in thresholds:\n",
    "    y_pred = (predictions > threshold).astype(int)\n",
    "    \n",
    "    if y_pred.sum() > 0:\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        leads_to_call = y_pred.sum()\n",
    "        efficiency = len(y_test) / leads_to_call\n",
    "        \n",
    "        print(f\"   {threshold:.1f}    |   {precision:.3f}   |  {recall:.3f}  | {f1:.3f} |     {leads_to_call:4d}      |     {efficiency:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12bda3",
   "metadata": {},
   "source": [
    "no sc and fu also gave good results. we will carry on with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8c792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
