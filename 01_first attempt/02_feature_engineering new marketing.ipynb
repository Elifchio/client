{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bd0fa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57495af0",
   "metadata": {},
   "source": [
    "copy of notebook 02 to chnange the marketing logic, also datasetes now have the 2 marketing columns instead of the old mktgparams one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "287d505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc_after_march (calendarevent logs) \n",
    "file_path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\sc_from_march.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_path)\n",
    "df1.columns = df1.columns.str.lower()\n",
    "\n",
    "#sc_ older contracts (calendarevent)\n",
    "path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\sc_older_contracts.csv\"\n",
    "df2 = pd.read_csv(path)\n",
    "df2.columns = df2.columns.str.lower()\n",
    "\n",
    "#concat\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = combined_df.copy()\n",
    "\n",
    "# qualitative (slider + contract)\n",
    "\n",
    "file2 = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\info_client2.csv\"\n",
    "\n",
    "df3 = pd.read_csv(file2)\n",
    "df3.columns = df3.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a0bce55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(r\"processed_data/df_model.csv\") # manualy saved leadtime info. hafter having gotten modified in older attempts/notebook\n",
    "behaviour = behaviour.drop(['converted', 'lead_to_sc1_days', 'sc1_schedule_to_appointment_days', 'showed_up_sc1' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9c02e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19981 entries, 0 to 19980\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          19981 non-null  int64  \n",
      " 1   total_bc_attempts           19981 non-null  int64  \n",
      " 2   total_bc_outcomes           19981 non-null  int64  \n",
      " 3   lead_to_first_bc_days       19981 non-null  float64\n",
      " 4   bc_duration_days            19981 non-null  float64\n",
      " 5   bc_frequency                19981 non-null  float64\n",
      " 6   positive_outcomes_count     19981 non-null  int64  \n",
      " 7   negative_outcomes_count     19981 non-null  int64  \n",
      " 8   noshow_outcomes_count       19981 non-null  int64  \n",
      " 9   positive_outcome_ratio      19981 non-null  float64\n",
      " 10  negative_outcome_ratio      19981 non-null  float64\n",
      " 11  noshow_outcome_ratio        19981 non-null  float64\n",
      " 12  reachability_score          19981 non-null  float64\n",
      " 13  outcome_trend               19981 non-null  int64  \n",
      " 14  persistence_after_negative  19981 non-null  int64  \n",
      " 15  engagement_score            19981 non-null  float64\n",
      " 16  efficiency_score            19981 non-null  float64\n",
      " 17  last_bc_outcome_encoded     19981 non-null  int64  \n",
      " 18  first_bc_outcome_encoded    19981 non-null  int64  \n",
      "dtypes: float64(9), int64(10)\n",
      "memory usage: 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "behaviour.columns = behaviour.columns.str.lower()\n",
    "behaviour.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9c09a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AD HOC\n",
    "# trying to match the positive contracts to leadtime\n",
    "#try1 = df2.merge(behaviour, left_on='leadid', right_on='id', how='left', indicator=True)\n",
    "#df2.info()\n",
    "#print(df2[\"leadid\"].nunique())\n",
    "#try1.info()\n",
    "#print(try1[\"id\"].nunique())\n",
    "#print(behaviour[\"id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ea6428ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83407 entries, 0 to 83406\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   createdat  83407 non-null  object\n",
      " 1   deletedat  34985 non-null  object\n",
      " 2   requestid  83407 non-null  int64 \n",
      " 3   tsstart    83407 non-null  object\n",
      " 4   type       83407 non-null  object\n",
      " 5   is_passed  83407 non-null  int64 \n",
      " 6   leadid     83407 non-null  int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 4.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24520"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 83346 sc logs of 28193 unique requests\n",
    "combined_df.info()\n",
    "combined_df[\"requestid\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e3fb0750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126489 entries, 0 to 126488\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   requestid               126489 non-null  int64  \n",
      " 1   zipregion               123407 non-null  object \n",
      " 2   evaluationtime          119790 non-null  object \n",
      " 3   desiredinstallationend  119870 non-null  object \n",
      " 4   electricitybill         105858 non-null  float64\n",
      " 5   heatingbill             92462 non-null   float64\n",
      " 6   channel__campaign       126489 non-null  object \n",
      " 7   channel                 125384 non-null  object \n",
      " 8   grosscontractsigned     2795 non-null    float64\n",
      " 9   selfipaimportedat       2632 non-null    object \n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 9.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grosscontractsigned\n",
       "1.0    2746\n",
       "0.0      49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# behavioral data that has 2.7k positive examples\n",
    "df3.info()\n",
    "df3[\"grosscontractsigned\"].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1648b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requestids with duplicates: 52\n",
      "Total duplicate rows: 52\n",
      "\n",
      "Dropped 5 absolute duplicate rows\n",
      "Remaining duplicate requestids before grosscontractsigned cleanup: 47\n",
      "Dropped 47 rows where grosscontractsigned = 0 from duplicate requestids\n",
      "\n",
      "==================================================\n",
      "FINAL DUPLICATE CHECK\n",
      "==================================================\n",
      "Number of requestids with duplicates: 0\n",
      "Total duplicate rows: 0\n",
      "âœ… No remaining duplicates!\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates in df3 -info client-,they were found in notebook 01. probably because same requestid have 2 entries in airtable contracts, one 0 one 1. we wanna keep only 1s.\n",
    "\n",
    "# Step 1: Count requestids that have duplicates\n",
    "duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df3['requestid'].duplicated().sum()}\")\n",
    "\n",
    "# Step 2: Drop absolute duplicates (excluding leadid column)\n",
    "comparison_cols = [col for col in df3.columns if col != 'leadid']\n",
    "df3_before = len(df3)\n",
    "df3 = df3.drop_duplicates(subset=comparison_cols, keep='first')\n",
    "df3_after = len(df3)\n",
    "\n",
    "print(f\"\\nDropped {df3_before - df3_after} absolute duplicate rows\")\n",
    "\n",
    "# Step 3: From the remaining duplicate requestids, drop rows where grosscontractsigned = 0\n",
    "remaining_duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "\n",
    "if len(remaining_duplicate_requestids) > 0:\n",
    "    print(f\"Remaining duplicate requestids before grosscontractsigned cleanup: {len(remaining_duplicate_requestids)}\")\n",
    "    \n",
    "    # Create condition: either NOT a duplicate requestid, OR grosscontractsigned != 0\n",
    "    condition = (~df3['requestid'].isin(remaining_duplicate_requestids)) | (df3['grosscontractsigned'] != 0)\n",
    "    \n",
    "    df3_before_net_drop = len(df3)\n",
    "    df3 = df3[condition]\n",
    "    df3_after_net_drop = len(df3)\n",
    "    \n",
    "    print(f\"Dropped {df3_before_net_drop - df3_after_net_drop} rows where grosscontractsigned = 0 from duplicate requestids\")\n",
    "\n",
    "# Step 4: Final duplicate check\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL DUPLICATE CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(final_duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df3['requestid'].duplicated().sum()}\")\n",
    "\n",
    "if len(final_duplicate_requestids) > 0:\n",
    "    # Check what columns still differ\n",
    "    all_differing_columns = set()\n",
    "    for requestid in final_duplicate_requestids:\n",
    "        group = df3[df3['requestid'] == requestid]\n",
    "        for col in df3.columns:\n",
    "            if col != 'requestid' and group[col].nunique() > 1:\n",
    "                all_differing_columns.add(col)\n",
    "    \n",
    "    print(f\"Differing columns: {sorted(all_differing_columns)}\")\n",
    "else:\n",
    "    print(f\"âœ… No remaining duplicates!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "53976aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grosscontractsigned\n",
       "1.0    2741\n",
       "0.0       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"grosscontractsigned\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353766",
   "metadata": {},
   "source": [
    "#### Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d1690b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing appointment types\n",
    "\n",
    "type_mapping = {\n",
    "    'FU': 'FU',\n",
    "    'FUVD': 'FU', \n",
    "    'REMINDER': 'FU',\n",
    "    'VDFIELD': 'SC',\n",
    "    'VD': 'SC'\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "df['grouped_type'] = df['type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ad51c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition for net events (not cancelled and past)\n",
    "df['is_net_event'] = df['deletedat'].isna() & df['is_passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "05bbb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross counts (all events)\n",
    "gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "# Net counts (only events that actually happened)\n",
    "net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "net_counts.columns = [f'net_{col}' for col in net_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "511b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gross and net counts\n",
    "counts_df = gross_counts.join(net_counts, how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b102cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8011647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn self ipa event to bbinary \n",
    "df3['selfipa_done'] = df3['selfipaimportedat'].notnull().astype(int)\n",
    "df3.drop('selfipaimportedat', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "867c26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  handle marketing\n",
    "# new marketing source\n",
    "\n",
    "\n",
    "def create_aggregated_column(df):\n",
    "    \"\"\"\n",
    "    Creates the 'Aggregated' column based on Channel and CHANNEL__CAMPAIGN values\n",
    "    \"\"\"\n",
    "    def get_aggregated_value(row):\n",
    "        # Check Youtube first\n",
    "        if row.get('Youtube') == 1:\n",
    "            return \"Youtube\"\n",
    "        \n",
    "        # Check specific CHANNEL__CAMPAIGN values\n",
    "        channel_campaign = row.get('channel_campaign')\n",
    "        if channel_campaign == \"d2d\":\n",
    "            return \"d2d\"\n",
    "        elif channel_campaign == \"form_classico\":\n",
    "            return \"Form_FB\"\n",
    "        elif channel_campaign == \"ranger\":\n",
    "            return \"ranger\"\n",
    "        elif channel_campaign == \"referral_link\":\n",
    "            return \"Referral\"\n",
    "        elif channel_campaign == \"referral_manual\":\n",
    "            return \"Referral\"\n",
    "        \n",
    "        # Check Channel column (case insensitive)\n",
    "        channel = row.get('channel')\n",
    "        if channel and pd.notna(channel):\n",
    "            channel_lower = channel.lower()\n",
    "            \n",
    "            if channel_lower == \"affiliation\":\n",
    "                return \"Affiliation\"\n",
    "            elif channel_lower == \"outbrain\":\n",
    "                return \"Outbrain\"\n",
    "            elif channel_lower in [\"taboola\", \"taboola_it\"]:\n",
    "                return \"Taboola\"\n",
    "            elif channel_lower in [\"facebook\", \"fb\", \"ig\", \"instagram\", \"facebook_marketplace\"]:\n",
    "                return \"Meta\"\n",
    "            elif channel_lower in [\"search ads\", \"searchads\", \"google\", \"google ads\"]:\n",
    "                return \"Google\"\n",
    "            elif channel_lower in [\"organic_brand\", \"organic\", \"direct\"]:\n",
    "                return \"Organic\"\n",
    "            elif channel_lower == \"seo\":\n",
    "                return \"SEO\"\n",
    "            elif channel_lower == \"criteo\":\n",
    "                return \"Criteo\"\n",
    "            elif channel_lower == \"mediago\":\n",
    "                return \"MediaGo\"\n",
    "            elif channel_lower == \"tiktok\":\n",
    "                return \"Tik Tok\"\n",
    "        \n",
    "        return \"Other\"\n",
    "    \n",
    "    df['aggregated'] = df.apply(get_aggregated_value, axis=1)\n",
    "    df.drop('channel', axis=1, inplace=True)\n",
    "    df.drop('channel__campaign', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1af4e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =   create_aggregated_column(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "399ef2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 126437 entries, 0 to 126488\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   requestid               126437 non-null  int64  \n",
      " 1   zipregion               123357 non-null  object \n",
      " 2   evaluationtime          119738 non-null  object \n",
      " 3   desiredinstallationend  119818 non-null  object \n",
      " 4   electricitybill         105809 non-null  float64\n",
      " 5   heatingbill             92430 non-null   float64\n",
      " 6   grosscontractsigned     2743 non-null    float64\n",
      " 7   selfipa_done            126437 non-null  int64  \n",
      " 8   aggregated              126437 non-null  object \n",
      "dtypes: float64(3), int64(2), object(4)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6096a79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregated\n",
       "Meta           41212\n",
       "Outbrain       27213\n",
       "Google         24548\n",
       "Other           9295\n",
       "Organic         7771\n",
       "Taboola         5172\n",
       "Affiliation     3987\n",
       "Tik Tok         3799\n",
       "MediaGo         1957\n",
       "Criteo          1306\n",
       "SEO              177\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"aggregated\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9be656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert createdat to datetime\n",
    "df['createdat'] = pd.to_datetime(df['createdat'])\n",
    "\n",
    "# Step 2: Get first gross SC and first net FU timestamps\n",
    "first_gross_sc = df[df['grouped_type'] == 'SC'].groupby('requestid')['createdat'].min()\n",
    "first_net_fu = df[(df['grouped_type'] == 'FU') & df['is_net_event']].groupby('requestid')['createdat'].min()\n",
    "\n",
    "# Step 3: Calculate time difference in hours\n",
    "counts_df['time_first_sc_to_first_net_fu'] = (first_net_fu - first_gross_sc).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "88cecdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gross_SC\n",
       "1     15378\n",
       "2      3169\n",
       "3       736\n",
       "4       230\n",
       "5        51\n",
       "6        16\n",
       "7         6\n",
       "10        2\n",
       "8         1\n",
       "23        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop older requests that havent had a sc since march\n",
    "counts_df = counts_df[counts_df['gross_SC'] != 0]\n",
    "counts_df['gross_SC'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9d420799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (19590, 5)\n",
      "No missing values: True\n"
     ]
    }
   ],
   "source": [
    "# handle missing value for customers who never made to fu\n",
    "counts_df['time_first_sc_to_first_net_fu'] = counts_df['time_first_sc_to_first_net_fu'].fillna(-1)\n",
    "\n",
    "# Check final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {counts_df.shape}\")\n",
    "print(f\"No missing values: {counts_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c23d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negative time differences (not NaN fills): 17\n",
      "Positive time differences: 8576\n",
      "NaN fills (-1): 10997\n",
      "\n",
      "Real negative values range: -3764.75 to -22.24 hours\n"
     ]
    }
   ],
   "source": [
    "# problematic entries with negative time difference: \n",
    "# Separate real negative times from NaN fill values\n",
    "real_negative = counts_df[\n",
    "    (counts_df['time_first_sc_to_first_net_fu'] < 0) & \n",
    "    (counts_df['time_first_sc_to_first_net_fu'] != -1)\n",
    "]\n",
    "\n",
    "actual_valid_times = counts_df[counts_df['time_first_sc_to_first_net_fu'] >= 0]\n",
    "\n",
    "print(f\"Actual negative time differences (not NaN fills): {len(real_negative)}\")\n",
    "print(f\"Positive time differences: {len(actual_valid_times)}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "if len(real_negative) > 0:\n",
    "    print(f\"\\nReal negative values range: {real_negative['time_first_sc_to_first_net_fu'].min():.2f} to {real_negative['time_first_sc_to_first_net_fu'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "06a370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle negative time difference\n",
    "# Remove outliers entirely (set to NaN, then fill with -1)\n",
    "counts_df.loc[counts_df['time_first_sc_to_first_net_fu'] < -1, 'time_first_sc_to_first_net_fu'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aec7cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing outliers:\n",
      "Negative values (excluding -1 fills): 0\n",
      "Valid positive time differences: 8537\n",
      "NaN fills (-1): 11014\n",
      "\n",
      "Final dataset shape: (19590, 5)\n",
      "Ready to join with target variable!\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Negative values (excluding -1 fills): {((counts_df['time_first_sc_to_first_net_fu'] < 0) & (counts_df['time_first_sc_to_first_net_fu'] != -1)).sum()}\")\n",
    "print(f\"Valid positive time differences: {(counts_df['time_first_sc_to_first_net_fu'] > 0).sum()}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {counts_df.shape}\")\n",
    "print(\"Ready to join with target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "98124e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1191</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1271</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1285</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1294</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1299</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requestid  gross_FU  gross_SC  net_FU  net_SC  \\\n",
       "0       1191         0         1     0.0     1.0   \n",
       "1       1271         0         1     0.0     1.0   \n",
       "2       1285         1         1     1.0     1.0   \n",
       "3       1294         0         1     0.0     1.0   \n",
       "4       1299         2         1     2.0     1.0   \n",
       "\n",
       "   time_first_sc_to_first_net_fu  \n",
       "0                      -1.000000  \n",
       "1                      -1.000000  \n",
       "2                       0.000000  \n",
       "3                      -1.000000  \n",
       "4                       0.000278  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point requestid is the index. make it explicit\n",
    "\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9b72fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [requestid, gross_FU, gross_SC, net_FU, net_SC, time_first_sc_to_first_net_fu]\n",
       "Index: []"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see net_sc accuracy\n",
    "mask1 = counts_df[counts_df['gross_SC'] == 0]\n",
    "mask1\n",
    "# 7k didnt make it to net sc but has grosssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e76056e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 6 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   requestid                      19590 non-null  int64  \n",
      " 1   gross_FU                       19590 non-null  int64  \n",
      " 2   gross_SC                       19590 non-null  int64  \n",
      " 3   net_FU                         19590 non-null  float64\n",
      " 4   net_SC                         19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu  19590 non-null  float64\n",
      "dtypes: float64(3), int64(3)\n",
      "memory usage: 918.4 KB\n"
     ]
    }
   ],
   "source": [
    "counts_df.info()\n",
    "#leadid not present here, we moved to 19.5 k distinct requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dd583cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# requestid duplicates? --> no, it was index anyways\n",
    "# Show which requestid values appear more than once\n",
    "duplicate_requestids = counts_df[counts_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dba7e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# also df2 has no duplicates also because we did prviously checked and handled\n",
    "duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b02d0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (19590, 14)\n",
      "Target variable distribution:\n",
      "grosscontractsigned\n",
      "0.0    17365\n",
      "1.0     2225\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['requestid', 'gross_FU', 'gross_SC', 'net_FU', 'net_SC', 'time_first_sc_to_first_net_fu', 'zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'grosscontractsigned', 'selfipa_done', 'aggregated']\n"
     ]
    }
   ],
   "source": [
    "# add qualitative data\n",
    "# now we have 17k 0 2.2 k 1. where did 300+ 1s went? maybe they were from requests that never had a sc, we dropped them previously\n",
    "final_df = counts_df.merge(df3, on='requestid', how='left')\n",
    "\n",
    "\n",
    "# Fill non-matching requestids with 0 for the target\n",
    "final_df[\"grosscontractsigned\"] = final_df[\"grosscontractsigned\"].fillna(0)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(final_df[\"grosscontractsigned\"].value_counts())\n",
    "print(f\"\\nColumns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6cde1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d59ece8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 14 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   requestid                      19590 non-null  int64  \n",
      " 1   gross_FU                       19590 non-null  int64  \n",
      " 2   gross_SC                       19590 non-null  int64  \n",
      " 3   net_FU                         19590 non-null  float64\n",
      " 4   net_SC                         19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu  19590 non-null  float64\n",
      " 6   zipregion                      18999 non-null  object \n",
      " 7   evaluationtime                 19346 non-null  object \n",
      " 8   desiredinstallationend         19347 non-null  object \n",
      " 9   electricitybill                13536 non-null  float64\n",
      " 10  heatingbill                    6462 non-null   float64\n",
      " 11  grosscontractsigned            19590 non-null  float64\n",
      " 12  selfipa_done                   19590 non-null  int64  \n",
      " 13  aggregated                     19590 non-null  object \n",
      "dtypes: float64(6), int64(4), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f59a1d",
   "metadata": {},
   "source": [
    "#### Merging with behaviour from leadtime might happen here so null handling and encoding is not repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38cbb0",
   "metadata": {},
   "source": [
    "#### Encoding and Null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "452d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of bills have -1, 1 and 0\n",
    "\n",
    "# set wrong bill values to nan\n",
    "# Replace specific values with NaN\n",
    "final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea01f",
   "metadata": {},
   "source": [
    "Optional for bill handling \n",
    "def clean_billing_column(series, column_name):\n",
    "    \"\"\"Clean billing column: set negative values and extreme outliers to replacement value\"\"\"\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    series_clean = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Define reasonable bounds (adjust these based on your domain knowledge)\n",
    "    min_reasonable = 0  # Bills shouldn't be negative\n",
    "    max_reasonable = 2000  # Adjust based on your market (e.g., â‚¬1000/month seems high)\n",
    "    \n",
    "    # Count issues for reporting\n",
    "    negative_count = (series_clean < min_reasonable).sum()\n",
    "    outlier_count = (series_clean > max_reasonable).sum()\n",
    "    \n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  - Negative values: {negative_count}\")\n",
    "    print(f\"  - Values > {max_reasonable}: {outlier_count}\")\n",
    "    print(f\"  - Original NaN: {series.isna().sum()}\")\n",
    "    \n",
    "    # Replace problematic values\n",
    "    series_clean.loc[series_clean < min_reasonable] = np.nan  # or -1\n",
    "    series_clean.loc[series_clean > max_reasonable] = np.nan  # or -1\n",
    "    \n",
    "    return series_clean\n",
    "\n",
    "# Apply cleaning\n",
    "final_df['heatingbill'] = clean_billing_column(final_df['heatingbill'], 'heatingbill')\n",
    "final_df['electricitybill'] = clean_billing_column(final_df['electricitybill'], 'electricitybill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5f97be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicator correlations with target:\n",
      "electricitybill_missing          -0.043034\n",
      "zipregion_missing                -0.022690\n",
      "evaluationtime_missing           -0.003936\n",
      "desiredinstallationend_missing   -0.002325\n",
      "heatingbill_missing               0.078719\n",
      "aggregated_missing                     NaN\n",
      "Name: grosscontractsigned, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# before encoding, mark the missingness inside the row;\n",
    "missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'aggregated']\n",
    "\n",
    "for col in missing_cols:\n",
    "    final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "\n",
    "# Check the predictive power of missing indicators\n",
    "print(\"Missing indicator correlations with target:\")\n",
    "missing_indicators = [f'{col}_missing' for col in missing_cols]\n",
    "missing_corrs = final_df[missing_indicators + [\"grosscontractsigned\"]].corr()[\"grosscontractsigned\"].drop(\"grosscontractsigned\")\n",
    "print(missing_corrs.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b47781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b111b4",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8f2ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# desiredinstallationend\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-4mesi': 'three_to_four_months', \n",
    "    '5+mesi': 'more_than_5_months',\n",
    "    '1-2mesi': 'one_to_two_months',\n",
    "    'Non lo so': 'dont_know',\n",
    "    'short': np.nan,\n",
    "    # Already mapped values (keep as-is)\n",
    "    'dont_know': 'dont_know',\n",
    "    'three_to_four_months': 'three_to_four_months',\n",
    "    'one_to_two_months': 'one_to_two_months', \n",
    "    'more_than_5_months': 'more_than_5_months',\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1889c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationtime\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-6 mesi': np.nan,\n",
    "    '<3 mesi': np.nan,\n",
    "    '>6 mesi': np.nan,\n",
    "    # Already mapped English values\n",
    "    'less_than_three_months': np.nan,\n",
    "    'more_than_six_months': np.nan,\n",
    "    # Other values that appear in your data\n",
    "    'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "    'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "    'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "    'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "09d77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the old columns directly\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "\n",
    "# Drop the temporary columns\n",
    "final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b24a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan before encoding\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c19a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with ordinal encoding for time-based features\n",
    "\n",
    "# Ordinal encoding for desiredinstallationend (time-based order)\n",
    "time_order = [ 'one_to_two_months', 'three_to_four_months', 'more_than_5_months', 'dont_know', 'missing']\n",
    "final_df['desiredinstallationend_encoded'] = final_df['desiredinstallationend'].map({val: i for i, val in enumerate(time_order)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78461e67",
   "metadata": {},
   "source": [
    "##### Analysis to see how to handle marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a2f7fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19590 non-null  int64  \n",
      " 1   gross_FU                        19590 non-null  int64  \n",
      " 2   gross_SC                        19590 non-null  int64  \n",
      " 3   net_FU                          19590 non-null  float64\n",
      " 4   net_SC                          19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19590 non-null  float64\n",
      " 6   zipregion                       18999 non-null  object \n",
      " 7   evaluationtime                  19590 non-null  object \n",
      " 8   desiredinstallationend          19590 non-null  object \n",
      " 9   electricitybill                 12735 non-null  object \n",
      " 10  heatingbill                     4681 non-null   object \n",
      " 11  grosscontractsigned             19590 non-null  float64\n",
      " 12  selfipa_done                    19590 non-null  int64  \n",
      " 13  aggregated                      19590 non-null  object \n",
      " 14  zipregion_missing               19590 non-null  int64  \n",
      " 15  evaluationtime_missing          19590 non-null  int64  \n",
      " 16  desiredinstallationend_missing  19590 non-null  int64  \n",
      " 17  electricitybill_missing         19590 non-null  int64  \n",
      " 18  heatingbill_missing             19590 non-null  int64  \n",
      " 19  aggregated_missing              19590 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  19590 non-null  int64  \n",
      "dtypes: float64(4), int64(11), object(6)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7d73e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rates by category:\n",
      "\n",
      "\n",
      "ZIPREGION:\n",
      "                       total_samples  conversions  conversion_rate\n",
      "zipregion                                                         \n",
      "Trentino-Alto Adige              170         40.0         0.235294\n",
      "Friuli-Venezia Giulia           1157        236.0         0.203976\n",
      "Liguria                          507         93.0         0.183432\n",
      "Lombardia                       3150        435.0         0.138095\n",
      "Piemonte                        1767        243.0         0.137521\n",
      "Umbria                           285         37.0         0.129825\n",
      "Molise                           114         14.0         0.122807\n",
      "Veneto                          1488        181.0         0.121640\n",
      "Valle D'Aosta                     75          9.0         0.120000\n",
      "Toscana                         1333        157.0         0.117779\n",
      "Emilia-Romagna                  1685        197.0         0.116914\n",
      "Marche                           383         40.0         0.104439\n",
      "Lazio                           1874        191.0         0.101921\n",
      "Campania                         675         61.0         0.090370\n",
      "Abruzzo                          390         34.0         0.087179\n",
      "Basilicata                       151         13.0         0.086093\n",
      "Sardegna                         586         44.0         0.075085\n",
      "Calabria                         673         38.0         0.056464\n",
      "Sicilia                         1124         56.0         0.049822\n",
      "Puglia                          1412         63.0         0.044618\n",
      "Overall variation: 0.0488\n",
      "\n",
      "AGGREGATED:\n",
      "             total_samples  conversions  conversion_rate\n",
      "aggregated                                              \n",
      "SEO                      4          3.0         0.750000\n",
      "Affiliation            134         50.0         0.373134\n",
      "Taboola                219         66.0         0.301370\n",
      "Google                2729        526.0         0.192745\n",
      "Criteo                 110         18.0         0.163636\n",
      "Organic               1755        266.0         0.151567\n",
      "Outbrain              3393        398.0         0.117300\n",
      "Meta                  6460        568.0         0.087926\n",
      "Other                 2945        225.0         0.076401\n",
      "MediaGo                488         34.0         0.069672\n",
      "Tik Tok               1353         71.0         0.052476\n",
      "Overall variation: 0.2045\n"
     ]
    }
   ],
   "source": [
    "# For each categorical column, see conversion rates\n",
    "print(\"Conversion rates by category:\\n\")\n",
    "\n",
    "for col in ['zipregion', 'aggregated']:  # replace with your actual column names\n",
    "    conversion_by_cat = final_df.groupby(col)[\"grosscontractsigned\"].agg(['count', 'sum', 'mean'])\n",
    "    conversion_by_cat.columns = ['total_samples', 'conversions', 'conversion_rate']\n",
    "    conversion_by_cat = conversion_by_cat.sort_values('conversion_rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(conversion_by_cat)\n",
    "    print(f\"Overall variation: {conversion_by_cat['conversion_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf1bfd",
   "metadata": {},
   "source": [
    "Marketing gave good variance, would like to maintain that information. Region is not that significant but still good. \n",
    "However given the * of unique values, in both columns I will opt for grouping instead of each value having its column. \n",
    "\n",
    "Instead of 30+ categorical features, you get ~6-8, keeping the predictive power but losing the noise.\n",
    "\n",
    "Downside; this grouping should occasionally double checked to see if it still makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163be87",
   "metadata": {},
   "source": [
    "##### one hot encoding for marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bcb8c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance groups\n",
    "aggregated_groups = {\n",
    "    'High': ['Affiliation', 'Taboola'],           # >25% conversion\n",
    "    'Medium': ['Google', 'Criteo', 'Organic'],    # 15-20% conversion  \n",
    "    'Low': ['Outbrain', 'Meta', 'Other', 'MediaGo', 'Tik Tok']  # <12% conversion\n",
    "}\n",
    "def group_aggregated(value):\n",
    "    if value == 'SEO':  # Handle the unreliable outlier\n",
    "        return 'Medium'  # Conservative assignment\n",
    "    \n",
    "    for group, channels in aggregated_groups.items():\n",
    "        if value in channels:\n",
    "            return group\n",
    "    return 'Low'  # fallback\n",
    "\n",
    "# Apply grouping\n",
    "final_df['mktg_grouped'] = final_df['aggregated'].apply(group_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9995467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group regions by performance\n",
    "def group_regions(region):\n",
    "    # Top performers with good sample sizes\n",
    "    if region in ['Trentino-Alto Adige', 'Friuli-Venezia Giulia', 'Liguria']:\n",
    "        return 'High_Performer'  # 18-24% conversion\n",
    "    \n",
    "    # Large regions with solid performance  \n",
    "    elif region in ['Lombardia', 'Piemonte', 'Veneto', 'Toscana', 'Emilia-Romagna']:\n",
    "        return 'Large_Solid'     # 11-14% conversion\n",
    "    \n",
    "    # Smaller regions with decent samples\n",
    "    elif region in ['Umbria', 'Lazio', 'Marche']:\n",
    "        return 'Medium'          # 10-13% conversion\n",
    "    \n",
    "    # Lower performing regions\n",
    "    else:\n",
    "        return 'Lower'   \n",
    "\n",
    "final_df['region_grouped'] = final_df['zipregion'].apply(group_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0c5957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the grouped categories\n",
    "final_df = pd.get_dummies(final_df, columns=['mktg_grouped', 'region_grouped'], prefix=['mktg', 'region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "be7d7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777490e3",
   "metadata": {},
   "source": [
    "df is the log of all calendar_events so a direct join will make our dataset exponentially duplicated.\n",
    "\n",
    "I will first get the unique requestid --> leadid cpairs and then merge that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "937eadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestid_leadid_pairs = df[['requestid', 'leadid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f4c8ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requestid_leadid_pairs[requestid_leadid_pairs['requestid'].duplicated(keep=False)]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c10a614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also add leadid to the final df for merging with behaviour data\n",
    "# probably there is a cleaner way to do this\n",
    "\n",
    "# Suppose the column you want from df1 is called \"col_from_df1\"\n",
    "final_df = final_df.merge(\n",
    "    requestid_leadid_pairs[['requestid', 'leadid']],  # keep only requestid + desired column\n",
    "    on='requestid',                      # join key\n",
    "    how='left'                           # keep all rows from df2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bf10b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 29 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19590 non-null  int64  \n",
      " 1   gross_FU                        19590 non-null  int64  \n",
      " 2   gross_SC                        19590 non-null  int64  \n",
      " 3   net_FU                          19590 non-null  float64\n",
      " 4   net_SC                          19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19590 non-null  float64\n",
      " 6   zipregion                       18999 non-null  object \n",
      " 7   evaluationtime                  19590 non-null  object \n",
      " 8   desiredinstallationend          19590 non-null  object \n",
      " 9   electricitybill                 12735 non-null  object \n",
      " 10  heatingbill                     4681 non-null   object \n",
      " 11  grosscontractsigned             19590 non-null  float64\n",
      " 12  selfipa_done                    19590 non-null  int64  \n",
      " 13  aggregated                      19590 non-null  object \n",
      " 14  zipregion_missing               19590 non-null  int64  \n",
      " 15  evaluationtime_missing          19590 non-null  int64  \n",
      " 16  desiredinstallationend_missing  19590 non-null  int64  \n",
      " 17  electricitybill_missing         19590 non-null  int64  \n",
      " 18  heatingbill_missing             19590 non-null  int64  \n",
      " 19  aggregated_missing              19590 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  19590 non-null  int64  \n",
      " 21  mktg_High                       19590 non-null  bool   \n",
      " 22  mktg_Low                        19590 non-null  bool   \n",
      " 23  mktg_Medium                     19590 non-null  bool   \n",
      " 24  region_High_Performer           19590 non-null  bool   \n",
      " 25  region_Large_Solid              19590 non-null  bool   \n",
      " 26  region_Lower                    19590 non-null  bool   \n",
      " 27  region_Medium                   19590 non-null  bool   \n",
      " 28  leadid                          19590 non-null  int64  \n",
      "dtypes: bool(7), float64(4), int64(12), object(6)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1713e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "73b32aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any requestid with no leadid?\n",
    "final_df[final_df['leadid'].isnull()]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "18644c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values: False\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns and create final feature matrix\n",
    "columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'aggregated']\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "print(f\"\\nNo missing values: {final_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215f64f",
   "metadata": {},
   "source": [
    "#### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "16e8505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n",
      "Dataset ready: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_22316\\1109642794.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_22316\\1109642794.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n"
     ]
    }
   ],
   "source": [
    "# Fix electricitybill missing values\n",
    "final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "\n",
    "# Fix desiredinstallationend_encoded - NaN means 'Unknown' which should be 0 --> check if needed\n",
    "#X_final['desiredinstallationend_encoded'] = X_final['desiredinstallationend_encoded'].fillna(0)\n",
    "\n",
    "# Verify all missing values are gone\n",
    "print(f\"Remaining missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Dataset ready: {final_df.isnull().sum().sum() == 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bc1b",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5df80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling when needed\n",
    "continuous_cols= [\n",
    "    'time_first_sc_to_first_net_fu',\n",
    "    'electricitybill', \n",
    "    'heatingbill'\n",
    "]\n",
    "\n",
    "# Scale only the continuous features\n",
    "scaler = StandardScaler()\n",
    "final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ca844c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19981 entries, 0 to 19980\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          19981 non-null  int64  \n",
      " 1   total_bc_attempts           19981 non-null  int64  \n",
      " 2   total_bc_outcomes           19981 non-null  int64  \n",
      " 3   lead_to_first_bc_days       19981 non-null  float64\n",
      " 4   bc_duration_days            19981 non-null  float64\n",
      " 5   bc_frequency                19981 non-null  float64\n",
      " 6   positive_outcomes_count     19981 non-null  int64  \n",
      " 7   negative_outcomes_count     19981 non-null  int64  \n",
      " 8   noshow_outcomes_count       19981 non-null  int64  \n",
      " 9   positive_outcome_ratio      19981 non-null  float64\n",
      " 10  negative_outcome_ratio      19981 non-null  float64\n",
      " 11  noshow_outcome_ratio        19981 non-null  float64\n",
      " 12  reachability_score          19981 non-null  float64\n",
      " 13  outcome_trend               19981 non-null  int64  \n",
      " 14  persistence_after_negative  19981 non-null  int64  \n",
      " 15  engagement_score            19981 non-null  float64\n",
      " 16  efficiency_score            19981 non-null  float64\n",
      " 17  last_bc_outcome_encoded     19981 non-null  int64  \n",
      " 18  first_bc_outcome_encoded    19981 non-null  int64  \n",
      "dtypes: float64(9), int64(10)\n",
      "memory usage: 2.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19590 non-null  int64  \n",
      " 1   gross_FU                        19590 non-null  int64  \n",
      " 2   gross_SC                        19590 non-null  int64  \n",
      " 3   net_FU                          19590 non-null  float64\n",
      " 4   net_SC                          19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19590 non-null  float64\n",
      " 6   electricitybill                 19590 non-null  float64\n",
      " 7   heatingbill                     19590 non-null  float64\n",
      " 8   grosscontractsigned             19590 non-null  float64\n",
      " 9   selfipa_done                    19590 non-null  int64  \n",
      " 10  zipregion_missing               19590 non-null  int64  \n",
      " 11  evaluationtime_missing          19590 non-null  int64  \n",
      " 12  desiredinstallationend_missing  19590 non-null  int64  \n",
      " 13  electricitybill_missing         19590 non-null  int64  \n",
      " 14  heatingbill_missing             19590 non-null  int64  \n",
      " 15  aggregated_missing              19590 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  19590 non-null  int64  \n",
      " 17  mktg_High                       19590 non-null  bool   \n",
      " 18  mktg_Low                        19590 non-null  bool   \n",
      " 19  mktg_Medium                     19590 non-null  bool   \n",
      " 20  region_High_Performer           19590 non-null  bool   \n",
      " 21  region_Large_Solid              19590 non-null  bool   \n",
      " 22  region_Lower                    19590 non-null  bool   \n",
      " 23  region_Medium                   19590 non-null  bool   \n",
      " 24  leadid                          19590 non-null  int64  \n",
      "dtypes: bool(7), float64(6), int64(12)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "behaviour.info()\n",
    "# id here is leadid, dataset is preselected. 38k leads\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c19f5",
   "metadata": {},
   "source": [
    "Adding behavioral data from Booking Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a7cb496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = final_df.merge(behaviour, \n",
    "                          left_on='leadid', \n",
    "                          right_on='id', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e2838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dc9f131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 44 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19590 non-null  int64  \n",
      " 1   gross_FU                        19590 non-null  int64  \n",
      " 2   gross_SC                        19590 non-null  int64  \n",
      " 3   net_FU                          19590 non-null  float64\n",
      " 4   net_SC                          19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19590 non-null  float64\n",
      " 6   electricitybill                 19590 non-null  float64\n",
      " 7   heatingbill                     19590 non-null  float64\n",
      " 8   grosscontractsigned             19590 non-null  float64\n",
      " 9   selfipa_done                    19590 non-null  int64  \n",
      " 10  zipregion_missing               19590 non-null  int64  \n",
      " 11  evaluationtime_missing          19590 non-null  int64  \n",
      " 12  desiredinstallationend_missing  19590 non-null  int64  \n",
      " 13  electricitybill_missing         19590 non-null  int64  \n",
      " 14  heatingbill_missing             19590 non-null  int64  \n",
      " 15  aggregated_missing              19590 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  19590 non-null  int64  \n",
      " 17  mktg_High                       19590 non-null  bool   \n",
      " 18  mktg_Low                        19590 non-null  bool   \n",
      " 19  mktg_Medium                     19590 non-null  bool   \n",
      " 20  region_High_Performer           19590 non-null  bool   \n",
      " 21  region_Large_Solid              19590 non-null  bool   \n",
      " 22  region_Lower                    19590 non-null  bool   \n",
      " 23  region_Medium                   19590 non-null  bool   \n",
      " 24  leadid                          19590 non-null  int64  \n",
      " 25  id                              19590 non-null  int64  \n",
      " 26  total_bc_attempts               19590 non-null  int64  \n",
      " 27  total_bc_outcomes               19590 non-null  int64  \n",
      " 28  lead_to_first_bc_days           19590 non-null  float64\n",
      " 29  bc_duration_days                19590 non-null  float64\n",
      " 30  bc_frequency                    19590 non-null  float64\n",
      " 31  positive_outcomes_count         19590 non-null  int64  \n",
      " 32  negative_outcomes_count         19590 non-null  int64  \n",
      " 33  noshow_outcomes_count           19590 non-null  int64  \n",
      " 34  positive_outcome_ratio          19590 non-null  float64\n",
      " 35  negative_outcome_ratio          19590 non-null  float64\n",
      " 36  noshow_outcome_ratio            19590 non-null  float64\n",
      " 37  reachability_score              19590 non-null  float64\n",
      " 38  outcome_trend                   19590 non-null  int64  \n",
      " 39  persistence_after_negative      19590 non-null  int64  \n",
      " 40  engagement_score                19590 non-null  float64\n",
      " 41  efficiency_score                19590 non-null  float64\n",
      " 42  last_bc_outcome_encoded         19590 non-null  int64  \n",
      " 43  first_bc_outcome_encoded        19590 non-null  int64  \n",
      "dtypes: bool(7), float64(15), int64(22)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()\n",
    "# all matched correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb3dc0",
   "metadata": {},
   "source": [
    "all the new extra contracts i added are getting droped with no match in leadtime. make sure that they get grabbed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fc1efb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = merged_df[merged_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n",
    "\n",
    "# --> loooks ok "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da2ee9",
   "metadata": {},
   "source": [
    "#### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3a919aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column pairs: []\n",
      "Original shape: (19590, 44)\n",
      "After removing duplicates: (19590, 44)\n"
     ]
    }
   ],
   "source": [
    "# Check your actual dataset for duplicates\n",
    "duplicate_cols = []\n",
    "cols = merged_df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if merged_df.equals(merged_df[cols[j]]):\n",
    "            duplicate_cols.append((cols[i], cols[j]))\n",
    "\n",
    "print(\"Duplicate column pairs:\", duplicate_cols)\n",
    "print(f\"Original shape: {merged_df.shape}\")\n",
    "\n",
    "# Remove duplicates if any found\n",
    "X_final_clean = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "print(f\"After removing duplicates: {X_final_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "947b90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame sizes:\n",
      "final_df: 19590 rows\n",
      "behaviour: 19981 rows\n",
      "merged_df: 19590 rows\n",
      "\n",
      "==================================================\n",
      "Rows with matches: 19590 (100.0%)\n",
      "Rows without matches: 0 (0.0%)\n",
      "\n",
      "==================================================\n",
      "Checking for duplicates in join keys:\n",
      "Duplicates in final_df['leadid']: 4151\n",
      "Duplicates in final_df['requestid']: 0\n",
      "Duplicates in behaviour['id']: 0\n",
      "\n",
      "==================================================\n",
      "Unique leadids in final_df: 15439\n",
      "Unique ids in behaviour: 19981\n",
      "Common IDs: 15439\n",
      "IDs only in final_df: 0\n",
      "IDs only in behaviour: 4542\n"
     ]
    }
   ],
   "source": [
    "# Check the original sizes\n",
    "print(\"Original DataFrame sizes:\")\n",
    "print(f\"final_df: {len(final_df)} rows\")\n",
    "print(f\"behaviour: {len(behaviour)} rows\")\n",
    "print(f\"merged_df: {len(merged_df)} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check how many matches we got\n",
    "matches = merged_df['id'].notna().sum()\n",
    "no_matches = merged_df['id'].isna().sum()\n",
    "\n",
    "print(f\"Rows with matches: {matches} ({matches/len(merged_df)*100:.1f}%)\")\n",
    "print(f\"Rows without matches: {no_matches} ({no_matches/len(merged_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for duplicates in the key columns before merge\n",
    "print(\"Checking for duplicates in join keys:\")\n",
    "print(f\"Duplicates in final_df['leadid']: {final_df['leadid'].duplicated().sum()}\")\n",
    "print(f\"Duplicates in final_df['requestid']: {final_df['requestid'].duplicated().sum()}\")\n",
    "print(f\"Duplicates in behaviour['id']: {behaviour['id'].duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check the overlap between the two key columns\n",
    "final_leadids = set(final_df['leadid'])\n",
    "behaviour_ids = set(behaviour['id'])\n",
    "\n",
    "print(f\"Unique leadids in final_df: {len(final_leadids)}\")\n",
    "print(f\"Unique ids in behaviour: {len(behaviour_ids)}\")\n",
    "print(f\"Common IDs: {len(final_leadids & behaviour_ids)}\")\n",
    "print(f\"IDs only in final_df: {len(final_leadids - behaviour_ids)}\")\n",
    "print(f\"IDs only in behaviour: {len(behaviour_ids - final_leadids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ab8a4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of 0 unmatched rows:\n",
      "==================================================\n",
      "Unmatched entries conversion rate: nan (nan%)\n",
      "Total conversions among unmatched: 0.0\n",
      "\n",
      "==================================================\n",
      "Matched entries conversion rate: 0.114 (11.4%)\n",
      "Total conversions among matched: 2225.0\n",
      "\n",
      "==================================================\n",
      "COMPARISON:\n",
      "Unmatched: nan (nan%)\n",
      "Matched:   0.114 (11.4%)\n",
      "Difference: nan percentage points\n"
     ]
    }
   ],
   "source": [
    "# checking if they are converters or not\n",
    "# Get the rows that didn't match (have NaN in the 'id' column from behaviour)\n",
    "unmatched_rows = merged_df[merged_df['id'].isna()]\n",
    "\n",
    "print(f\"Analysis of {len(unmatched_rows)} unmatched rows:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check conversion rate for unmatched entries\n",
    "conversion_rate_unmatched = unmatched_rows['grosscontractsigned'].mean()\n",
    "total_conversions_unmatched = unmatched_rows['grosscontractsigned'].sum()\n",
    "\n",
    "print(f\"Unmatched entries conversion rate: {conversion_rate_unmatched:.3f} ({conversion_rate_unmatched*100:.1f}%)\")\n",
    "print(f\"Total conversions among unmatched: {total_conversions_unmatched}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Compare with matched entries\n",
    "matched_rows = merged_df[merged_df['id'].notna()]\n",
    "conversion_rate_matched = matched_rows['grosscontractsigned'].mean()\n",
    "total_conversions_matched = matched_rows['grosscontractsigned'].sum()\n",
    "\n",
    "print(f\"Matched entries conversion rate: {conversion_rate_matched:.3f} ({conversion_rate_matched*100:.1f}%)\")\n",
    "print(f\"Total conversions among matched: {total_conversions_matched}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Overall comparison\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"Unmatched: {conversion_rate_unmatched:.3f} ({conversion_rate_unmatched*100:.1f}%)\")\n",
    "print(f\"Matched:   {conversion_rate_matched:.3f} ({conversion_rate_matched*100:.1f}%)\")\n",
    "print(f\"Difference: {(conversion_rate_matched - conversion_rate_unmatched)*100:.1f} percentage points\")\n",
    "\n",
    "# Statistical significance test (optional)\n",
    "from scipy import stats\n",
    "if len(unmatched_rows) > 0 and len(matched_rows) > 0:\n",
    "    stat, p_value = stats.chi2_contingency([[\n",
    "        unmatched_rows['grosscontractsigned'].sum(), \n",
    "        len(unmatched_rows) - unmatched_rows['grosscontractsigned'].sum()\n",
    "    ], [\n",
    "        matched_rows['grosscontractsigned'].sum(), \n",
    "        len(matched_rows) - matched_rows['grosscontractsigned'].sum()\n",
    "    ]])[:2]\n",
    "    \n",
    "    print(f\"\\nStatistical test p-value: {p_value:.6f}\")\n",
    "    print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "86b2c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19590 entries, 0 to 19589\n",
      "Data columns (total 44 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19590 non-null  int64  \n",
      " 1   gross_FU                        19590 non-null  int64  \n",
      " 2   gross_SC                        19590 non-null  int64  \n",
      " 3   net_FU                          19590 non-null  float64\n",
      " 4   net_SC                          19590 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19590 non-null  float64\n",
      " 6   electricitybill                 19590 non-null  float64\n",
      " 7   heatingbill                     19590 non-null  float64\n",
      " 8   grosscontractsigned             19590 non-null  float64\n",
      " 9   selfipa_done                    19590 non-null  int64  \n",
      " 10  zipregion_missing               19590 non-null  int64  \n",
      " 11  evaluationtime_missing          19590 non-null  int64  \n",
      " 12  desiredinstallationend_missing  19590 non-null  int64  \n",
      " 13  electricitybill_missing         19590 non-null  int64  \n",
      " 14  heatingbill_missing             19590 non-null  int64  \n",
      " 15  aggregated_missing              19590 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  19590 non-null  int64  \n",
      " 17  mktg_High                       19590 non-null  bool   \n",
      " 18  mktg_Low                        19590 non-null  bool   \n",
      " 19  mktg_Medium                     19590 non-null  bool   \n",
      " 20  region_High_Performer           19590 non-null  bool   \n",
      " 21  region_Large_Solid              19590 non-null  bool   \n",
      " 22  region_Lower                    19590 non-null  bool   \n",
      " 23  region_Medium                   19590 non-null  bool   \n",
      " 24  leadid                          19590 non-null  int64  \n",
      " 25  id                              19590 non-null  int64  \n",
      " 26  total_bc_attempts               19590 non-null  int64  \n",
      " 27  total_bc_outcomes               19590 non-null  int64  \n",
      " 28  lead_to_first_bc_days           19590 non-null  float64\n",
      " 29  bc_duration_days                19590 non-null  float64\n",
      " 30  bc_frequency                    19590 non-null  float64\n",
      " 31  positive_outcomes_count         19590 non-null  int64  \n",
      " 32  negative_outcomes_count         19590 non-null  int64  \n",
      " 33  noshow_outcomes_count           19590 non-null  int64  \n",
      " 34  positive_outcome_ratio          19590 non-null  float64\n",
      " 35  negative_outcome_ratio          19590 non-null  float64\n",
      " 36  noshow_outcome_ratio            19590 non-null  float64\n",
      " 37  reachability_score              19590 non-null  float64\n",
      " 38  outcome_trend                   19590 non-null  int64  \n",
      " 39  persistence_after_negative      19590 non-null  int64  \n",
      " 40  engagement_score                19590 non-null  float64\n",
      " 41  efficiency_score                19590 non-null  float64\n",
      " 42  last_bc_outcome_encoded         19590 non-null  int64  \n",
      " 43  first_bc_outcome_encoded        19590 non-null  int64  \n",
      "dtypes: bool(7), float64(15), int64(22)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f607eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original unmatched rows: 0\n",
      "Conversions among unmatched: 0.0\n",
      "Non-conversions among unmatched: 0\n",
      "\n",
      "After filtering unmatched rows:\n",
      "Keeping only unmatched conversions: 0\n",
      "\n",
      "Final merged_df:\n",
      "Matched rows (with behavior): 19590\n",
      "Unmatched conversions (without behavior): 0\n",
      "Total rows: 19590\n",
      "All remaining unmatched rows are conversions: True\n"
     ]
    }
   ],
   "source": [
    "# drop non converters with no behavioral data. keep the 7 that converted and recover their data manually\n",
    "# Get the unmatched rows (the 860 entries without behavior data)\n",
    "# Get the unmatched rows (the 860 entries without behavior data)\n",
    "unmatched_rows = merged_df[merged_df['id'].isna()]\n",
    "\n",
    "print(f\"Original unmatched rows: {len(unmatched_rows)}\")\n",
    "print(f\"Conversions among unmatched: {unmatched_rows['grosscontractsigned'].sum()}\")\n",
    "print(f\"Non-conversions among unmatched: {(unmatched_rows['grosscontractsigned'] == 0).sum()}\")\n",
    "\n",
    "# Filter unmatched rows to keep only conversions (grosscontractsigned == 1)\n",
    "unmatched_conversions = unmatched_rows[unmatched_rows['grosscontractsigned'] == 1]\n",
    "\n",
    "print(f\"\\nAfter filtering unmatched rows:\")\n",
    "print(f\"Keeping only unmatched conversions: {len(unmatched_conversions)}\")\n",
    "\n",
    "# Get the matched rows (the 13,296 entries with behavior data)\n",
    "matched_rows = merged_df[merged_df['id'].notna()]\n",
    "\n",
    "# Combine and replace merged_df\n",
    "merged_df = pd.concat([matched_rows, unmatched_conversions], ignore_index=True)\n",
    "\n",
    "print(f\"\\nFinal merged_df:\")\n",
    "print(f\"Matched rows (with behavior): {len(matched_rows)}\")\n",
    "print(f\"Unmatched conversions (without behavior): {len(unmatched_conversions)}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "print(f\"All remaining unmatched rows are conversions: {(merged_df[merged_df['id'].isna()]['grosscontractsigned'] == 1).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b8d6121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9          1387\n",
       "22         2039\n",
       "57         3668\n",
       "69         4202\n",
       "72         4359\n",
       "          ...  \n",
       "19543    126967\n",
       "19551    126980\n",
       "19552    126981\n",
       "19571    127538\n",
       "19579    128268\n",
       "Name: requestid, Length: 7310, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM THREE\n",
    "# see leads that didnt make it to sc --> le\n",
    "merged_df.loc[merged_df['net_SC'] == 0, 'requestid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7518fbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9          1387\n",
       "57         3668\n",
       "69         4202\n",
       "76         4502\n",
       "90         4967\n",
       "153        7227\n",
       "190        9286\n",
       "195        9600\n",
       "246       12136\n",
       "259       13082\n",
       "294       14602\n",
       "333       16694\n",
       "338       16872\n",
       "409       20096\n",
       "456       22490\n",
       "457       22538\n",
       "477       23302\n",
       "490       23673\n",
       "498       24071\n",
       "532       25455\n",
       "565       26629\n",
       "640       28791\n",
       "642       28884\n",
       "652       29149\n",
       "671       29752\n",
       "686       30361\n",
       "714       31401\n",
       "733       32048\n",
       "735       32121\n",
       "851       38499\n",
       "903       40431\n",
       "927       41579\n",
       "942       42034\n",
       "945       42088\n",
       "955       42424\n",
       "1013      46146\n",
       "1024      46888\n",
       "1067      48878\n",
       "1073      49142\n",
       "1143      53645\n",
       "1350      66892\n",
       "1739      88220\n",
       "2158     106390\n",
       "2159     106391\n",
       "4694     109781\n",
       "7581     113297\n",
       "7683     113414\n",
       "9411     115357\n",
       "9628     115615\n",
       "11966    118332\n",
       "14474    121188\n",
       "19005    126300\n",
       "19571    127538\n",
       "19579    128268\n",
       "Name: requestid, dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of 7k no net sc (but yess gross) 52 are converted could it be after the cutoff?\n",
    "x = merged_df.loc[\n",
    "    (merged_df['net_SC'] == 0) & (merged_df['grosscontractsigned'] == 1),\n",
    "    'requestid'\n",
    "]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9adf4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to investigate\n",
    "# ids\n",
    "# 1387, 3668, 4202, 4502, 4967, 7227, 9286, 9600, 12136, 13082, 14602, 16694, 16872, 20096, 22490, 22538, 23302, 23673, 24071, 25455, 26629, 28791, 28884, 29149, 29752, 30361, 31401, 32048, 32121, 38499, 40431, 41579, 42034, 42088, 42424, 46146, 46888, 48878, 49142, 53645, 66892, 88220, 106390, 106391, 109781, 113297, 113414, 115357, 115615, 118332, 121188, 126300, 127538, 128268\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bda973",
   "metadata": {},
   "source": [
    "they really dont have vd or vdfield that is not deleted --> sus entries. you can look into them later but for now lets drop only sc_net = 0 and grosscontractsigned = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "dc568ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop the ones who havent made to net sc\n",
    "# Keep rows where not ( net_SC is equal to 0 and grosscontractsigned is 1)\n",
    "merged_df = merged_df[~((merged_df['net_SC'] == 0) & (merged_df['grosscontractsigned'] == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5e5749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop requestid ad other indicatiors\n",
    "merged_df = merged_df.drop(columns=['requestid', 'id', 'leadid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "64a85535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversions in missing rows: 0.0\n",
      "Conversion rate in missing rows: nan\n",
      "Conversion rate in complete rows: 0.1111\n"
     ]
    }
   ],
   "source": [
    "# Check conversions in missing call data rows\n",
    "missing_mask = merged_df['total_bc_attempts'].isnull()\n",
    "print(f\"Conversions in missing rows: {merged_df[missing_mask]['grosscontractsigned'].sum()}\")\n",
    "print(f\"Conversion rate in missing rows: {merged_df[missing_mask]['grosscontractsigned'].mean():.4f}\")\n",
    "print(f\"Conversion rate in complete rows: {merged_df[~missing_mask]['grosscontractsigned'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b0fc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these requests are very old and dont have propre registstration, some have some registration but essentially there are only 7 contract signed insode 860, so essentially dropping as a first step of undersampling.\n",
    "# ill keep 7 positives and recover their data, drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5c84629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (19536, 41)\n",
      "New conversion rate: 0.1111\n",
      "Total conversions kept: 2171.0\n"
     ]
    }
   ],
   "source": [
    "# this would drop all entries with any missing values, so lets hold our horses here. \n",
    "#merged_df = merged_df.dropna().copy()\n",
    "print(f\"New dataset shape: {merged_df.shape}\")\n",
    "print(f\"New conversion rate: {merged_df['grosscontractsigned'].mean():.4f}\")\n",
    "print(f\"Total conversions kept: {merged_df['grosscontractsigned'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4526a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create subfolder if it doesn't exist\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# Save your merged dataframe\n",
    "merged_df.to_csv('processed_data/merged_df.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ad60cd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grosscontractsigned\n",
       "0.0    17365\n",
       "1.0     2171\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"grosscontractsigned\"].value_counts() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
