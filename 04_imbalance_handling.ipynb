{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbf54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dff50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_data/merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923bac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "\n",
    "target_column = 'netcontractsigned'\n",
    "X = df.drop(target_column, axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler() # important for logistic regressor\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a simple logistic regression (NO imbalance handling yet)\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Check if there's any signal\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n=== SIGNAL TEST RESULTS ===\")\n",
    "print(f\"AUC-ROC Score: {auc_score:.3f}\")\n",
    "if auc_score > 0.5:\n",
    "    print(\"✅ GOOD NEWS: Your features contain useful signal!\")\n",
    "    print(\"We can proceed with imbalance fixing strategies.\")\n",
    "else:\n",
    "    print(\"❌ BAD NEWS: No signal detected. Features need work first.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate AUC-PR (more reliable for imbalanced data)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "# Random baseline for comparison\n",
    "random_baseline = y_test.mean()  # 0.023\n",
    "\n",
    "print(\"\\n=== DETAILED SIGNAL ANALYSIS ===\")\n",
    "print(f\"AUC-ROC: {auc_score:.3f}\")\n",
    "print(f\"AUC-PR: {auc_pr:.3f}\")\n",
    "print(f\"Random baseline AUC-PR: {random_baseline:.3f}\")\n",
    "print(f\"Improvement over random: {auc_pr/random_baseline:.1f}x\")\n",
    "\n",
    "# Look at actual predictions with default 0.5 threshold\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "print(f\"\\nWith 0.5 threshold:\")\n",
    "print(f\"Predicted positives: {y_pred.sum()}\")\n",
    "print(f\"Actual positives: {y_test.sum()}\")\n",
    "\n",
    "# Check top 10% of predictions\n",
    "top_10_percent_threshold = np.percentile(y_pred_proba, 90)\n",
    "print(f\"\\nTop 10% analysis:\")\n",
    "print(f\"Threshold for top 10%: {top_10_percent_threshold:.3f}\")\n",
    "top_10_mask = y_pred_proba >= top_10_percent_threshold\n",
    "print(f\"Conversion rate in top 10%: {y_test[top_10_mask].mean():.3f}\")\n",
    "print(f\"That's {y_test[top_10_mask].mean()/y_test.mean():.1f}x better than average!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f758cb4",
   "metadata": {},
   "source": [
    "#### First Strategy\n",
    "\n",
    "Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7e989",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate AUC-PR (more reliable for imbalanced data)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m auc_pr = auc(\u001b[43mrecall\u001b[49m, precision)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Test class weights with your strong signal\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== TESTING CLASS WEIGHTS ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'recall' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test class weights with your strong signal\n",
    "print(\"\\n=== TESTING CLASS WEIGHTS ===\")\n",
    "\n",
    "# Calculate balanced class weight\n",
    "pos_weight = (1 - y_train.mean()) / y_train.mean()  # About 42 for your data\n",
    "print(f\"Calculated positive class weight: {pos_weight:.1f}\")\n",
    "\n",
    "# Train with class weights\n",
    "lr_weighted = LogisticRegression(\n",
    "    class_weight={0: 1, 1: pos_weight},\n",
    "    random_state=42, \n",
    "    max_iter=1000\n",
    ")\n",
    "lr_weighted.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_weighted = lr_weighted.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "precision_w, recall_w, _ = precision_recall_curve(y_test, y_pred_weighted)\n",
    "auc_pr_weighted = auc(recall_w, precision_w)\n",
    "auc_roc_weighted = roc_auc_score(y_test, y_pred_weighted)\n",
    "\n",
    "print(f\"\\nClass Weights Results:\")\n",
    "print(f\"AUC-ROC: {auc_roc_weighted:.3f}\")\n",
    "print(f\"AUC-PR: {auc_pr_weighted:.3f}\")\n",
    "print(f\"Improvement over baseline: {auc_pr_weighted/auc_pr:.2f}x\")\n",
    "\n",
    "# Check different thresholds\n",
    "for thresh in [0.1, 0.2, 0.3]:\n",
    "    pred_at_thresh = (y_pred_weighted >= thresh).astype(int)\n",
    "    if pred_at_thresh.sum() > 0:\n",
    "        precision_at_thresh = (y_test[pred_at_thresh == 1]).mean()\n",
    "        print(f\"At {thresh} threshold: {pred_at_thresh.sum()} predicted positive, {precision_at_thresh:.3f} precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f549f2",
   "metadata": {},
   "source": [
    "Class weights actually made things slightly worse (AUC-PR dropped from 0.174 to 0.158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the difference thresholds make\n",
    "\n",
    "# Use the original (better) model predictions\n",
    "print(\"=== THRESHOLD OPTIMIZATION ===\")\n",
    "print(\"Using original model (no class weights)\")\n",
    "\n",
    "# Try many different thresholds\n",
    "thresholds = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    pred_at_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "    \n",
    "    if pred_at_thresh.sum() > 0:\n",
    "        precision = (y_test[pred_at_thresh == 1]).mean()\n",
    "        recall = (y_test * pred_at_thresh).sum() / y_test.sum()\n",
    "        \n",
    "        if precision > 0 and recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "            \n",
    "        results.append({\n",
    "            'threshold': thresh,\n",
    "            'predicted_positive': pred_at_thresh.sum(),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'percent_flagged': pred_at_thresh.sum() / len(y_test) * 100\n",
    "        })\n",
    "\n",
    "# Show results\n",
    "for r in results:\n",
    "    print(f\"Threshold {r['threshold']:.2f}: {r['predicted_positive']:4d} flagged ({r['percent_flagged']:4.1f}%), \"\n",
    "          f\"Precision {r['precision']:.3f}, Recall {r['recall']:.3f}, F1 {r['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594fb36a",
   "metadata": {},
   "source": [
    "#### Second Strategy\n",
    "\n",
    "Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_t = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        focal_loss = alpha_t * tf.pow((1 - p_t), self.gamma) * ce_loss\n",
    "        return tf.reduce_mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what metrics are actually available\n",
    "print(\"Available metrics:\", list(history.history.keys()))\n",
    "\n",
    "# Get predictions and calculate F1\n",
    "val_pred_probs = model.predict(X_val)\n",
    "val_pred_binary = (val_pred_probs > 0.07).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "f1 = f1_score(y_val, val_pred_binary)\n",
    "precision = precision_score(y_val, val_pred_binary)\n",
    "recall = recall_score(y_val, val_pred_binary)\n",
    "auc = roc_auc_score(y_val, val_pred_probs)\n",
    "\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce4819",
   "metadata": {},
   "source": [
    "#### Third Strategy\n",
    "\n",
    "Smoteen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e261b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SMOTE-ENN\n",
    "smote_enn = SMOTEENN(\n",
    "    smote=SMOTE(random_state=42, k_neighbors=3),  # Reduced neighbors for small minority class\n",
    "    enn=EditedNearestNeighbours(n_neighbors=3),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply resampling\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Before SMOTE-ENN: {X_train_scaled.shape}, {y_train.value_counts().to_dict()}\")\n",
    "print(f\"After SMOTE-ENN: {X_train_resampled.shape}, {pd.Series(y_train_resampled).value_counts().to_dict()}\")\n",
    "\n",
    "# Calculate the change\n",
    "original_pos = y_train.sum()\n",
    "resampled_pos = pd.Series(y_train_resampled).sum()\n",
    "original_neg = len(y_train) - original_pos\n",
    "resampled_neg = len(y_train_resampled) - resampled_pos\n",
    "\n",
    "print(f\"\\nPositive samples: {original_pos} → {resampled_pos} (added {resampled_pos - original_pos})\")\n",
    "print(f\"Negative samples: {original_neg} → {resampled_neg} (removed {original_neg - resampled_neg})\")\n",
    "\n",
    "# 5. CREATE MODEL\n",
    "def create_model(input_dim, dropout_rate=0.3):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall', 'AUC']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6. TRAIN MODEL WITH SMOTE-ENN DATA\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING MODEL WITH SMOTE-ENN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_smoteenn = create_model(X_train_resampled.shape[1])\n",
    "\n",
    "# Train on resampled data\n",
    "history_smoteenn = model_smoteenn.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,  # Split from resampled data\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7. EVALUATE ON ORIGINAL TEST SET\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION ON ORIGINAL TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "y_test_pred_proba = model_smoteenn.predict(X_test_scaled)\n",
    "y_test_pred_proba = y_test_pred_proba.flatten()\n",
    "\n",
    "# Calculate PR-AUC and ROC-AUC\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_test_pred_proba)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "random_baseline = y_test.sum() / len(y_test)\n",
    "\n",
    "print(f\"AUC-ROC: {roc_auc:.3f}\")\n",
    "print(f\"AUC-PR: {pr_auc:.3f}\")\n",
    "print(f\"Random baseline AUC-PR: {random_baseline:.3f}\")\n",
    "print(f\"Improvement over random: {pr_auc/random_baseline:.1f}x\")\n",
    "\n",
    "# 8. THRESHOLD ANALYSIS (focusing on recall for your use case)\n",
    "print(f\"\\n=== THRESHOLD ANALYSIS (RECALL-FOCUSED) ===\")\n",
    "thresholds = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.15, 0.20, 0.30, 0.50]\n",
    "\n",
    "best_recall = 0\n",
    "best_threshold = 0\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_test_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    if y_pred.sum() > 0:  # Avoid division by zero\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        flagged_count = y_pred.sum()\n",
    "        flagged_pct = flagged_count / len(y_test) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'flagged': flagged_count,\n",
    "            'flagged_pct': flagged_pct,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"Threshold {threshold:.2f}: {flagged_count:4d} flagged ({flagged_pct:4.1f}%), \"\n",
    "              f\"Precision {precision:.3f}, Recall {recall:.3f}, F1 {f1:.3f}\")\n",
    "        \n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            best_threshold = threshold\n",
    "\n",
    "print(f\"\\nBest threshold for recall: {best_threshold} (Recall: {best_recall:.3f})\")\n",
    "\n",
    "# 9. COMPARE WITH YOUR PREVIOUS RESULTS\n",
    "print(f\"\\n=== COMPARISON WITH PREVIOUS APPROACHES ===\")\n",
    "print(\"SMOTE-ENN vs Previous Results:\")\n",
    "print(f\"Best recall achieved: {best_recall:.3f} (vs ~0.35 from class weights)\")\n",
    "print(f\"AUC-PR: {pr_auc:.3f} (vs 0.174 from no modification)\")\n",
    "\n",
    "# 10. TOP PERFORMERS ANALYSIS\n",
    "results_df = pd.DataFrame(results)\n",
    "# Sort by recall (since that's your priority)\n",
    "top_recall = results_df.nlargest(5, 'recall')\n",
    "print(f\"\\nTop 5 thresholds by RECALL (your priority):\")\n",
    "print(top_recall[['threshold', 'flagged_pct', 'precision', 'recall', 'f1']].to_string(index=False))\n",
    "\n",
    "# Also show balanced options\n",
    "results_df['recall_precision_product'] = results_df['recall'] * results_df['precision']\n",
    "balanced_options = results_df.nlargest(5, 'recall_precision_product')\n",
    "print(f\"\\nTop 5 balanced recall-precision options:\")\n",
    "print(balanced_options[['threshold', 'flagged_pct', 'precision', 'recall', 'f1']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"SMOTE-ENN ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(\"Key takeaways:\")\n",
    "print(f\"1. SMOTE-ENN created {resampled_pos - original_pos} synthetic positive samples\")\n",
    "print(f\"2. Best recall achieved: {best_recall:.3f}\")\n",
    "print(f\"3. For your use case (capture more buyers), consider threshold around {best_threshold}\")\n",
    "print(\"4. Ready to compare with ensemble methods next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780af1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
