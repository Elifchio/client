{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bd0fa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57495af0",
   "metadata": {},
   "source": [
    "copy of notebook 02 to chnange the marketing logic, also datasetes now have the 2 marketing columns instead of the old mktgparams one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "287d505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc bigger sample (calendarevent logs) \n",
    "path= r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\reroute, more samples\\processed_data\\sc_biigger_sample.csv\"\n",
    "df1 = pd.read_csv(path)\n",
    "df1.columns = df1.columns.str.lower()\n",
    "\n",
    "#sc_ older contracts (calendarevent)\n",
    "path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\sc_older_contracts.csv\"\n",
    "df2 = pd.read_csv(path)\n",
    "df2.columns = df2.columns.str.lower()\n",
    "older = df2[\"requestid\"].tolist()\n",
    "\n",
    "#concat\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = combined_df.copy()\n",
    "\n",
    "# qualitative (slider + contract)\n",
    "\n",
    "file2 = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\new data\\info_client2.csv\"\n",
    "\n",
    "df3 = pd.read_csv(file2)\n",
    "df3.columns = df3.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "214d9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "older = list(set(older))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "18565e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(older)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a0bce55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\reroute, more samples\\processed_data\\df_model_new.csv\"\n",
    "behaviour = pd.read_csv(path) # manualy saved leadtime info. hafter having gotten modified in older attempts/notebook\n",
    "behaviour = behaviour.drop(['converted', 'lead_to_sc1_days', 'sc1_schedule_to_appointment_days', 'showed_up_sc1' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9c02e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151441 entries, 0 to 151440\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   id                          151441 non-null  int64  \n",
      " 1   total_bc_attempts           151441 non-null  int64  \n",
      " 2   total_bc_outcomes           151441 non-null  int64  \n",
      " 3   lead_to_first_bc_days       151441 non-null  float64\n",
      " 4   bc_duration_days            151441 non-null  float64\n",
      " 5   bc_frequency                151441 non-null  float64\n",
      " 6   positive_outcomes_count     151441 non-null  int64  \n",
      " 7   negative_outcomes_count     151441 non-null  int64  \n",
      " 8   noshow_outcomes_count       151441 non-null  int64  \n",
      " 9   positive_outcome_ratio      151441 non-null  float64\n",
      " 10  negative_outcome_ratio      151441 non-null  float64\n",
      " 11  noshow_outcome_ratio        151441 non-null  float64\n",
      " 12  reachability_score          151441 non-null  float64\n",
      " 13  outcome_trend               151441 non-null  int64  \n",
      " 14  persistence_after_negative  151441 non-null  int64  \n",
      " 15  engagement_score            151441 non-null  float64\n",
      " 16  efficiency_score            151441 non-null  float64\n",
      " 17  last_bc_outcome_encoded     151441 non-null  int64  \n",
      " 18  first_bc_outcome_encoded    151441 non-null  int64  \n",
      "dtypes: float64(9), int64(10)\n",
      "memory usage: 22.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "behaviour.columns = behaviour.columns.str.lower()\n",
    "behaviour.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9c09a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AD HOC\n",
    "# trying to match the positive contracts to leadtime\n",
    "#try1 = df2.merge(behaviour, left_on='leadid', right_on='id', how='left', indicator=True)\n",
    "#df2.info()\n",
    "#print(df2[\"leadid\"].nunique())\n",
    "#try1.info()\n",
    "#print(try1[\"id\"].nunique())\n",
    "#print(behaviour[\"id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ea6428ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 115435 entries, 0 to 115434\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   createdat  115435 non-null  object \n",
      " 1   deletedat  51091 non-null   object \n",
      " 2   requestid  114203 non-null  float64\n",
      " 3   tsstart    115435 non-null  object \n",
      " 4   type       115435 non-null  object \n",
      " 5   is_passed  115435 non-null  int64  \n",
      " 6   leadid     114203 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 6.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30527"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 83346 sc logs of 28193 unique requests\n",
    "combined_df.info()\n",
    "combined_df[\"requestid\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e3fb0750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126489 entries, 0 to 126488\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   requestid               126489 non-null  int64  \n",
      " 1   zipregion               123407 non-null  object \n",
      " 2   evaluationtime          119790 non-null  object \n",
      " 3   desiredinstallationend  119870 non-null  object \n",
      " 4   electricitybill         105858 non-null  float64\n",
      " 5   heatingbill             92462 non-null   float64\n",
      " 6   channel__campaign       126489 non-null  object \n",
      " 7   channel                 125384 non-null  object \n",
      " 8   grosscontractsigned     2795 non-null    float64\n",
      " 9   selfipaimportedat       2632 non-null    object \n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 9.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grosscontractsigned\n",
       "1.0    2746\n",
       "0.0      49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# behavioral data that has 2.7k positive examples\n",
    "df3.info()\n",
    "df3[\"grosscontractsigned\"].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1648b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requestids with duplicates: 52\n",
      "Total duplicate rows: 52\n",
      "\n",
      "Dropped 5 absolute duplicate rows\n",
      "Remaining duplicate requestids before grosscontractsigned cleanup: 47\n",
      "Dropped 47 rows where grosscontractsigned = 0 from duplicate requestids\n",
      "\n",
      "==================================================\n",
      "FINAL DUPLICATE CHECK\n",
      "==================================================\n",
      "Number of requestids with duplicates: 0\n",
      "Total duplicate rows: 0\n",
      "âœ… No remaining duplicates!\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates in df3 -info client-,they were found in notebook 01. probably because same requestid have 2 entries in airtable contracts, one 0 one 1. we wanna keep only 1s.\n",
    "\n",
    "# Step 1: Count requestids that have duplicates\n",
    "duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df3['requestid'].duplicated().sum()}\")\n",
    "\n",
    "# Step 2: Drop absolute duplicates (excluding leadid column)\n",
    "comparison_cols = [col for col in df3.columns if col != 'leadid']\n",
    "df3_before = len(df3)\n",
    "df3 = df3.drop_duplicates(subset=comparison_cols, keep='first')\n",
    "df3_after = len(df3)\n",
    "\n",
    "print(f\"\\nDropped {df3_before - df3_after} absolute duplicate rows\")\n",
    "\n",
    "# Step 3: From the remaining duplicate requestids, drop rows where grosscontractsigned = 0\n",
    "remaining_duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "\n",
    "if len(remaining_duplicate_requestids) > 0:\n",
    "    print(f\"Remaining duplicate requestids before grosscontractsigned cleanup: {len(remaining_duplicate_requestids)}\")\n",
    "    \n",
    "    # Create condition: either NOT a duplicate requestid, OR grosscontractsigned != 0\n",
    "    condition = (~df3['requestid'].isin(remaining_duplicate_requestids)) | (df3['grosscontractsigned'] != 0)\n",
    "    \n",
    "    df3_before_net_drop = len(df3)\n",
    "    df3 = df3[condition]\n",
    "    df3_after_net_drop = len(df3)\n",
    "    \n",
    "    print(f\"Dropped {df3_before_net_drop - df3_after_net_drop} rows where grosscontractsigned = 0 from duplicate requestids\")\n",
    "\n",
    "# Step 4: Final duplicate check\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL DUPLICATE CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(final_duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df3['requestid'].duplicated().sum()}\")\n",
    "\n",
    "if len(final_duplicate_requestids) > 0:\n",
    "    # Check what columns still differ\n",
    "    all_differing_columns = set()\n",
    "    for requestid in final_duplicate_requestids:\n",
    "        group = df3[df3['requestid'] == requestid]\n",
    "        for col in df3.columns:\n",
    "            if col != 'requestid' and group[col].nunique() > 1:\n",
    "                all_differing_columns.add(col)\n",
    "    \n",
    "    print(f\"Differing columns: {sorted(all_differing_columns)}\")\n",
    "else:\n",
    "    print(f\"âœ… No remaining duplicates!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "53976aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grosscontractsigned\n",
       "1.0    2741\n",
       "0.0       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"grosscontractsigned\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353766",
   "metadata": {},
   "source": [
    "#### Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d1690b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing appointment types\n",
    "\n",
    "type_mapping = {\n",
    "    'FU': 'FU',\n",
    "    'FUVD': 'FU', \n",
    "    'REMINDER': 'FU',\n",
    "    'VDFIELD': 'SC',\n",
    "    'VD': 'SC'\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "df['grouped_type'] = df['type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ad51c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition for net events (not cancelled and past)\n",
    "df['is_net_event'] = df['deletedat'].isna() & df['is_passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "05bbb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross counts (all events)\n",
    "gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "# Net counts (only events that actually happened)\n",
    "net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "net_counts.columns = [f'net_{col}' for col in net_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "511b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gross and net counts\n",
    "counts_df = gross_counts.join(net_counts, how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b102cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8011647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn self ipa event to bbinary \n",
    "df3['selfipa_done'] = df3['selfipaimportedat'].notnull().astype(int)\n",
    "df3.drop('selfipaimportedat', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "867c26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  handle marketing\n",
    "# new marketing source\n",
    "\n",
    "\n",
    "def create_aggregated_column(df):\n",
    "    \"\"\"\n",
    "    Creates the 'Aggregated' column based on Channel and CHANNEL__CAMPAIGN values\n",
    "    \"\"\"\n",
    "    def get_aggregated_value(row):\n",
    "        # Check Youtube first\n",
    "        if row.get('Youtube') == 1:\n",
    "            return \"Youtube\"\n",
    "        \n",
    "        # Check specific CHANNEL__CAMPAIGN values\n",
    "        channel_campaign = row.get('channel_campaign')\n",
    "        if channel_campaign == \"d2d\":\n",
    "            return \"d2d\"\n",
    "        elif channel_campaign == \"form_classico\":\n",
    "            return \"Form_FB\"\n",
    "        elif channel_campaign == \"ranger\":\n",
    "            return \"ranger\"\n",
    "        elif channel_campaign == \"referral_link\":\n",
    "            return \"Referral\"\n",
    "        elif channel_campaign == \"referral_manual\":\n",
    "            return \"Referral\"\n",
    "        \n",
    "        # Check Channel column (case insensitive)\n",
    "        channel = row.get('channel')\n",
    "        if channel and pd.notna(channel):\n",
    "            channel_lower = channel.lower()\n",
    "            \n",
    "            if channel_lower == \"affiliation\":\n",
    "                return \"Affiliation\"\n",
    "            elif channel_lower == \"outbrain\":\n",
    "                return \"Outbrain\"\n",
    "            elif channel_lower in [\"taboola\", \"taboola_it\"]:\n",
    "                return \"Taboola\"\n",
    "            elif channel_lower in [\"facebook\", \"fb\", \"ig\", \"instagram\", \"facebook_marketplace\"]:\n",
    "                return \"Meta\"\n",
    "            elif channel_lower in [\"search ads\", \"searchads\", \"google\", \"google ads\"]:\n",
    "                return \"Google\"\n",
    "            elif channel_lower in [\"organic_brand\", \"organic\", \"direct\"]:\n",
    "                return \"Organic\"\n",
    "            elif channel_lower == \"seo\":\n",
    "                return \"SEO\"\n",
    "            elif channel_lower == \"criteo\":\n",
    "                return \"Criteo\"\n",
    "            elif channel_lower == \"mediago\":\n",
    "                return \"MediaGo\"\n",
    "            elif channel_lower == \"tiktok\":\n",
    "                return \"Tik Tok\"\n",
    "        \n",
    "        return \"Other\"\n",
    "    \n",
    "    df['aggregated'] = df.apply(get_aggregated_value, axis=1)\n",
    "    df.drop('channel', axis=1, inplace=True)\n",
    "    df.drop('channel__campaign', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1af4e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =   create_aggregated_column(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "399ef2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 126437 entries, 0 to 126488\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   requestid               126437 non-null  int64  \n",
      " 1   zipregion               123357 non-null  object \n",
      " 2   evaluationtime          119738 non-null  object \n",
      " 3   desiredinstallationend  119818 non-null  object \n",
      " 4   electricitybill         105809 non-null  float64\n",
      " 5   heatingbill             92430 non-null   float64\n",
      " 6   grosscontractsigned     2743 non-null    float64\n",
      " 7   selfipa_done            126437 non-null  int64  \n",
      " 8   aggregated              126437 non-null  object \n",
      "dtypes: float64(3), int64(2), object(4)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6096a79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregated\n",
       "Meta           41212\n",
       "Outbrain       27213\n",
       "Google         24548\n",
       "Other           9295\n",
       "Organic         7771\n",
       "Taboola         5172\n",
       "Affiliation     3987\n",
       "Tik Tok         3799\n",
       "MediaGo         1957\n",
       "Criteo          1306\n",
       "SEO              177\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"aggregated\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9be656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert createdat to datetime\n",
    "df['createdat'] = pd.to_datetime(df['createdat'])\n",
    "\n",
    "# Step 2: Get first gross SC and first net FU timestamps\n",
    "first_gross_sc = df[df['grouped_type'] == 'SC'].groupby('requestid')['createdat'].min()\n",
    "first_net_fu = df[(df['grouped_type'] == 'FU') & df['is_net_event']].groupby('requestid')['createdat'].min()\n",
    "\n",
    "# Step 3: Calculate time difference in hours\n",
    "counts_df['time_first_sc_to_first_net_fu'] = (first_net_fu - first_gross_sc).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "88cecdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gross_SC\n",
       "1     19938\n",
       "2      4329\n",
       "3      1034\n",
       "4       352\n",
       "5        77\n",
       "6        33\n",
       "7        12\n",
       "8         3\n",
       "10        3\n",
       "23        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop older requests that havent had a sc since march\n",
    "counts_df = counts_df[counts_df['gross_SC'] != 0]\n",
    "counts_df['gross_SC'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9d420799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (25782, 5)\n",
      "No missing values: True\n"
     ]
    }
   ],
   "source": [
    "# handle missing value for customers who never made to fu\n",
    "counts_df['time_first_sc_to_first_net_fu'] = counts_df['time_first_sc_to_first_net_fu'].fillna(-1)\n",
    "\n",
    "# Check final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {counts_df.shape}\")\n",
    "print(f\"No missing values: {counts_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c23d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negative time differences (not NaN fills): 25\n",
      "Positive time differences: 11848\n",
      "NaN fills (-1): 13909\n",
      "\n",
      "Real negative values range: -4701.88 to -4.83 hours\n"
     ]
    }
   ],
   "source": [
    "# problematic entries with negative time difference: \n",
    "# Separate real negative times from NaN fill values\n",
    "real_negative = counts_df[\n",
    "    (counts_df['time_first_sc_to_first_net_fu'] < 0) & \n",
    "    (counts_df['time_first_sc_to_first_net_fu'] != -1)\n",
    "]\n",
    "\n",
    "actual_valid_times = counts_df[counts_df['time_first_sc_to_first_net_fu'] >= 0]\n",
    "\n",
    "print(f\"Actual negative time differences (not NaN fills): {len(real_negative)}\")\n",
    "print(f\"Positive time differences: {len(actual_valid_times)}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "if len(real_negative) > 0:\n",
    "    print(f\"\\nReal negative values range: {real_negative['time_first_sc_to_first_net_fu'].min():.2f} to {real_negative['time_first_sc_to_first_net_fu'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "06a370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle negative time difference\n",
    "# Remove outliers entirely (set to NaN, then fill with -1)\n",
    "counts_df.loc[counts_df['time_first_sc_to_first_net_fu'] < -1, 'time_first_sc_to_first_net_fu'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "aec7cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing outliers:\n",
      "Negative values (excluding -1 fills): 0\n",
      "Valid positive time differences: 11809\n",
      "NaN fills (-1): 13934\n",
      "\n",
      "Final dataset shape: (25782, 5)\n",
      "Ready to join with target variable!\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Negative values (excluding -1 fills): {((counts_df['time_first_sc_to_first_net_fu'] < 0) & (counts_df['time_first_sc_to_first_net_fu'] != -1)).sum()}\")\n",
    "print(f\"Valid positive time differences: {(counts_df['time_first_sc_to_first_net_fu'] > 0).sum()}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {counts_df.shape}\")\n",
    "print(\"Ready to join with target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "98124e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1191.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1258.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1271.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1285.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requestid  gross_FU  gross_SC  net_FU  net_SC  \\\n",
       "0     1191.0         0         1     0.0     1.0   \n",
       "1     1258.0         0         1     0.0     1.0   \n",
       "2     1271.0         0         1     0.0     1.0   \n",
       "3     1285.0         1         1     1.0     1.0   \n",
       "4     1294.0         0         1     0.0     1.0   \n",
       "\n",
       "   time_first_sc_to_first_net_fu  \n",
       "0                           -1.0  \n",
       "1                           -1.0  \n",
       "2                           -1.0  \n",
       "3                            0.0  \n",
       "4                           -1.0  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point requestid is the index. make it explicit\n",
    "\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9b72fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [requestid, gross_FU, gross_SC, net_FU, net_SC, time_first_sc_to_first_net_fu]\n",
       "Index: []"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see net_sc accuracy\n",
    "mask1 = counts_df[counts_df['gross_SC'] == 0]\n",
    "mask1\n",
    "# 7k didnt make it to net sc but has grosssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e76056e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 6 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   requestid                      25782 non-null  float64\n",
      " 1   gross_FU                       25782 non-null  int64  \n",
      " 2   gross_SC                       25782 non-null  int64  \n",
      " 3   net_FU                         25782 non-null  float64\n",
      " 4   net_SC                         25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu  25782 non-null  float64\n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "counts_df.info()\n",
    "#leadid not present here, we moved to 19.5 k distinct requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dd583cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# requestid duplicates? --> no, it was index anyways\n",
    "# Show which requestid values appear more than once\n",
    "duplicate_requestids = counts_df[counts_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dba7e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# also df2 has no duplicates also because we did prviously checked and handled\n",
    "duplicate_requestids = df3[df3['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b02d0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (25782, 14)\n",
      "Target variable distribution:\n",
      "grosscontractsigned\n",
      "0.0    23509\n",
      "1.0     2273\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['requestid', 'gross_FU', 'gross_SC', 'net_FU', 'net_SC', 'time_first_sc_to_first_net_fu', 'zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'grosscontractsigned', 'selfipa_done', 'aggregated']\n"
     ]
    }
   ],
   "source": [
    "# add qualitative data\n",
    "# now we have 17k 0 2.2 k 1. where did 300+ 1s went? maybe they were from requests that never had a sc, we dropped them previously\n",
    "final_df = counts_df.merge(df3, on='requestid', how='left')\n",
    "\n",
    "\n",
    "# Fill non-matching requestids with 0 for the target\n",
    "final_df[\"grosscontractsigned\"] = final_df[\"grosscontractsigned\"].fillna(0)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(final_df[\"grosscontractsigned\"].value_counts())\n",
    "print(f\"\\nColumns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6cde1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d59ece8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 14 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   requestid                      25782 non-null  float64\n",
      " 1   gross_FU                       25782 non-null  int64  \n",
      " 2   gross_SC                       25782 non-null  int64  \n",
      " 3   net_FU                         25782 non-null  float64\n",
      " 4   net_SC                         25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu  25782 non-null  float64\n",
      " 6   zipregion                      24760 non-null  object \n",
      " 7   evaluationtime                 25504 non-null  object \n",
      " 8   desiredinstallationend         25506 non-null  object \n",
      " 9   electricitybill                16595 non-null  float64\n",
      " 10  heatingbill                    7963 non-null   float64\n",
      " 11  grosscontractsigned            25782 non-null  float64\n",
      " 12  selfipa_done                   25756 non-null  float64\n",
      " 13  aggregated                     25756 non-null  object \n",
      "dtypes: float64(8), int64(2), object(4)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f59a1d",
   "metadata": {},
   "source": [
    "#### Merging with behaviour from leadtime might happen here so null handling and encoding is not repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38cbb0",
   "metadata": {},
   "source": [
    "#### Encoding and Null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "452d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of bills have -1, 1 and 0\n",
    "\n",
    "# set wrong bill values to nan\n",
    "# Replace specific values with NaN\n",
    "final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea01f",
   "metadata": {},
   "source": [
    "Optional for bill handling \n",
    "def clean_billing_column(series, column_name):\n",
    "    \"\"\"Clean billing column: set negative values and extreme outliers to replacement value\"\"\"\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    series_clean = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Define reasonable bounds (adjust these based on your domain knowledge)\n",
    "    min_reasonable = 0  # Bills shouldn't be negative\n",
    "    max_reasonable = 2000  # Adjust based on your market (e.g., â‚¬1000/month seems high)\n",
    "    \n",
    "    # Count issues for reporting\n",
    "    negative_count = (series_clean < min_reasonable).sum()\n",
    "    outlier_count = (series_clean > max_reasonable).sum()\n",
    "    \n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  - Negative values: {negative_count}\")\n",
    "    print(f\"  - Values > {max_reasonable}: {outlier_count}\")\n",
    "    print(f\"  - Original NaN: {series.isna().sum()}\")\n",
    "    \n",
    "    # Replace problematic values\n",
    "    series_clean.loc[series_clean < min_reasonable] = np.nan  # or -1\n",
    "    series_clean.loc[series_clean > max_reasonable] = np.nan  # or -1\n",
    "    \n",
    "    return series_clean\n",
    "\n",
    "# Apply cleaning\n",
    "final_df['heatingbill'] = clean_billing_column(final_df['heatingbill'], 'heatingbill')\n",
    "final_df['electricitybill'] = clean_billing_column(final_df['electricitybill'], 'electricitybill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5f97be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicator correlations with target:\n",
      "electricitybill_missing          -0.070529\n",
      "zipregion_missing                -0.030921\n",
      "aggregated_missing               -0.009879\n",
      "evaluationtime_missing            0.000650\n",
      "desiredinstallationend_missing    0.002216\n",
      "heatingbill_missing               0.047951\n",
      "Name: grosscontractsigned, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# before encoding, mark the missingness inside the row;\n",
    "missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'aggregated']\n",
    "\n",
    "for col in missing_cols:\n",
    "    final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "\n",
    "# Check the predictive power of missing indicators\n",
    "print(\"Missing indicator correlations with target:\")\n",
    "missing_indicators = [f'{col}_missing' for col in missing_cols]\n",
    "missing_corrs = final_df[missing_indicators + [\"grosscontractsigned\"]].corr()[\"grosscontractsigned\"].drop(\"grosscontractsigned\")\n",
    "print(missing_corrs.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b47781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b111b4",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8f2ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# desiredinstallationend\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-4mesi': 'three_to_four_months', \n",
    "    '5+mesi': 'more_than_5_months',\n",
    "    '1-2mesi': 'one_to_two_months',\n",
    "    'Non lo so': 'dont_know',\n",
    "    'short': np.nan,\n",
    "    # Already mapped values (keep as-is)\n",
    "    'dont_know': 'dont_know',\n",
    "    'three_to_four_months': 'three_to_four_months',\n",
    "    'one_to_two_months': 'one_to_two_months', \n",
    "    'more_than_5_months': 'more_than_5_months',\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1889c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationtime\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-6 mesi': np.nan,\n",
    "    '<3 mesi': np.nan,\n",
    "    '>6 mesi': np.nan,\n",
    "    # Already mapped English values\n",
    "    'less_than_three_months': np.nan,\n",
    "    'more_than_six_months': np.nan,\n",
    "    # Other values that appear in your data\n",
    "    'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "    'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "    'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "    'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "09d77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the old columns directly\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "\n",
    "# Drop the temporary columns\n",
    "final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b24a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan before encoding\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c19a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with ordinal encoding for time-based features\n",
    "\n",
    "# Ordinal encoding for desiredinstallationend (time-based order)\n",
    "time_order = [ 'one_to_two_months', 'three_to_four_months', 'more_than_5_months', 'dont_know', 'missing']\n",
    "final_df['desiredinstallationend_encoded'] = final_df['desiredinstallationend'].map({val: i for i, val in enumerate(time_order)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78461e67",
   "metadata": {},
   "source": [
    "##### Analysis to see how to handle marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a2f7fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       25782 non-null  float64\n",
      " 1   gross_FU                        25782 non-null  int64  \n",
      " 2   gross_SC                        25782 non-null  int64  \n",
      " 3   net_FU                          25782 non-null  float64\n",
      " 4   net_SC                          25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   25782 non-null  float64\n",
      " 6   zipregion                       24760 non-null  object \n",
      " 7   evaluationtime                  25782 non-null  object \n",
      " 8   desiredinstallationend          25782 non-null  object \n",
      " 9   electricitybill                 15062 non-null  object \n",
      " 10  heatingbill                     5305 non-null   object \n",
      " 11  grosscontractsigned             25782 non-null  float64\n",
      " 12  selfipa_done                    25756 non-null  float64\n",
      " 13  aggregated                      25756 non-null  object \n",
      " 14  zipregion_missing               25782 non-null  int64  \n",
      " 15  evaluationtime_missing          25782 non-null  int64  \n",
      " 16  desiredinstallationend_missing  25782 non-null  int64  \n",
      " 17  electricitybill_missing         25782 non-null  int64  \n",
      " 18  heatingbill_missing             25782 non-null  int64  \n",
      " 19  aggregated_missing              25782 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  25782 non-null  int64  \n",
      "dtypes: float64(6), int64(9), object(6)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7d73e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rates by category:\n",
      "\n",
      "\n",
      "ZIPREGION:\n",
      "                       total_samples  conversions  conversion_rate\n",
      "zipregion                                                         \n",
      "Trentino-Alto Adige              218         40.0         0.183486\n",
      "Friuli-Venezia Giulia           1414        245.0         0.173267\n",
      "Liguria                          626         93.0         0.148562\n",
      "Valle D'Aosta                     85         11.0         0.129412\n",
      "Umbria                           345         37.0         0.107246\n",
      "Lombardia                       4213        446.0         0.105863\n",
      "Piemonte                        2369        245.0         0.103419\n",
      "Veneto                          1990        184.0         0.092462\n",
      "Emilia-Romagna                  2207        200.0         0.090621\n",
      "Molise                           155         14.0         0.090323\n",
      "Toscana                         1836        164.0         0.089325\n",
      "Lazio                           2476        195.0         0.078756\n",
      "Marche                           531         41.0         0.077213\n",
      "Campania                         862         62.0         0.071926\n",
      "Basilicata                       187         13.0         0.069519\n",
      "Abruzzo                          516         34.0         0.065891\n",
      "Sardegna                         716         45.0         0.062849\n",
      "Calabria                         851         38.0         0.044653\n",
      "Sicilia                         1450         56.0         0.038621\n",
      "Puglia                          1713         64.0         0.037361\n",
      "Overall variation: 0.0404\n",
      "\n",
      "AGGREGATED:\n",
      "             total_samples  conversions  conversion_rate\n",
      "aggregated                                              \n",
      "SEO                      4          3.0         0.750000\n",
      "Affiliation            212         50.0         0.235849\n",
      "Taboola                448         68.0         0.151786\n",
      "Google                3809        532.0         0.139669\n",
      "Organic               2310        273.0         0.118182\n",
      "Criteo                 184         18.0         0.097826\n",
      "Outbrain              4134        402.0         0.097242\n",
      "Meta                  7885        579.0         0.073431\n",
      "Other                 4165        233.0         0.055942\n",
      "MediaGo                768         40.0         0.052083\n",
      "Tik Tok               1837         75.0         0.040827\n",
      "Overall variation: 0.2019\n"
     ]
    }
   ],
   "source": [
    "# For each categorical column, see conversion rates\n",
    "print(\"Conversion rates by category:\\n\")\n",
    "\n",
    "for col in ['zipregion', 'aggregated']:  # replace with your actual column names\n",
    "    conversion_by_cat = final_df.groupby(col)[\"grosscontractsigned\"].agg(['count', 'sum', 'mean'])\n",
    "    conversion_by_cat.columns = ['total_samples', 'conversions', 'conversion_rate']\n",
    "    conversion_by_cat = conversion_by_cat.sort_values('conversion_rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(conversion_by_cat)\n",
    "    print(f\"Overall variation: {conversion_by_cat['conversion_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf1bfd",
   "metadata": {},
   "source": [
    "Marketing gave good variance, would like to maintain that information. Region is not that significant but still good. \n",
    "However given the * of unique values, in both columns I will opt for grouping instead of each value having its column. \n",
    "\n",
    "Instead of 30+ categorical features, you get ~6-8, keeping the predictive power but losing the noise.\n",
    "\n",
    "Downside; this grouping should occasionally double checked to see if it still makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163be87",
   "metadata": {},
   "source": [
    "##### one hot encoding for marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bcb8c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance groups\n",
    "aggregated_groups = {\n",
    "    'High': ['Affiliation', 'Taboola'],           # >25% conversion\n",
    "    'Medium': ['Google', 'Criteo', 'Organic'],    # 15-20% conversion  \n",
    "    'Low': ['Outbrain', 'Meta', 'Other', 'MediaGo', 'Tik Tok']  # <12% conversion\n",
    "}\n",
    "def group_aggregated(value):\n",
    "    if value == 'SEO':  # Handle the unreliable outlier\n",
    "        return 'Medium'  # Conservative assignment\n",
    "    \n",
    "    for group, channels in aggregated_groups.items():\n",
    "        if value in channels:\n",
    "            return group\n",
    "    return 'Low'  # fallback\n",
    "\n",
    "# Apply grouping\n",
    "final_df['mktg_grouped'] = final_df['aggregated'].apply(group_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9995467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group regions by performance\n",
    "def group_regions(region):\n",
    "    # Top performers with good sample sizes\n",
    "    if region in ['Trentino-Alto Adige', 'Friuli-Venezia Giulia', 'Liguria']:\n",
    "        return 'High_Performer'  # 18-24% conversion\n",
    "    \n",
    "    # Large regions with solid performance  \n",
    "    elif region in ['Lombardia', 'Piemonte', 'Veneto', 'Toscana', 'Emilia-Romagna']:\n",
    "        return 'Large_Solid'     # 11-14% conversion\n",
    "    \n",
    "    # Smaller regions with decent samples\n",
    "    elif region in ['Umbria', 'Lazio', 'Marche']:\n",
    "        return 'Medium'          # 10-13% conversion\n",
    "    \n",
    "    # Lower performing regions\n",
    "    else:\n",
    "        return 'Lower'   \n",
    "\n",
    "final_df['region_grouped'] = final_df['zipregion'].apply(group_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0c5957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the grouped categories\n",
    "final_df = pd.get_dummies(final_df, columns=['mktg_grouped', 'region_grouped'], prefix=['mktg', 'region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "be7d7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777490e3",
   "metadata": {},
   "source": [
    "df is the log of all calendar_events so a direct join will make our dataset exponentially duplicated.\n",
    "\n",
    "I will first get the unique requestid --> leadid cpairs and then merge that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "937eadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestid_leadid_pairs = df[['requestid', 'leadid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f4c8ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requestid_leadid_pairs[requestid_leadid_pairs['requestid'].duplicated(keep=False)]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c10a614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also add leadid to the final df for merging with behaviour data\n",
    "# probably there is a cleaner way to do this\n",
    "\n",
    "# Suppose the column you want from df1 is called \"col_from_df1\"\n",
    "final_df = final_df.merge(\n",
    "    requestid_leadid_pairs[['requestid', 'leadid']],  # keep only requestid + desired column\n",
    "    on='requestid',                      # join key\n",
    "    how='left'                           # keep all rows from df2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bf10b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 29 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       25782 non-null  float64\n",
      " 1   gross_FU                        25782 non-null  int64  \n",
      " 2   gross_SC                        25782 non-null  int64  \n",
      " 3   net_FU                          25782 non-null  float64\n",
      " 4   net_SC                          25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   25782 non-null  float64\n",
      " 6   zipregion                       24760 non-null  object \n",
      " 7   evaluationtime                  25782 non-null  object \n",
      " 8   desiredinstallationend          25782 non-null  object \n",
      " 9   electricitybill                 15062 non-null  object \n",
      " 10  heatingbill                     5305 non-null   object \n",
      " 11  grosscontractsigned             25782 non-null  float64\n",
      " 12  selfipa_done                    25756 non-null  float64\n",
      " 13  aggregated                      25756 non-null  object \n",
      " 14  zipregion_missing               25782 non-null  int64  \n",
      " 15  evaluationtime_missing          25782 non-null  int64  \n",
      " 16  desiredinstallationend_missing  25782 non-null  int64  \n",
      " 17  electricitybill_missing         25782 non-null  int64  \n",
      " 18  heatingbill_missing             25782 non-null  int64  \n",
      " 19  aggregated_missing              25782 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  25782 non-null  int64  \n",
      " 21  mktg_High                       25782 non-null  bool   \n",
      " 22  mktg_Low                        25782 non-null  bool   \n",
      " 23  mktg_Medium                     25782 non-null  bool   \n",
      " 24  region_High_Performer           25782 non-null  bool   \n",
      " 25  region_Large_Solid              25782 non-null  bool   \n",
      " 26  region_Lower                    25782 non-null  bool   \n",
      " 27  region_Medium                   25782 non-null  bool   \n",
      " 28  leadid                          25782 non-null  float64\n",
      "dtypes: bool(7), float64(7), int64(9), object(6)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1713e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "73b32aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any requestid with no leadid?\n",
    "final_df[final_df['leadid'].isnull()]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "18644c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values: False\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns and create final feature matrix\n",
    "columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'aggregated']\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "print(f\"\\nNo missing values: {final_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215f64f",
   "metadata": {},
   "source": [
    "#### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "16e8505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 26\n",
      "Dataset ready: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_19132\\1109642794.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_19132\\1109642794.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n"
     ]
    }
   ],
   "source": [
    "# Fix electricitybill missing values\n",
    "final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "\n",
    "# Fix desiredinstallationend_encoded - NaN means 'Unknown' which should be 0 --> check if needed\n",
    "#X_final['desiredinstallationend_encoded'] = X_final['desiredinstallationend_encoded'].fillna(0)\n",
    "\n",
    "# Verify all missing values are gone\n",
    "print(f\"Remaining missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Dataset ready: {final_df.isnull().sum().sum() == 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bc1b",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5df80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling when needed\n",
    "continuous_cols= [\n",
    "    'time_first_sc_to_first_net_fu',\n",
    "    'electricitybill', \n",
    "    'heatingbill'\n",
    "]\n",
    "\n",
    "# Scale only the continuous features\n",
    "scaler = StandardScaler()\n",
    "final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ca844c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151441 entries, 0 to 151440\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   id                          151441 non-null  int64  \n",
      " 1   total_bc_attempts           151441 non-null  int64  \n",
      " 2   total_bc_outcomes           151441 non-null  int64  \n",
      " 3   lead_to_first_bc_days       151441 non-null  float64\n",
      " 4   bc_duration_days            151441 non-null  float64\n",
      " 5   bc_frequency                151441 non-null  float64\n",
      " 6   positive_outcomes_count     151441 non-null  int64  \n",
      " 7   negative_outcomes_count     151441 non-null  int64  \n",
      " 8   noshow_outcomes_count       151441 non-null  int64  \n",
      " 9   positive_outcome_ratio      151441 non-null  float64\n",
      " 10  negative_outcome_ratio      151441 non-null  float64\n",
      " 11  noshow_outcome_ratio        151441 non-null  float64\n",
      " 12  reachability_score          151441 non-null  float64\n",
      " 13  outcome_trend               151441 non-null  int64  \n",
      " 14  persistence_after_negative  151441 non-null  int64  \n",
      " 15  engagement_score            151441 non-null  float64\n",
      " 16  efficiency_score            151441 non-null  float64\n",
      " 17  last_bc_outcome_encoded     151441 non-null  int64  \n",
      " 18  first_bc_outcome_encoded    151441 non-null  int64  \n",
      "dtypes: float64(9), int64(10)\n",
      "memory usage: 22.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       25782 non-null  float64\n",
      " 1   gross_FU                        25782 non-null  int64  \n",
      " 2   gross_SC                        25782 non-null  int64  \n",
      " 3   net_FU                          25782 non-null  float64\n",
      " 4   net_SC                          25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   25782 non-null  float64\n",
      " 6   electricitybill                 25782 non-null  float64\n",
      " 7   heatingbill                     25782 non-null  float64\n",
      " 8   grosscontractsigned             25782 non-null  float64\n",
      " 9   selfipa_done                    25756 non-null  float64\n",
      " 10  zipregion_missing               25782 non-null  int64  \n",
      " 11  evaluationtime_missing          25782 non-null  int64  \n",
      " 12  desiredinstallationend_missing  25782 non-null  int64  \n",
      " 13  electricitybill_missing         25782 non-null  int64  \n",
      " 14  heatingbill_missing             25782 non-null  int64  \n",
      " 15  aggregated_missing              25782 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  25782 non-null  int64  \n",
      " 17  mktg_High                       25782 non-null  bool   \n",
      " 18  mktg_Low                        25782 non-null  bool   \n",
      " 19  mktg_Medium                     25782 non-null  bool   \n",
      " 20  region_High_Performer           25782 non-null  bool   \n",
      " 21  region_Large_Solid              25782 non-null  bool   \n",
      " 22  region_Lower                    25782 non-null  bool   \n",
      " 23  region_Medium                   25782 non-null  bool   \n",
      " 24  leadid                          25782 non-null  float64\n",
      "dtypes: bool(7), float64(9), int64(9)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "behaviour.info()\n",
    "# id here is leadid, dataset is preselected. 38k leads\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c19f5",
   "metadata": {},
   "source": [
    "Adding behavioral data from Booking Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a7cb496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = final_df.merge(behaviour, \n",
    "                          left_on='leadid', \n",
    "                          right_on='id', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e2838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dc9f131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 44 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       25782 non-null  float64\n",
      " 1   gross_FU                        25782 non-null  int64  \n",
      " 2   gross_SC                        25782 non-null  int64  \n",
      " 3   net_FU                          25782 non-null  float64\n",
      " 4   net_SC                          25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   25782 non-null  float64\n",
      " 6   electricitybill                 25782 non-null  float64\n",
      " 7   heatingbill                     25782 non-null  float64\n",
      " 8   grosscontractsigned             25782 non-null  float64\n",
      " 9   selfipa_done                    25756 non-null  float64\n",
      " 10  zipregion_missing               25782 non-null  int64  \n",
      " 11  evaluationtime_missing          25782 non-null  int64  \n",
      " 12  desiredinstallationend_missing  25782 non-null  int64  \n",
      " 13  electricitybill_missing         25782 non-null  int64  \n",
      " 14  heatingbill_missing             25782 non-null  int64  \n",
      " 15  aggregated_missing              25782 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  25782 non-null  int64  \n",
      " 17  mktg_High                       25782 non-null  bool   \n",
      " 18  mktg_Low                        25782 non-null  bool   \n",
      " 19  mktg_Medium                     25782 non-null  bool   \n",
      " 20  region_High_Performer           25782 non-null  bool   \n",
      " 21  region_Large_Solid              25782 non-null  bool   \n",
      " 22  region_Lower                    25782 non-null  bool   \n",
      " 23  region_Medium                   25782 non-null  bool   \n",
      " 24  leadid                          25782 non-null  float64\n",
      " 25  id                              25782 non-null  int64  \n",
      " 26  total_bc_attempts               25782 non-null  int64  \n",
      " 27  total_bc_outcomes               25782 non-null  int64  \n",
      " 28  lead_to_first_bc_days           25782 non-null  float64\n",
      " 29  bc_duration_days                25782 non-null  float64\n",
      " 30  bc_frequency                    25782 non-null  float64\n",
      " 31  positive_outcomes_count         25782 non-null  int64  \n",
      " 32  negative_outcomes_count         25782 non-null  int64  \n",
      " 33  noshow_outcomes_count           25782 non-null  int64  \n",
      " 34  positive_outcome_ratio          25782 non-null  float64\n",
      " 35  negative_outcome_ratio          25782 non-null  float64\n",
      " 36  noshow_outcome_ratio            25782 non-null  float64\n",
      " 37  reachability_score              25782 non-null  float64\n",
      " 38  outcome_trend                   25782 non-null  int64  \n",
      " 39  persistence_after_negative      25782 non-null  int64  \n",
      " 40  engagement_score                25782 non-null  float64\n",
      " 41  efficiency_score                25782 non-null  float64\n",
      " 42  last_bc_outcome_encoded         25782 non-null  int64  \n",
      " 43  first_bc_outcome_encoded        25782 non-null  int64  \n",
      "dtypes: bool(7), float64(18), int64(19)\n",
      "memory usage: 7.5 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()\n",
    "# all matched correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb3dc0",
   "metadata": {},
   "source": [
    "all the new extra contracts i added are getting droped with no match in leadtime. make sure that they get grabbed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fc1efb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = merged_df[merged_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n",
    "\n",
    "# --> loooks ok "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da2ee9",
   "metadata": {},
   "source": [
    "#### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3a919aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column pairs: []\n",
      "Original shape: (25782, 44)\n",
      "After removing duplicates: (25782, 44)\n"
     ]
    }
   ],
   "source": [
    "# Check your actual dataset for duplicates\n",
    "duplicate_cols = []\n",
    "cols = merged_df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if merged_df.equals(merged_df[cols[j]]):\n",
    "            duplicate_cols.append((cols[i], cols[j]))\n",
    "\n",
    "print(\"Duplicate column pairs:\", duplicate_cols)\n",
    "print(f\"Original shape: {merged_df.shape}\")\n",
    "\n",
    "# Remove duplicates if any found\n",
    "X_final_clean = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "print(f\"After removing duplicates: {X_final_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "947b90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame sizes:\n",
      "final_df: 25782 rows\n",
      "behaviour: 151441 rows\n",
      "merged_df: 25782 rows\n",
      "\n",
      "==================================================\n",
      "Rows with matches: 25782 (100.0%)\n",
      "Rows without matches: 0 (0.0%)\n",
      "\n",
      "==================================================\n",
      "Checking for duplicates in join keys:\n",
      "Duplicates in final_df['leadid']: 5465\n",
      "Duplicates in final_df['requestid']: 0\n",
      "Duplicates in behaviour['id']: 0\n",
      "\n",
      "==================================================\n",
      "Unique leadids in final_df: 20317\n",
      "Unique ids in behaviour: 151441\n",
      "Common IDs: 20317\n",
      "IDs only in final_df: 0\n",
      "IDs only in behaviour: 131124\n"
     ]
    }
   ],
   "source": [
    "# Check the original sizes\n",
    "print(\"Original DataFrame sizes:\")\n",
    "print(f\"final_df: {len(final_df)} rows\")\n",
    "print(f\"behaviour: {len(behaviour)} rows\")\n",
    "print(f\"merged_df: {len(merged_df)} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check how many matches we got\n",
    "matches = merged_df['id'].notna().sum()\n",
    "no_matches = merged_df['id'].isna().sum()\n",
    "\n",
    "print(f\"Rows with matches: {matches} ({matches/len(merged_df)*100:.1f}%)\")\n",
    "print(f\"Rows without matches: {no_matches} ({no_matches/len(merged_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for duplicates in the key columns before merge\n",
    "print(\"Checking for duplicates in join keys:\")\n",
    "print(f\"Duplicates in final_df['leadid']: {final_df['leadid'].duplicated().sum()}\")\n",
    "print(f\"Duplicates in final_df['requestid']: {final_df['requestid'].duplicated().sum()}\")\n",
    "print(f\"Duplicates in behaviour['id']: {behaviour['id'].duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check the overlap between the two key columns\n",
    "final_leadids = set(final_df['leadid'])\n",
    "behaviour_ids = set(behaviour['id'])\n",
    "\n",
    "print(f\"Unique leadids in final_df: {len(final_leadids)}\")\n",
    "print(f\"Unique ids in behaviour: {len(behaviour_ids)}\")\n",
    "print(f\"Common IDs: {len(final_leadids & behaviour_ids)}\")\n",
    "print(f\"IDs only in final_df: {len(final_leadids - behaviour_ids)}\")\n",
    "print(f\"IDs only in behaviour: {len(behaviour_ids - final_leadids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ab8a4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of 0 unmatched rows:\n",
      "==================================================\n",
      "Unmatched entries conversion rate: nan (nan%)\n",
      "Total conversions among unmatched: 0.0\n",
      "\n",
      "==================================================\n",
      "Matched entries conversion rate: 0.088 (8.8%)\n",
      "Total conversions among matched: 2273.0\n",
      "\n",
      "==================================================\n",
      "COMPARISON:\n",
      "Unmatched: nan (nan%)\n",
      "Matched:   0.088 (8.8%)\n",
      "Difference: nan percentage points\n"
     ]
    }
   ],
   "source": [
    "# checking if they are converters or not\n",
    "# Get the rows that didn't match (have NaN in the 'id' column from behaviour)\n",
    "unmatched_rows = merged_df[merged_df['id'].isna()]\n",
    "\n",
    "print(f\"Analysis of {len(unmatched_rows)} unmatched rows:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check conversion rate for unmatched entries\n",
    "conversion_rate_unmatched = unmatched_rows['grosscontractsigned'].mean()\n",
    "total_conversions_unmatched = unmatched_rows['grosscontractsigned'].sum()\n",
    "\n",
    "print(f\"Unmatched entries conversion rate: {conversion_rate_unmatched:.3f} ({conversion_rate_unmatched*100:.1f}%)\")\n",
    "print(f\"Total conversions among unmatched: {total_conversions_unmatched}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Compare with matched entries\n",
    "matched_rows = merged_df[merged_df['id'].notna()]\n",
    "conversion_rate_matched = matched_rows['grosscontractsigned'].mean()\n",
    "total_conversions_matched = matched_rows['grosscontractsigned'].sum()\n",
    "\n",
    "print(f\"Matched entries conversion rate: {conversion_rate_matched:.3f} ({conversion_rate_matched*100:.1f}%)\")\n",
    "print(f\"Total conversions among matched: {total_conversions_matched}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Overall comparison\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"Unmatched: {conversion_rate_unmatched:.3f} ({conversion_rate_unmatched*100:.1f}%)\")\n",
    "print(f\"Matched:   {conversion_rate_matched:.3f} ({conversion_rate_matched*100:.1f}%)\")\n",
    "print(f\"Difference: {(conversion_rate_matched - conversion_rate_unmatched)*100:.1f} percentage points\")\n",
    "\n",
    "# Statistical significance test (optional)\n",
    "from scipy import stats\n",
    "if len(unmatched_rows) > 0 and len(matched_rows) > 0:\n",
    "    stat, p_value = stats.chi2_contingency([[\n",
    "        unmatched_rows['grosscontractsigned'].sum(), \n",
    "        len(unmatched_rows) - unmatched_rows['grosscontractsigned'].sum()\n",
    "    ], [\n",
    "        matched_rows['grosscontractsigned'].sum(), \n",
    "        len(matched_rows) - matched_rows['grosscontractsigned'].sum()\n",
    "    ]])[:2]\n",
    "    \n",
    "    print(f\"\\nStatistical test p-value: {p_value:.6f}\")\n",
    "    print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "86b2c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25782 entries, 0 to 25781\n",
      "Data columns (total 44 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       25782 non-null  float64\n",
      " 1   gross_FU                        25782 non-null  int64  \n",
      " 2   gross_SC                        25782 non-null  int64  \n",
      " 3   net_FU                          25782 non-null  float64\n",
      " 4   net_SC                          25782 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   25782 non-null  float64\n",
      " 6   electricitybill                 25782 non-null  float64\n",
      " 7   heatingbill                     25782 non-null  float64\n",
      " 8   grosscontractsigned             25782 non-null  float64\n",
      " 9   selfipa_done                    25756 non-null  float64\n",
      " 10  zipregion_missing               25782 non-null  int64  \n",
      " 11  evaluationtime_missing          25782 non-null  int64  \n",
      " 12  desiredinstallationend_missing  25782 non-null  int64  \n",
      " 13  electricitybill_missing         25782 non-null  int64  \n",
      " 14  heatingbill_missing             25782 non-null  int64  \n",
      " 15  aggregated_missing              25782 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  25782 non-null  int64  \n",
      " 17  mktg_High                       25782 non-null  bool   \n",
      " 18  mktg_Low                        25782 non-null  bool   \n",
      " 19  mktg_Medium                     25782 non-null  bool   \n",
      " 20  region_High_Performer           25782 non-null  bool   \n",
      " 21  region_Large_Solid              25782 non-null  bool   \n",
      " 22  region_Lower                    25782 non-null  bool   \n",
      " 23  region_Medium                   25782 non-null  bool   \n",
      " 24  leadid                          25782 non-null  float64\n",
      " 25  id                              25782 non-null  int64  \n",
      " 26  total_bc_attempts               25782 non-null  int64  \n",
      " 27  total_bc_outcomes               25782 non-null  int64  \n",
      " 28  lead_to_first_bc_days           25782 non-null  float64\n",
      " 29  bc_duration_days                25782 non-null  float64\n",
      " 30  bc_frequency                    25782 non-null  float64\n",
      " 31  positive_outcomes_count         25782 non-null  int64  \n",
      " 32  negative_outcomes_count         25782 non-null  int64  \n",
      " 33  noshow_outcomes_count           25782 non-null  int64  \n",
      " 34  positive_outcome_ratio          25782 non-null  float64\n",
      " 35  negative_outcome_ratio          25782 non-null  float64\n",
      " 36  noshow_outcome_ratio            25782 non-null  float64\n",
      " 37  reachability_score              25782 non-null  float64\n",
      " 38  outcome_trend                   25782 non-null  int64  \n",
      " 39  persistence_after_negative      25782 non-null  int64  \n",
      " 40  engagement_score                25782 non-null  float64\n",
      " 41  efficiency_score                25782 non-null  float64\n",
      " 42  last_bc_outcome_encoded         25782 non-null  int64  \n",
      " 43  first_bc_outcome_encoded        25782 non-null  int64  \n",
      "dtypes: bool(7), float64(18), int64(19)\n",
      "memory usage: 7.5 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f607eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original unmatched rows: 0\n",
      "Conversions among unmatched: 0.0\n",
      "Non-conversions among unmatched: 0\n",
      "\n",
      "After filtering unmatched rows:\n",
      "Keeping only unmatched conversions: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final merged_df:\n",
      "Matched rows (with behavior): 25782\n",
      "Unmatched conversions (without behavior): 0\n",
      "Total rows: 25782\n",
      "All remaining unmatched rows are conversions: True\n"
     ]
    }
   ],
   "source": [
    "# drop non converters with no behavioral data. keep the 7 that converted and recover their data manually\n",
    "# Get the unmatched rows (the 860 entries without behavior data)\n",
    "# Get the unmatched rows (the 860 entries without behavior data)\n",
    "unmatched_rows = merged_df[merged_df['id'].isna()]\n",
    "\n",
    "print(f\"Original unmatched rows: {len(unmatched_rows)}\")\n",
    "print(f\"Conversions among unmatched: {unmatched_rows['grosscontractsigned'].sum()}\")\n",
    "print(f\"Non-conversions among unmatched: {(unmatched_rows['grosscontractsigned'] == 0).sum()}\")\n",
    "\n",
    "# Filter unmatched rows to keep only conversions (grosscontractsigned == 1)\n",
    "unmatched_conversions = unmatched_rows[unmatched_rows['grosscontractsigned'] == 1]\n",
    "\n",
    "print(f\"\\nAfter filtering unmatched rows:\")\n",
    "print(f\"Keeping only unmatched conversions: {len(unmatched_conversions)}\")\n",
    "\n",
    "# Get the matched rows (the 13,296 entries with behavior data)\n",
    "matched_rows = merged_df[merged_df['id'].notna()]\n",
    "\n",
    "# Combine and replace merged_df\n",
    "merged_df = pd.concat([matched_rows, unmatched_conversions], ignore_index=True)\n",
    "\n",
    "print(f\"\\nFinal merged_df:\")\n",
    "print(f\"Matched rows (with behavior): {len(matched_rows)}\")\n",
    "print(f\"Unmatched conversions (without behavior): {len(unmatched_conversions)}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "print(f\"All remaining unmatched rows are conversions: {(merged_df[merged_df['id'].isna()]['grosscontractsigned'] == 1).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b8d6121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10         1387.0\n",
       "24         2039.0\n",
       "47         2921.0\n",
       "49         3054.0\n",
       "57         3339.0\n",
       "           ...   \n",
       "25735    126967.0\n",
       "25743    126980.0\n",
       "25744    126981.0\n",
       "25763    127538.0\n",
       "25771    128268.0\n",
       "Name: requestid, Length: 9617, dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM THREE\n",
    "# see leads that didnt make it to sc --> le\n",
    "merged_df.loc[merged_df['net_SC'] == 0, 'requestid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7518fbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10         1387.0\n",
       "67         3668.0\n",
       "83         4202.0\n",
       "90         4502.0\n",
       "105        4967.0\n",
       "174        7227.0\n",
       "212        9286.0\n",
       "218        9600.0\n",
       "274       12136.0\n",
       "289       13082.0\n",
       "326       14602.0\n",
       "372       16694.0\n",
       "377       16872.0\n",
       "451       20096.0\n",
       "499       22490.0\n",
       "500       22538.0\n",
       "520       23302.0\n",
       "533       23673.0\n",
       "543       24071.0\n",
       "581       25455.0\n",
       "614       26629.0\n",
       "691       28791.0\n",
       "693       28884.0\n",
       "704       29149.0\n",
       "723       29752.0\n",
       "738       30361.0\n",
       "768       31401.0\n",
       "790       32048.0\n",
       "793       32121.0\n",
       "925       38499.0\n",
       "981       40431.0\n",
       "1007      41579.0\n",
       "1022      42034.0\n",
       "1025      42088.0\n",
       "1037      42424.0\n",
       "1105      46146.0\n",
       "1117      46888.0\n",
       "1162      48878.0\n",
       "1168      49142.0\n",
       "1248      53645.0\n",
       "1500      66892.0\n",
       "2163      88220.0\n",
       "8079     106390.0\n",
       "8080     106391.0\n",
       "10886    109781.0\n",
       "13773    113297.0\n",
       "13875    113414.0\n",
       "15603    115357.0\n",
       "15820    115615.0\n",
       "18158    118332.0\n",
       "20666    121188.0\n",
       "25197    126300.0\n",
       "25763    127538.0\n",
       "25771    128268.0\n",
       "Name: requestid, dtype: float64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of 7k no net sc (but yess gross) 52 are converted could it be after the cutoff?\n",
    "x = merged_df.loc[\n",
    "    (merged_df['net_SC'] == 0) & (merged_df['grosscontractsigned'] == 1),\n",
    "    'requestid'\n",
    "]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9adf4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to investigate\n",
    "# ids\n",
    "# 1387, 3668, 4202, 4502, 4967, 7227, 9286, 9600, 12136, 13082, 14602, 16694, 16872, 20096, 22490, 22538, 23302, 23673, 24071, 25455, 26629, 28791, 28884, 29149, 29752, 30361, 31401, 32048, 32121, 38499, 40431, 41579, 42034, 42088, 42424, 46146, 46888, 48878, 49142, 53645, 66892, 88220, 106390, 106391, 109781, 113297, 113414, 115357, 115615, 118332, 121188, 126300, 127538, 128268\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bda973",
   "metadata": {},
   "source": [
    "they really dont have vd or vdfield that is not deleted --> sus entries. you can look into them later but for now lets drop only sc_net = 0 and grosscontractsigned = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "dc568ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop the ones who havent made to net sc\n",
    "# Keep rows where not ( net_SC is equal to 0 and grosscontractsigned is 1)\n",
    "merged_df = merged_df[~((merged_df['net_SC'] == 0) & (merged_df['grosscontractsigned'] == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c0e2a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"old\"] = 0 \n",
    "# Mark the older ones\n",
    "merged_df[\"old\"] = merged_df[\"requestid\"].isin(older).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a52da698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1682 entries, 0 to 25781\n",
      "Data columns (total 45 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       1682 non-null   float64\n",
      " 1   gross_FU                        1682 non-null   int64  \n",
      " 2   gross_SC                        1682 non-null   int64  \n",
      " 3   net_FU                          1682 non-null   float64\n",
      " 4   net_SC                          1682 non-null   float64\n",
      " 5   time_first_sc_to_first_net_fu   1682 non-null   float64\n",
      " 6   electricitybill                 1682 non-null   float64\n",
      " 7   heatingbill                     1682 non-null   float64\n",
      " 8   grosscontractsigned             1682 non-null   float64\n",
      " 9   selfipa_done                    1682 non-null   float64\n",
      " 10  zipregion_missing               1682 non-null   int64  \n",
      " 11  evaluationtime_missing          1682 non-null   int64  \n",
      " 12  desiredinstallationend_missing  1682 non-null   int64  \n",
      " 13  electricitybill_missing         1682 non-null   int64  \n",
      " 14  heatingbill_missing             1682 non-null   int64  \n",
      " 15  aggregated_missing              1682 non-null   int64  \n",
      " 16  desiredinstallationend_encoded  1682 non-null   int64  \n",
      " 17  mktg_High                       1682 non-null   bool   \n",
      " 18  mktg_Low                        1682 non-null   bool   \n",
      " 19  mktg_Medium                     1682 non-null   bool   \n",
      " 20  region_High_Performer           1682 non-null   bool   \n",
      " 21  region_Large_Solid              1682 non-null   bool   \n",
      " 22  region_Lower                    1682 non-null   bool   \n",
      " 23  region_Medium                   1682 non-null   bool   \n",
      " 24  leadid                          1682 non-null   float64\n",
      " 25  id                              1682 non-null   int64  \n",
      " 26  total_bc_attempts               1682 non-null   int64  \n",
      " 27  total_bc_outcomes               1682 non-null   int64  \n",
      " 28  lead_to_first_bc_days           1682 non-null   float64\n",
      " 29  bc_duration_days                1682 non-null   float64\n",
      " 30  bc_frequency                    1682 non-null   float64\n",
      " 31  positive_outcomes_count         1682 non-null   int64  \n",
      " 32  negative_outcomes_count         1682 non-null   int64  \n",
      " 33  noshow_outcomes_count           1682 non-null   int64  \n",
      " 34  positive_outcome_ratio          1682 non-null   float64\n",
      " 35  negative_outcome_ratio          1682 non-null   float64\n",
      " 36  noshow_outcome_ratio            1682 non-null   float64\n",
      " 37  reachability_score              1682 non-null   float64\n",
      " 38  outcome_trend                   1682 non-null   int64  \n",
      " 39  persistence_after_negative      1682 non-null   int64  \n",
      " 40  engagement_score                1682 non-null   float64\n",
      " 41  efficiency_score                1682 non-null   float64\n",
      " 42  last_bc_outcome_encoded         1682 non-null   int64  \n",
      " 43  first_bc_outcome_encoded        1682 non-null   int64  \n",
      " 44  old                             1682 non-null   int64  \n",
      "dtypes: bool(7), float64(18), int64(20)\n",
      "memory usage: 524.0 KB\n"
     ]
    }
   ],
   "source": [
    "mask1 = merged_df[merged_df[\"old\"] == 1]\n",
    "old_after = list(set())\n",
    "mask1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2e5944ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"old\"].value_counts()\n",
    "\n",
    "old_after = list(set(mask1['requestid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "afcaee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cede2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = list (set (older) - set (old_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "19200e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69632,\n",
       " 71684,\n",
       " 26629,\n",
       " 24071,\n",
       " 22538,\n",
       " 128531,\n",
       " 78873,\n",
       " 81947,\n",
       " 123940,\n",
       " 123941,\n",
       " 54315,\n",
       " 42034,\n",
       " 127538,\n",
       " 29752,\n",
       " 7227,\n",
       " 30269,\n",
       " 51264,\n",
       " 72256,\n",
       " 46146,\n",
       " 9286,\n",
       " 35401,\n",
       " 3668,\n",
       " 126045,\n",
       " 58464,\n",
       " 38499,\n",
       " 42088,\n",
       " 4202,\n",
       " 41579,\n",
       " 124011,\n",
       " 28791,\n",
       " 23673,\n",
       " 20096,\n",
       " 50821,\n",
       " 54925,\n",
       " 30361,\n",
       " 88220,\n",
       " 31401,\n",
       " 124092,\n",
       " 91330,\n",
       " 126674,\n",
       " 28884,\n",
       " 87266,\n",
       " 123108,\n",
       " 104166,\n",
       " 123629,\n",
       " 48878,\n",
       " 23302,\n",
       " 47879,\n",
       " 53001,\n",
       " 78602,\n",
       " 14602,\n",
       " 128268,\n",
       " 59661,\n",
       " 68877,\n",
       " 1817,\n",
       " 13082,\n",
       " 46888,\n",
       " 32048,\n",
       " 66354,\n",
       " 72499,\n",
       " 59700,\n",
       " 127797,\n",
       " 16694,\n",
       " 66892,\n",
       " 123216,\n",
       " 102737,\n",
       " 126300,\n",
       " 102243,\n",
       " 4967,\n",
       " 12136,\n",
       " 53608,\n",
       " 1387,\n",
       " 25455,\n",
       " 127343,\n",
       " 53616,\n",
       " 48495,\n",
       " 122739,\n",
       " 32121,\n",
       " 9600,\n",
       " 53645,\n",
       " 4502,\n",
       " 106390,\n",
       " 106391,\n",
       " 72604,\n",
       " 59808,\n",
       " 102327,\n",
       " 42424,\n",
       " 65471,\n",
       " 53696,\n",
       " 122817,\n",
       " 103887,\n",
       " 91088,\n",
       " 22490,\n",
       " 29149,\n",
       " 84448,\n",
       " 73189,\n",
       " 16872,\n",
       " 40431,\n",
       " 106482,\n",
       " 106998,\n",
       " 49142]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# request ids that we initially had from sc counts of the older coontracts but after pre processing we dropped them\n",
    "to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5e5749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop requestid ad other indicatiors\n",
    "merged_df = merged_df.drop(columns=['requestid', 'id', 'leadid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "64a85535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversions in missing rows: 0.0\n",
      "Conversion rate in missing rows: nan\n",
      "Conversion rate in complete rows: 0.0862\n"
     ]
    }
   ],
   "source": [
    "# Check conversions in missing call data rows\n",
    "missing_mask = merged_df['total_bc_attempts'].isnull()\n",
    "print(f\"Conversions in missing rows: {merged_df[missing_mask]['grosscontractsigned'].sum()}\")\n",
    "print(f\"Conversion rate in missing rows: {merged_df[missing_mask]['grosscontractsigned'].mean():.4f}\")\n",
    "print(f\"Conversion rate in complete rows: {merged_df[~missing_mask]['grosscontractsigned'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b0fc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these requests are very old and dont have propre registstration, some have some registration but essentially there are only 7 contract signed insode 860, so essentially dropping as a first step of undersampling.\n",
    "# ill keep 7 positives and recover their data, drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5c84629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (25728, 42)\n",
      "New conversion rate: 0.0862\n",
      "Total conversions kept: 2219.0\n"
     ]
    }
   ],
   "source": [
    "# this would drop all entries with any missing values, so lets hold our horses here. \n",
    "#merged_df = merged_df.dropna().copy()\n",
    "print(f\"New dataset shape: {merged_df.shape}\")\n",
    "print(f\"New conversion rate: {merged_df['grosscontractsigned'].mean():.4f}\")\n",
    "print(f\"Total conversions kept: {merged_df['grosscontractsigned'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4526a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create subfolder if it doesn't exist\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# Save your merged dataframe\n",
    "merged_df.to_csv('merged_df_bigger_sample.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4d7a6a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25728 entries, 0 to 25781\n",
      "Data columns (total 42 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   gross_FU                        25728 non-null  int64  \n",
      " 1   gross_SC                        25728 non-null  int64  \n",
      " 2   net_FU                          25728 non-null  float64\n",
      " 3   net_SC                          25728 non-null  float64\n",
      " 4   time_first_sc_to_first_net_fu   25728 non-null  float64\n",
      " 5   electricitybill                 25728 non-null  float64\n",
      " 6   heatingbill                     25728 non-null  float64\n",
      " 7   grosscontractsigned             25728 non-null  float64\n",
      " 8   selfipa_done                    25702 non-null  float64\n",
      " 9   zipregion_missing               25728 non-null  int64  \n",
      " 10  evaluationtime_missing          25728 non-null  int64  \n",
      " 11  desiredinstallationend_missing  25728 non-null  int64  \n",
      " 12  electricitybill_missing         25728 non-null  int64  \n",
      " 13  heatingbill_missing             25728 non-null  int64  \n",
      " 14  aggregated_missing              25728 non-null  int64  \n",
      " 15  desiredinstallationend_encoded  25728 non-null  int64  \n",
      " 16  mktg_High                       25728 non-null  bool   \n",
      " 17  mktg_Low                        25728 non-null  bool   \n",
      " 18  mktg_Medium                     25728 non-null  bool   \n",
      " 19  region_High_Performer           25728 non-null  bool   \n",
      " 20  region_Large_Solid              25728 non-null  bool   \n",
      " 21  region_Lower                    25728 non-null  bool   \n",
      " 22  region_Medium                   25728 non-null  bool   \n",
      " 23  total_bc_attempts               25728 non-null  int64  \n",
      " 24  total_bc_outcomes               25728 non-null  int64  \n",
      " 25  lead_to_first_bc_days           25728 non-null  float64\n",
      " 26  bc_duration_days                25728 non-null  float64\n",
      " 27  bc_frequency                    25728 non-null  float64\n",
      " 28  positive_outcomes_count         25728 non-null  int64  \n",
      " 29  negative_outcomes_count         25728 non-null  int64  \n",
      " 30  noshow_outcomes_count           25728 non-null  int64  \n",
      " 31  positive_outcome_ratio          25728 non-null  float64\n",
      " 32  negative_outcome_ratio          25728 non-null  float64\n",
      " 33  noshow_outcome_ratio            25728 non-null  float64\n",
      " 34  reachability_score              25728 non-null  float64\n",
      " 35  outcome_trend                   25728 non-null  int64  \n",
      " 36  persistence_after_negative      25728 non-null  int64  \n",
      " 37  engagement_score                25728 non-null  float64\n",
      " 38  efficiency_score                25728 non-null  float64\n",
      " 39  last_bc_outcome_encoded         25728 non-null  int64  \n",
      " 40  first_bc_outcome_encoded        25728 non-null  int64  \n",
      " 41  old                             25728 non-null  int64  \n",
      "dtypes: bool(7), float64(16), int64(19)\n",
      "memory usage: 7.2 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ad60cd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grosscontractsigned\n",
       "0.0    23509\n",
       "1.0     2219\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"grosscontractsigned\"].value_counts() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
