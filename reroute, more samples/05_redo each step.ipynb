{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef237563",
   "metadata": {},
   "source": [
    "# Re route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f1192",
   "metadata": {},
   "source": [
    "## dataset set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fed4ef",
   "metadata": {},
   "source": [
    "older dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccd29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up of the same dataset of performance test\n",
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\reroute, more samples\\merged_df_bigger_sample.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "\n",
    "# optionally turn it into net sc only \n",
    "#df = df [ df ['net_sc'] != 0].copy()\n",
    "\n",
    "#separating into old positives, new positives and all negatives nb: would be better to mark also the older/ newer negatives maybe later\n",
    "new_positives = df[ (df['old'] == 0) & (df['grosscontractsigned']== 1)].reset_index(drop=True)\n",
    "old_positives =  df[df['old']==1].reset_index(drop=True)\n",
    "rest_negatives = df[ df[ 'grosscontractsigned'] == 0].reset_index(drop=True)\n",
    "\n",
    "negative_train = rest_negatives.sample(frac=0.7, random_state=42)\n",
    "negative_test = rest_negatives.drop(negative_train.index)\n",
    "\n",
    "# these is 4184. to keep the original ratio i have to add 210 new positives\n",
    "test_new_positives = new_positives.sample(n=244, random_state = 42)\n",
    "train_new_positives = new_positives.drop(test_new_positives.index)\n",
    "\n",
    "# create final train and test\n",
    "train = pd.concat( [negative_train, train_new_positives, old_positives])\n",
    "test = pd.concat ( [test_new_positives, negative_test])\n",
    "\n",
    "# boo dont forget to drop old mark\n",
    "train = train.drop('old', axis=1)\n",
    "test = test.drop('old', axis=1)\n",
    "\n",
    "# for some reason selfipadone is missing on a few columns, later look but for now fill with 0 \n",
    "train['selfipa_done'] = train['selfipa_done'].fillna(0)\n",
    "test['selfipa_done'] = test['selfipa_done'].fillna(0)\n",
    "\n",
    "#split \n",
    "X_train = train.drop('grosscontractsigned', axis=1)\n",
    "y_train = train['grosscontractsigned']\n",
    "X_test = test.drop('grosscontractsigned', axis=1)\n",
    "y_test = test['grosscontractsigned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb915f",
   "metadata": {},
   "source": [
    "newer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62cb4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 14364\n",
      "Test set size: 3591\n",
      "\n",
      "Train class distribution:\n",
      "grosscontractsigned\n",
      "0.0    0.96714\n",
      "1.0    0.03286\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "grosscontractsigned\n",
      "0.0    0.96714\n",
      "1.0    0.03286\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = r\"C:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\processed_data\\merged_df_only_after_march.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "\n",
    "# optionally turn it into net sc only \n",
    "#df = df [ df ['net_sc'] != 0].copy()\n",
    "# since double layer model, we keep it as is\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X = df.drop('grosscontractsigned', axis=1)\n",
    "y = df['grosscontractsigned']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7c0ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10699 entries, 1 to 17954\n",
      "Data columns (total 41 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   gross_fu                        10699 non-null  int64  \n",
      " 1   gross_sc                        10699 non-null  int64  \n",
      " 2   net_fu                          10699 non-null  float64\n",
      " 3   net_sc                          10699 non-null  float64\n",
      " 4   time_first_sc_to_first_net_fu   10699 non-null  float64\n",
      " 5   electricitybill                 10699 non-null  float64\n",
      " 6   heatingbill                     10699 non-null  float64\n",
      " 7   grosscontractsigned             10699 non-null  float64\n",
      " 8   selfipa_done                    10699 non-null  int64  \n",
      " 9   zipregion_missing               10699 non-null  int64  \n",
      " 10  evaluationtime_missing          10699 non-null  int64  \n",
      " 11  desiredinstallationend_missing  10699 non-null  int64  \n",
      " 12  electricitybill_missing         10699 non-null  int64  \n",
      " 13  heatingbill_missing             10699 non-null  int64  \n",
      " 14  aggregated_missing              10699 non-null  int64  \n",
      " 15  desiredinstallationend_encoded  10699 non-null  int64  \n",
      " 16  mktg_high                       10699 non-null  bool   \n",
      " 17  mktg_low                        10699 non-null  bool   \n",
      " 18  mktg_medium                     10699 non-null  bool   \n",
      " 19  region_high_performer           10699 non-null  bool   \n",
      " 20  region_large_solid              10699 non-null  bool   \n",
      " 21  region_lower                    10699 non-null  bool   \n",
      " 22  region_medium                   10699 non-null  bool   \n",
      " 23  total_bc_attempts               10699 non-null  int64  \n",
      " 24  total_bc_outcomes               10699 non-null  int64  \n",
      " 25  lead_to_first_bc_days           10699 non-null  float64\n",
      " 26  bc_duration_days                10699 non-null  float64\n",
      " 27  bc_frequency                    10699 non-null  float64\n",
      " 28  positive_outcomes_count         10699 non-null  int64  \n",
      " 29  negative_outcomes_count         10699 non-null  int64  \n",
      " 30  noshow_outcomes_count           10699 non-null  int64  \n",
      " 31  positive_outcome_ratio          10699 non-null  float64\n",
      " 32  negative_outcome_ratio          10699 non-null  float64\n",
      " 33  noshow_outcome_ratio            10699 non-null  float64\n",
      " 34  reachability_score              10699 non-null  float64\n",
      " 35  outcome_trend                   10699 non-null  int64  \n",
      " 36  persistence_after_negative      10699 non-null  int64  \n",
      " 37  engagement_score                10699 non-null  float64\n",
      " 38  efficiency_score                10699 non-null  float64\n",
      " 39  last_bc_outcome_encoded         10699 non-null  int64  \n",
      " 40  first_bc_outcome_encoded        10699 non-null  int64  \n",
      "dtypes: bool(7), float64(15), int64(19)\n",
      "memory usage: 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa223de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up further \n",
    "y_sc = (df['net_sc'] >= 1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d69c1",
   "metadata": {},
   "source": [
    "##  Base model and Fisrt NN - 2 Layer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ec6c6",
   "metadata": {},
   "source": [
    "Step 1: Setup + Labels + Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30523223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of full dataset: (17955, 40)\n",
      "Layer 1 features: 34\n",
      "Layer 2 features: 40\n",
      "y_sc positive rate: 0.596\n",
      "y_sign positive rate: 0.033\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: define targets and feature sets ---\n",
    "\n",
    "# Layer 1 target: whether lead reached at least one SC\n",
    "y_sc = (df['net_sc'] >= 1).astype(int)\n",
    "\n",
    "# Layer 2 target: contract signed (already split into y_train / y_test separately)\n",
    "y_sign = df['grosscontractsigned']\n",
    "\n",
    "# Features for layer 1: drop SC/FU outcome columns (leakage for this stage)\n",
    "drop_cols_layer1 = [\n",
    "    'gross_fu', 'gross_sc', 'net_fu', 'net_sc',\n",
    "    'time_first_sc_to_first_net_fu', 'selfipa_done'\n",
    "]\n",
    "X_layer1 = X.drop(columns=drop_cols_layer1)\n",
    "\n",
    "# Features for layer 2: keep everything (since SC/FU info is valid here)\n",
    "X_layer2 = X.copy()\n",
    "\n",
    "print(\"Shape of full dataset:\", X.shape)\n",
    "print(\"Layer 1 features:\", X_layer1.shape[1])\n",
    "print(\"Layer 2 features:\", X_layer2.shape[1])\n",
    "\n",
    "# Just to sanity check alignment\n",
    "print(\"y_sc positive rate:\", y_sc.mean().round(3))\n",
    "print(\"y_sign positive rate:\", y_sign.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67676e32",
   "metadata": {},
   "source": [
    "Step 2: Metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f08d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recall_at_percent(y_true, y_scores, top_percent=0.05):\n",
    "    \"\"\"\n",
    "    Recall@% = fraction of all positives captured in the top % of ranked leads.\n",
    "    \"\"\"\n",
    "    n_top = int(len(y_true) * top_percent)\n",
    "    if n_top < 1: \n",
    "        n_top = 1\n",
    "    # rank by predicted score\n",
    "    idx = np.argsort(y_scores)[::-1][:n_top]\n",
    "    return y_true.iloc[idx].sum() / y_true.sum()\n",
    "\n",
    "def lift_at_percent(y_true, y_scores, top_percent=0.05):\n",
    "    \"\"\"\n",
    "    Lift@% = (positive rate in top % of ranked leads) / (overall positive rate).\n",
    "    \"\"\"\n",
    "    n_top = int(len(y_true) * top_percent)\n",
    "    if n_top < 1:\n",
    "        n_top = 1\n",
    "    idx = np.argsort(y_scores)[::-1][:n_top]\n",
    "    top_rate = y_true.iloc[idx].mean()\n",
    "    overall_rate = y_true.mean()\n",
    "    return top_rate / overall_rate if overall_rate > 0 else np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de29ffc",
   "metadata": {},
   "source": [
    "Step 3: cross-validation loop.\n",
    "\n",
    "Here weâ€™ll:\n",
    "\n",
    "create pipelines for baseline (logistic regression) and NN (MLP),\n",
    "\n",
    "run StratifiedKFold,\n",
    "\n",
    "train both layer 1 and layer 2 models inside each fold,\n",
    "\n",
    "compute P_final = P_sc * P_sign_given_sc,\n",
    "\n",
    "collect recall and lift at 1%, 5%, 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86016498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>percent</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>lift_mean</th>\n",
       "      <th>lift_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.049153</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>5.043051</td>\n",
       "      <td>0.952480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.138983</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>2.788202</td>\n",
       "      <td>0.707137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.240678</td>\n",
       "      <td>0.028488</td>\n",
       "      <td>2.407450</td>\n",
       "      <td>0.284956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.091525</td>\n",
       "      <td>0.056022</td>\n",
       "      <td>9.390508</td>\n",
       "      <td>5.747859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.145309</td>\n",
       "      <td>5.780419</td>\n",
       "      <td>2.915104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.173780</td>\n",
       "      <td>4.153699</td>\n",
       "      <td>1.738289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  percent  recall_mean  recall_std  lift_mean  lift_std\n",
       "0  Baseline     0.01     0.049153    0.009283   5.043051  0.952480\n",
       "1  Baseline     0.05     0.138983    0.035249   2.788202  0.707137\n",
       "2  Baseline     0.10     0.240678    0.028488   2.407450  0.284956\n",
       "3        NN     0.01     0.091525    0.056022   9.390508  5.747859\n",
       "4        NN     0.05     0.288136    0.145309   5.780419  2.915104\n",
       "5        NN     0.10     0.415254    0.173780   4.153699  1.738289"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Models\n",
    "baseline_clf = LogisticRegression(class_weight='balanced', max_iter=5000)\n",
    "nn_clf = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu',\n",
    "                       early_stopping=True, random_state=42, max_iter=200)\n",
    "\n",
    "# Wrap NN in a pipeline with scaling\n",
    "nn_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", nn_clf)\n",
    "])\n",
    "\n",
    "# CV setup\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_layer2, y_sign)):\n",
    "    # Split indexes\n",
    "    X1_train, X1_val = X_layer1.iloc[train_idx], X_layer1.iloc[val_idx]\n",
    "    y1_train, y1_val = y_sc.iloc[train_idx], y_sc.iloc[val_idx]\n",
    "    \n",
    "    X2_train, X2_val = X_layer2.iloc[train_idx], X_layer2.iloc[val_idx]\n",
    "    y2_train, y2_val = y_sign.iloc[train_idx], y_sign.iloc[val_idx]\n",
    "    \n",
    "    for model_name, model in [(\"Baseline\", baseline_clf), (\"NN\", nn_pipeline)]:\n",
    "        # ---- Layer 1 ----\n",
    "        model1 = model.fit(X1_train, y1_train)\n",
    "        p_sc = model1.predict_proba(X1_val)[:,1]\n",
    "        \n",
    "        # ---- Layer 2 ----\n",
    "        # train only on those with y_sc=1 in training set\n",
    "        mask_sc_train = (y1_train == 1)\n",
    "        model2 = model.fit(X2_train[mask_sc_train], y2_train[mask_sc_train])\n",
    "        p_sign_given_sc = model2.predict_proba(X2_val)[:,1]\n",
    "        \n",
    "        # ---- Final probability ----\n",
    "        p_final = p_sc * p_sign_given_sc\n",
    "        \n",
    "        # ---- Metrics ----\n",
    "        for pct in [0.01, 0.05, 0.10]:\n",
    "            rec = recall_at_percent(y2_val, p_final, top_percent=pct)\n",
    "            lift = lift_at_percent(y2_val, p_final, top_percent=pct)\n",
    "            results.append({\n",
    "                \"fold\": fold+1,\n",
    "                \"model\": model_name,\n",
    "                \"percent\": pct,\n",
    "                \"recall\": rec,\n",
    "                \"lift\": lift\n",
    "            })\n",
    "\n",
    "cv_results = pd.DataFrame(results)\n",
    "summary = cv_results.groupby([\"model\",\"percent\"]).agg(\n",
    "    recall_mean=(\"recall\",\"mean\"), recall_std=(\"recall\",\"std\"),\n",
    "    lift_mean=(\"lift\",\"mean\"), lift_std=(\"lift\",\"std\")\n",
    ").reset_index()\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e3264",
   "metadata": {},
   "source": [
    "what these numbers mean  Two-layer cascade  The NN results you see already include both layers.  For each fold we did:  Train NN on Layer 1 (BCâ†’SC) â†’ get  ð‘ƒ ( SC ) P(SC).  Train NN on Layer 2 (SCâ†’Contract) â†’ get  ð‘ƒ ( sign âˆ£ SC ) P(signâˆ£SC). \n",
    " Multiply: P(finalÂ sign)=P(SC)Ã—P(signâˆ£SC).\n",
    "\n",
    "The metrics are calculated on this final combined probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844d3f3",
   "metadata": {},
   "source": [
    "Step 4: final benchmark on the untouched test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c321d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Cascade on Test Set:\n",
      "  Top 1% -> Recall=0.008, Lift=0.87\n",
      "  Top 5% -> Recall=0.017, Lift=0.34\n",
      "  Top 10% -> Recall=0.068, Lift=0.68\n",
      "\n",
      "NN Cascade on Test Set:\n",
      "  Top 1% -> Recall=0.144, Lift=14.78\n",
      "  Top 5% -> Recall=0.407, Lift=8.16\n",
      "  Top 10% -> Recall=0.525, Lift=5.26\n"
     ]
    }
   ],
   "source": [
    "# --- retrain cascades on full training data ---\n",
    "def fit_and_score_cascade(model, X1_train, y1_train, X1_test, y1_test,\n",
    "                          X2_train, y2_train, X2_test, y2_test):\n",
    "    \"\"\"\n",
    "    Train a two-layer cascade and return final predicted probabilities for test set.\n",
    "    \"\"\"\n",
    "    # Layer 1: BC->SC\n",
    "    m1 = model.fit(X1_train, y1_train)\n",
    "    p_sc = m1.predict_proba(X1_test)[:, 1]\n",
    "    \n",
    "    # Layer 2: SC->Contract (train only on SC=1 leads)\n",
    "    mask_sc_train = (y1_train == 1)\n",
    "    m2 = model.fit(X2_train[mask_sc_train], y2_train[mask_sc_train])\n",
    "    p_sign_given_sc = m2.predict_proba(X2_test)[:, 1]\n",
    "    \n",
    "    # Final probability\n",
    "    return p_sc * p_sign_given_sc\n",
    "\n",
    "# Prepare train/test splits for both layers\n",
    "drop_cols_layer1 = [\n",
    "    'gross_fu','gross_sc','net_fu','net_sc',\n",
    "    'time_first_sc_to_first_net_fu','selfipa_done'\n",
    "]\n",
    "X1_train = X_train.drop(columns=drop_cols_layer1)\n",
    "X1_test  = X_test.drop(columns=drop_cols_layer1)\n",
    "y1_train = (df.loc[X_train.index, 'net_sc'] >= 1).astype(int)\n",
    "y1_test  = (df.loc[X_test.index, 'net_sc'] >= 1).astype(int)\n",
    "\n",
    "X2_train, X2_test = X_train.copy(), X_test.copy()\n",
    "y2_train, y2_test = y_train.copy(), y_test.copy()\n",
    "\n",
    "# Models\n",
    "baseline_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(class_weight='balanced', max_iter=5000))\n",
    "])\n",
    "\n",
    "nn_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(hidden_layer_sizes=(64,32),\n",
    "                         activation='relu',\n",
    "                         early_stopping=True,\n",
    "                         random_state=42,\n",
    "                         max_iter=200))\n",
    "])\n",
    "\n",
    "# Predictions\n",
    "p_baseline = fit_and_score_cascade(baseline_model,\n",
    "                                   X1_train, y1_train, X1_test, y1_test,\n",
    "                                   X2_train, y2_train, X2_test, y2_test)\n",
    "\n",
    "p_nn = fit_and_score_cascade(nn_model,\n",
    "                             X1_train, y1_train, X1_test, y1_test,\n",
    "                             X2_train, y2_train, X2_test, y2_test)\n",
    "\n",
    "# Evaluate\n",
    "for name, preds in [(\"Baseline\", p_baseline), (\"NN\", p_nn)]:\n",
    "    print(f\"\\n{name} Cascade on Test Set:\")\n",
    "    for pct in [0.01, 0.05, 0.10]:\n",
    "        rec = recall_at_percent(y2_test, preds, top_percent=pct)\n",
    "        lift = lift_at_percent(y2_test, preds, top_percent=pct)\n",
    "        print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04de95d",
   "metadata": {},
   "source": [
    "### Trying dynamic scoring (base + NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b5bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) train models (baseline + NN for both layers, just like before).\n",
    "# Train Layer 1 on BC-only features\n",
    "drop_cols_layer1 = [\n",
    "    'gross_fu','gross_sc','net_fu','net_sc',\n",
    "    'time_first_sc_to_first_net_fu','selfipa_done'\n",
    "]\n",
    "X_bc = X.drop(columns=drop_cols_layer1)\n",
    "y_sc = (df['net_sc'] >= 1).astype(int)\n",
    "\n",
    "# Train Layer 2 on SC-leads\n",
    "X_sc = X.copy()\n",
    "y_sign = df['grosscontractsigned']\n",
    "\n",
    "# Fit models (example: NN, but you can also do baseline)\n",
    "model_layer1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(hidden_layer_sizes=(64,32),\n",
    "                         activation=\"relu\",\n",
    "                         early_stopping=True,\n",
    "                         random_state=42,\n",
    "                         max_iter=200))\n",
    "]).fit(X_bc, y_sc)\n",
    "\n",
    "model_layer2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(hidden_layer_sizes=(64,32),\n",
    "                         activation=\"relu\",\n",
    "                         early_stopping=True,\n",
    "                         random_state=42,\n",
    "                         max_iter=200))\n",
    "]).fit(X_sc[y_sc==1], y_sign[y_sc==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61770a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) scoring functions\n",
    "\n",
    "def score_at_bc(model_layer1, X_bc, avg_sign_given_sc):\n",
    "    \"\"\"Score leads immediately after BC.\"\"\"\n",
    "    p_sc = model_layer1.predict_proba(X_bc)[:,1]\n",
    "    # Rough proxy for sign likelihood\n",
    "    p_sign_proxy = p_sc * avg_sign_given_sc\n",
    "    return p_sc, p_sign_proxy\n",
    "\n",
    "def score_at_sc(model_layer1, model_layer2, X_bc, X_sc):\n",
    "    \"\"\"Update scores for leads that reached SC.\"\"\"\n",
    "    p_sc = model_layer1.predict_proba(X_bc)[:,1]\n",
    "    p_sign_given_sc = model_layer2.predict_proba(X_sc)[:,1]\n",
    "    p_final = p_sc * p_sign_given_sc\n",
    "    return p_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4f8a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) using in practice\n",
    "\n",
    "# Average conditional probability of signing given SC (from training set)\n",
    "avg_sign_given_sc = y_sign[y_sc==1].mean()\n",
    "\n",
    "# 1. At BC time (all leads)\n",
    "p_sc, p_sign_proxy = score_at_bc(model_layer1, X_bc, avg_sign_given_sc)\n",
    "\n",
    "# 2. Later, for those who reached SC\n",
    "mask_sc = (df['net_sc'] >= 1)\n",
    "p_final = score_at_sc(model_layer1, model_layer2, \n",
    "                      X_bc[mask_sc], X_sc[mask_sc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a59b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BC-time scoring (all leads):\n",
      "  Top 1% -> Recall=0.025, Lift=2.55\n",
      "  Top 5% -> Recall=0.075, Lift=1.49\n",
      "  Top 10% -> Recall=0.164, Lift=1.64\n",
      "\n",
      "SC-time re-scoring (only leads that had >=1 SC):\n",
      "  Top 1% -> Recall=0.122, Lift=12.32\n",
      "  Top 5% -> Recall=0.329, Lift=6.59\n",
      "  Top 10% -> Recall=0.468, Lift=4.68\n"
     ]
    }
   ],
   "source": [
    "# 4) evaluate separately \n",
    "# ---- 1. BC-time evaluation (all leads) ----\n",
    "avg_sign_given_sc = y_sign[y_sc == 1].mean()\n",
    "p_sc, p_sign_proxy = score_at_bc(model_layer1, X_bc, avg_sign_given_sc)\n",
    "\n",
    "print(\"\\nBC-time scoring (all leads):\")\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n",
    "\n",
    "# ---- 2. SC-time evaluation (subset that reached SC) ----\n",
    "mask_sc = (df['net_sc'] >= 1)\n",
    "p_final = score_at_sc(model_layer1, model_layer2, X_bc[mask_sc], X_sc[mask_sc])\n",
    "\n",
    "print(\"\\nSC-time re-scoring (only leads that had >=1 SC):\")\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62756a",
   "metadata": {},
   "source": [
    "### NN Model tuning, Randomized Search for both layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4beb6ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best params for Layer 1: {'nn__alpha': np.float64(0.005934145688620425), 'nn__hidden_layer_sizes': (128,), 'nn__learning_rate_init': np.float64(0.008699404067363204), 'nn__solver': 'adam'}\n",
      "Best score (PR-AUC) for Layer 1: 0.7071305107528186\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best params for Layer 2: {'nn__alpha': np.float64(0.00231893825622149), 'nn__hidden_layer_sizes': (64, 32), 'nn__learning_rate_init': np.float64(0.004501524937396013), 'nn__solver': 'adam'}\n",
      "Best score (PR-AUC) for Layer 2: 0.22126000855818034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# -----------------\n",
    "# Layer 1 (BC -> SC)\n",
    "# -----------------\n",
    "param_dist_layer1 = {\n",
    "    \"nn__hidden_layer_sizes\": [(32,), (64,), (128,), (64,32), (128,64)],\n",
    "    \"nn__alpha\": uniform(1e-5, 1e-2),          # L2 regularization\n",
    "    \"nn__learning_rate_init\": uniform(1e-4, 1e-2),\n",
    "    \"nn__solver\": [\"adam\", \"lbfgs\"]\n",
    "}\n",
    "\n",
    "nn_clf_layer1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(\n",
    "        activation=\"relu\",\n",
    "        early_stopping=True,\n",
    "        max_iter=500, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "search_layer1 = RandomizedSearchCV(\n",
    "    nn_clf_layer1,\n",
    "    param_distributions=param_dist_layer1,\n",
    "    n_iter=20,            # try 20 random combos\n",
    "    scoring=\"average_precision\",  # PR-AUC as proxy metric\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "search_layer1.fit(X_layer1, y_sc)\n",
    "\n",
    "print(\"Best params for Layer 1:\", search_layer1.best_params_)\n",
    "print(\"Best score (PR-AUC) for Layer 1:\", search_layer1.best_score_)\n",
    "\n",
    "# -----------------\n",
    "# Layer 2 (SC -> Contract)\n",
    "# -----------------\n",
    "param_dist_layer2 = {\n",
    "    \"nn__hidden_layer_sizes\": [(32,), (64,), (128,), (64,32), (128,64)],\n",
    "    \"nn__alpha\": uniform(1e-5, 1e-2),\n",
    "    \"nn__learning_rate_init\": uniform(1e-4, 1e-2),\n",
    "    \"nn__solver\": [\"adam\", \"lbfgs\"]\n",
    "}\n",
    "\n",
    "nn_clf_layer2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(\n",
    "        activation=\"relu\",\n",
    "        early_stopping=True,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "mask_sc = (df['net_sc'] >= 1)   # only SC leads\n",
    "search_layer2 = RandomizedSearchCV(\n",
    "    nn_clf_layer2,\n",
    "    param_distributions=param_dist_layer2,\n",
    "    n_iter=20,\n",
    "    scoring=\"average_precision\",\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "search_layer2.fit(X_layer2[mask_sc], y_sign[mask_sc])\n",
    "\n",
    "print(\"Best params for Layer 2:\", search_layer2.best_params_)\n",
    "print(\"Best score (PR-AUC) for Layer 2:\", search_layer2.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6786bb",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "Best params for Layer 1: {'nn__alpha': np.float64(0.005934145688620425), 'nn__hidden_layer_sizes': (128,), 'nn__learning_rate_init': np.float64(0.008699404067363204), 'nn__solver': 'adam'}\n",
    "Best score (PR-AUC) for Layer 1: 0.7071305107528186\n",
    "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "Best params for Layer 2: {'nn__alpha': np.float64(0.00231893825622149), 'nn__hidden_layer_sizes': (64, 32), 'nn__learning_rate_init': np.float64(0.004501524937396013), 'nn__solver': 'adam'}\n",
    "Best score (PR-AUC) for Layer 2: 0.22126000855818034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d06b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BC-time scoring (all leads, tuned Layer 1):\n",
      "  Top 1% -> Recall=0.022, Lift=2.21\n",
      "  Top 5% -> Recall=0.081, Lift=1.63\n",
      "  Top 10% -> Recall=0.142, Lift=1.42\n",
      "\n",
      "SC-time re-scoring (only SC leads, tuned Layer 1 + Layer 2):\n",
      "  Top 1% -> Recall=0.120, Lift=12.15\n",
      "  Top 5% -> Recall=0.342, Lift=6.86\n",
      "  Top 10% -> Recall=0.497, Lift=4.97\n"
     ]
    }
   ],
   "source": [
    "# with these params\n",
    "# --- retrain with best hyperparams ---\n",
    "\n",
    "# Layer 1: BC â†’ SC\n",
    "best_layer1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(\n",
    "        hidden_layer_sizes=(128,),\n",
    "        alpha=0.005934145688620425,\n",
    "        learning_rate_init=0.008699404067363204,\n",
    "        solver=\"adam\",\n",
    "        activation=\"relu\",\n",
    "        early_stopping=True,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "best_layer1.fit(X_layer1, y_sc)\n",
    "\n",
    "# Layer 2: SC â†’ Contract\n",
    "mask_sc = (df['net_sc'] >= 1)\n",
    "best_layer2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        alpha=0.00231893825622149,\n",
    "        learning_rate_init=0.004501524937396013,\n",
    "        solver=\"adam\",\n",
    "        activation=\"relu\",\n",
    "        early_stopping=True,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "best_layer2.fit(X_layer2[mask_sc], y_sign[mask_sc])\n",
    "\n",
    "# --- evaluation ---\n",
    "print(\"\\nBC-time scoring (all leads, tuned Layer 1):\")\n",
    "avg_sign_given_sc = y_sign[mask_sc].mean()\n",
    "p_sc, p_sign_proxy = score_at_bc(best_layer1, X_layer1, avg_sign_given_sc)\n",
    "\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n",
    "\n",
    "print(\"\\nSC-time re-scoring (only SC leads, tuned Layer 1 + Layer 2):\")\n",
    "p_final = score_at_sc(best_layer1, best_layer2, X_layer1[mask_sc], X_layer2[mask_sc])\n",
    "\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80894723",
   "metadata": {},
   "source": [
    "hyperparam tuning wasnt a big gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4ab9e",
   "metadata": {},
   "source": [
    "### NN imbalance handling. New model for Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "def355c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define focal loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        ce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        fl = ce * ((1 - p_t) ** gamma)\n",
    "        if alpha is not None:\n",
    "            alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "            fl *= alpha_t\n",
    "        return K.mean(fl)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812a66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple NN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def make_model(input_dim, hidden_layers=(64,32), alpha=0.25, gamma=2.0):\n",
    "    model = Sequential()\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        model.add(Dense(units, activation=\"relu\", input_dim=input_dim if i==0 else None))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=focal_loss(gamma=gamma, alpha=alpha),\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1b6e7",
   "metadata": {},
   "source": [
    "Training flow\n",
    "\n",
    "Layer 1: train on X_layer1, y_sc\n",
    "\n",
    "Layer 2: train on X_layer2[mask_sc], y_sign[mask_sc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66fcbe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 - 1s - 10ms/step - auc: 0.5199 - loss: 0.1201 - precision: 0.6204 - recall: 0.2824 - val_auc: 0.5925 - val_loss: 0.0726 - val_precision: 0.8000 - val_recall: 0.0773\n",
      "Epoch 2/50\n",
      "113/113 - 0s - 2ms/step - auc: 0.5435 - loss: 0.1007 - precision: 0.6506 - recall: 0.2271 - val_auc: 0.6094 - val_loss: 0.0711 - val_precision: 0.8767 - val_recall: 0.0302\n",
      "Epoch 3/50\n",
      "113/113 - 0s - 2ms/step - auc: 0.5689 - loss: 0.0959 - precision: 0.6852 - recall: 0.1792 - val_auc: 0.6050 - val_loss: 0.0712 - val_precision: 0.7756 - val_recall: 0.1107\n",
      "Epoch 4/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.5708 - loss: 0.0961 - precision: 0.7003 - recall: 0.1278 - val_auc: 0.6197 - val_loss: 0.0702 - val_precision: 0.8380 - val_recall: 0.0561\n",
      "Epoch 5/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.5797 - loss: 0.0908 - precision: 0.7098 - recall: 0.0953 - val_auc: 0.6171 - val_loss: 0.0701 - val_precision: 0.9057 - val_recall: 0.0452\n",
      "Epoch 6/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.5843 - loss: 0.0903 - precision: 0.7185 - recall: 0.0792 - val_auc: 0.6063 - val_loss: 0.0706 - val_precision: 0.7500 - val_recall: 0.0820\n",
      "Epoch 7/50\n",
      "113/113 - 0s - 4ms/step - auc: 0.5938 - loss: 0.0853 - precision: 0.7349 - recall: 0.0808 - val_auc: 0.6285 - val_loss: 0.0703 - val_precision: 0.8529 - val_recall: 0.0683\n",
      "Epoch 8/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.5911 - loss: 0.0896 - precision: 0.7352 - recall: 0.0780 - val_auc: 0.6336 - val_loss: 0.0706 - val_precision: 0.8495 - val_recall: 0.1277\n",
      "Epoch 9/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.5987 - loss: 0.0822 - precision: 0.7661 - recall: 0.0596 - val_auc: 0.6334 - val_loss: 0.0699 - val_precision: 0.8626 - val_recall: 0.1065\n",
      "Epoch 10/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6015 - loss: 0.0812 - precision: 0.7610 - recall: 0.0687 - val_auc: 0.6349 - val_loss: 0.0698 - val_precision: 0.9659 - val_recall: 0.0401\n",
      "Epoch 11/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6091 - loss: 0.0796 - precision: 0.7988 - recall: 0.0630 - val_auc: 0.6340 - val_loss: 0.0697 - val_precision: 0.9263 - val_recall: 0.0415\n",
      "Epoch 12/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6076 - loss: 0.0793 - precision: 0.8149 - recall: 0.0626 - val_auc: 0.6380 - val_loss: 0.0695 - val_precision: 0.8675 - val_recall: 0.0957\n",
      "Epoch 13/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6069 - loss: 0.0803 - precision: 0.8031 - recall: 0.0613 - val_auc: 0.6348 - val_loss: 0.0696 - val_precision: 0.8807 - val_recall: 0.0905\n",
      "Epoch 14/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6110 - loss: 0.0779 - precision: 0.8011 - recall: 0.0658 - val_auc: 0.6400 - val_loss: 0.0695 - val_precision: 0.8915 - val_recall: 0.0891\n",
      "Epoch 15/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6114 - loss: 0.0786 - precision: 0.7927 - recall: 0.0660 - val_auc: 0.6385 - val_loss: 0.0694 - val_precision: 0.8733 - val_recall: 0.0617\n",
      "Epoch 16/50\n",
      "113/113 - 1s - 5ms/step - auc: 0.6142 - loss: 0.0758 - precision: 0.8164 - recall: 0.0627 - val_auc: 0.6375 - val_loss: 0.0695 - val_precision: 0.8687 - val_recall: 0.1371\n",
      "Epoch 17/50\n",
      "113/113 - 0s - 4ms/step - auc: 0.6094 - loss: 0.0745 - precision: 0.8019 - recall: 0.0680 - val_auc: 0.6400 - val_loss: 0.0696 - val_precision: 0.9327 - val_recall: 0.0914\n",
      "Epoch 18/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6144 - loss: 0.0760 - precision: 0.8326 - recall: 0.0684 - val_auc: 0.6438 - val_loss: 0.0692 - val_precision: 0.9375 - val_recall: 0.0848\n",
      "Epoch 19/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6143 - loss: 0.0746 - precision: 0.8237 - recall: 0.0714 - val_auc: 0.6424 - val_loss: 0.0692 - val_precision: 0.9170 - val_recall: 0.1145\n",
      "Epoch 20/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6182 - loss: 0.0738 - precision: 0.8312 - recall: 0.0764 - val_auc: 0.6443 - val_loss: 0.0693 - val_precision: 0.9118 - val_recall: 0.1169\n",
      "Epoch 21/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6147 - loss: 0.0737 - precision: 0.8316 - recall: 0.0743 - val_auc: 0.6500 - val_loss: 0.0688 - val_precision: 0.9329 - val_recall: 0.1310\n",
      "Epoch 22/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6160 - loss: 0.0725 - precision: 0.8519 - recall: 0.0731 - val_auc: 0.6492 - val_loss: 0.0689 - val_precision: 0.9060 - val_recall: 0.1362\n",
      "Epoch 23/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6174 - loss: 0.0728 - precision: 0.8584 - recall: 0.0778 - val_auc: 0.6533 - val_loss: 0.0692 - val_precision: 0.8939 - val_recall: 0.1946\n",
      "Epoch 24/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6163 - loss: 0.0729 - precision: 0.8519 - recall: 0.0811 - val_auc: 0.6556 - val_loss: 0.0683 - val_precision: 0.9431 - val_recall: 0.1249\n",
      "Epoch 25/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6160 - loss: 0.0722 - precision: 0.8712 - recall: 0.0804 - val_auc: 0.6547 - val_loss: 0.0682 - val_precision: 0.9290 - val_recall: 0.1480\n",
      "Epoch 26/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6246 - loss: 0.0715 - precision: 0.8561 - recall: 0.0936 - val_auc: 0.6559 - val_loss: 0.0682 - val_precision: 0.9266 - val_recall: 0.1428\n",
      "Epoch 27/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6209 - loss: 0.0717 - precision: 0.8614 - recall: 0.0870 - val_auc: 0.6577 - val_loss: 0.0681 - val_precision: 0.9234 - val_recall: 0.1876\n",
      "Epoch 28/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6250 - loss: 0.0714 - precision: 0.8761 - recall: 0.0956 - val_auc: 0.6621 - val_loss: 0.0679 - val_precision: 0.9201 - val_recall: 0.2008\n",
      "Epoch 29/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6216 - loss: 0.0713 - precision: 0.8870 - recall: 0.0897 - val_auc: 0.6624 - val_loss: 0.0679 - val_precision: 0.9323 - val_recall: 0.2012\n",
      "Epoch 30/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6225 - loss: 0.0716 - precision: 0.8713 - recall: 0.0915 - val_auc: 0.6621 - val_loss: 0.0673 - val_precision: 0.9211 - val_recall: 0.2144\n",
      "Epoch 31/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6248 - loss: 0.0707 - precision: 0.8761 - recall: 0.0964 - val_auc: 0.6631 - val_loss: 0.0670 - val_precision: 0.9382 - val_recall: 0.2074\n",
      "Epoch 32/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6235 - loss: 0.0698 - precision: 0.8645 - recall: 0.1101 - val_auc: 0.6629 - val_loss: 0.0668 - val_precision: 0.9669 - val_recall: 0.1927\n",
      "Epoch 33/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6243 - loss: 0.0703 - precision: 0.8889 - recall: 0.0942 - val_auc: 0.6652 - val_loss: 0.0667 - val_precision: 0.9286 - val_recall: 0.2267\n",
      "Epoch 34/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6259 - loss: 0.0702 - precision: 0.8810 - recall: 0.0992 - val_auc: 0.6648 - val_loss: 0.0666 - val_precision: 0.9331 - val_recall: 0.2300\n",
      "Epoch 35/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6239 - loss: 0.0703 - precision: 0.8892 - recall: 0.1011 - val_auc: 0.6681 - val_loss: 0.0668 - val_precision: 0.9232 - val_recall: 0.2380\n",
      "Epoch 36/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6280 - loss: 0.0695 - precision: 0.8948 - recall: 0.0942 - val_auc: 0.6687 - val_loss: 0.0664 - val_precision: 0.9267 - val_recall: 0.2385\n",
      "Epoch 37/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6273 - loss: 0.0699 - precision: 0.8886 - recall: 0.1023 - val_auc: 0.6636 - val_loss: 0.0664 - val_precision: 0.9393 - val_recall: 0.2262\n",
      "Epoch 38/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6275 - loss: 0.0700 - precision: 0.8959 - recall: 0.0963 - val_auc: 0.6643 - val_loss: 0.0664 - val_precision: 0.9291 - val_recall: 0.2347\n",
      "Epoch 39/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6251 - loss: 0.0700 - precision: 0.8804 - recall: 0.1029 - val_auc: 0.6647 - val_loss: 0.0664 - val_precision: 0.9266 - val_recall: 0.2380\n",
      "Epoch 40/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6272 - loss: 0.0695 - precision: 0.9000 - recall: 0.0965 - val_auc: 0.6678 - val_loss: 0.0660 - val_precision: 0.9246 - val_recall: 0.2427\n",
      "Epoch 41/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6315 - loss: 0.0693 - precision: 0.9032 - recall: 0.0990 - val_auc: 0.6707 - val_loss: 0.0659 - val_precision: 0.9221 - val_recall: 0.2399\n",
      "Epoch 42/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6262 - loss: 0.0694 - precision: 0.8991 - recall: 0.0977 - val_auc: 0.6636 - val_loss: 0.0656 - val_precision: 0.9375 - val_recall: 0.2333\n",
      "Epoch 43/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6294 - loss: 0.0693 - precision: 0.8966 - recall: 0.1021 - val_auc: 0.6698 - val_loss: 0.0654 - val_precision: 0.9349 - val_recall: 0.2370\n",
      "Epoch 44/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6305 - loss: 0.0693 - precision: 0.8802 - recall: 0.1019 - val_auc: 0.6599 - val_loss: 0.0657 - val_precision: 0.9296 - val_recall: 0.2427\n",
      "Epoch 45/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6326 - loss: 0.0691 - precision: 0.9062 - recall: 0.1014 - val_auc: 0.6700 - val_loss: 0.0656 - val_precision: 0.9314 - val_recall: 0.2366\n",
      "Epoch 46/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6335 - loss: 0.0691 - precision: 0.8983 - recall: 0.1061 - val_auc: 0.6669 - val_loss: 0.0658 - val_precision: 0.9238 - val_recall: 0.2455\n",
      "Epoch 47/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6290 - loss: 0.0691 - precision: 0.8982 - recall: 0.1009 - val_auc: 0.6721 - val_loss: 0.0656 - val_precision: 0.9037 - val_recall: 0.2653\n",
      "Epoch 48/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6329 - loss: 0.0693 - precision: 0.9065 - recall: 0.0972 - val_auc: 0.6684 - val_loss: 0.0654 - val_precision: 0.9381 - val_recall: 0.2356\n",
      "Epoch 49/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6306 - loss: 0.0689 - precision: 0.8895 - recall: 0.1004 - val_auc: 0.6692 - val_loss: 0.0653 - val_precision: 0.9236 - val_recall: 0.2451\n",
      "Epoch 50/50\n",
      "113/113 - 0s - 3ms/step - auc: 0.6285 - loss: 0.0691 - precision: 0.9010 - recall: 0.0986 - val_auc: 0.6666 - val_loss: 0.0655 - val_precision: 0.9313 - val_recall: 0.2427\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 - 2s - 26ms/step - auc: 0.5368 - loss: 0.0481 - precision: 0.0457 - recall: 0.0162 - val_auc: 0.5656 - val_loss: 0.0190 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.5748 - loss: 0.0318 - precision: 0.0556 - recall: 0.0020 - val_auc: 0.6304 - val_loss: 0.0186 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.6002 - loss: 0.0281 - precision: 0.2500 - recall: 0.0020 - val_auc: 0.7156 - val_loss: 0.0180 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.6188 - loss: 0.0264 - precision: 0.1429 - recall: 0.0020 - val_auc: 0.7316 - val_loss: 0.0180 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.6466 - loss: 0.0250 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7502 - val_loss: 0.0168 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.6509 - loss: 0.0255 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7687 - val_loss: 0.0161 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/50\n",
      "67/67 - 0s - 3ms/step - auc: 0.6692 - loss: 0.0240 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7700 - val_loss: 0.0165 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.6903 - loss: 0.0232 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7682 - val_loss: 0.0160 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7028 - loss: 0.0228 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7738 - val_loss: 0.0159 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7045 - loss: 0.0234 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7748 - val_loss: 0.0171 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7199 - loss: 0.0225 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7832 - val_loss: 0.0159 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7155 - loss: 0.0224 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7907 - val_loss: 0.0167 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7220 - loss: 0.0221 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7942 - val_loss: 0.0150 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7232 - loss: 0.0218 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7943 - val_loss: 0.0153 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7315 - loss: 0.0213 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7997 - val_loss: 0.0156 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7166 - loss: 0.0214 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8004 - val_loss: 0.0157 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7516 - loss: 0.0206 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7952 - val_loss: 0.0158 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7475 - loss: 0.0209 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8008 - val_loss: 0.0149 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 19/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7417 - loss: 0.0210 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8082 - val_loss: 0.0149 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7468 - loss: 0.0204 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8059 - val_loss: 0.0155 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/50\n",
      "67/67 - 0s - 5ms/step - auc: 0.7448 - loss: 0.0214 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8052 - val_loss: 0.0156 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/50\n",
      "67/67 - 0s - 3ms/step - auc: 0.7541 - loss: 0.0208 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8079 - val_loss: 0.0156 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/50\n",
      "67/67 - 0s - 3ms/step - auc: 0.7412 - loss: 0.0205 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8024 - val_loss: 0.0147 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/50\n",
      "67/67 - 0s - 3ms/step - auc: 0.7409 - loss: 0.0205 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7979 - val_loss: 0.0152 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7424 - loss: 0.0209 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8002 - val_loss: 0.0157 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/50\n",
      "67/67 - 1s - 7ms/step - auc: 0.7591 - loss: 0.0203 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.7995 - val_loss: 0.0150 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/50\n",
      "67/67 - 0s - 5ms/step - auc: 0.7392 - loss: 0.0210 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_auc: 0.8047 - val_loss: 0.0158 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/50\n",
      "67/67 - 0s - 4ms/step - auc: 0.7579 - loss: 0.0204 - precision: 1.0000 - recall: 0.0020 - val_auc: 0.7993 - val_loss: 0.0169 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x214442ea3d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer 1 model\n",
    "model_layer1 = make_model(input_dim=X_layer1.shape[1], hidden_layers=(128,))\n",
    "model_layer1.fit(X_layer1, y_sc, \n",
    "                 validation_split=0.2,\n",
    "                 epochs=50, batch_size=128, verbose=2,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "\n",
    "# Layer 2 model\n",
    "mask_sc = (df['net_sc'] >= 1)\n",
    "model_layer2 = make_model(input_dim=X_layer2.shape[1], hidden_layers=(64,32))\n",
    "model_layer2.fit(X_layer2[mask_sc], y_sign[mask_sc],\n",
    "                 validation_split=0.2,\n",
    "                 epochs=50, batch_size=128, verbose=2,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f67b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring functions\n",
    "def score_at_bc_keras(model_layer1, X_bc, avg_sign_given_sc):\n",
    "    p_sc = model_layer1.predict(X_bc).flatten()\n",
    "    p_sign_proxy = p_sc * avg_sign_given_sc\n",
    "    return p_sc, p_sign_proxy\n",
    "\n",
    "def score_at_sc_keras(model_layer1, model_layer2, X_bc, X_sc):\n",
    "    p_sc = model_layer1.predict(X_bc).flatten()\n",
    "    p_sign_given_sc = model_layer2.predict(X_sc).flatten()\n",
    "    p_final = p_sc * p_sign_given_sc\n",
    "    return p_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "002f11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m562/562\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step\n",
      "\n",
      "BC-time scoring (all leads, focal loss):\n",
      "  Top 1% -> Recall=0.012, Lift=1.19\n",
      "  Top 5% -> Recall=0.080, Lift=1.59\n",
      "  Top 10% -> Recall=0.154, Lift=1.54\n",
      "\u001b[1m335/335\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m335/335\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step\n",
      "\n",
      "SC-time re-scoring (only SC leads, focal loss):\n",
      "  Top 1% -> Recall=0.054, Lift=5.47\n",
      "  Top 5% -> Recall=0.249, Lift=4.99\n",
      "  Top 10% -> Recall=0.375, Lift=3.75\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BC vs SC performance\n",
    "# Average conditional sign rate (for BC proxy score)\n",
    "avg_sign_given_sc = y_sign[mask_sc].mean()\n",
    "\n",
    "# 1. BC-time evaluation\n",
    "p_sc, p_sign_proxy = score_at_bc_keras(model_layer1, X_layer1, avg_sign_given_sc)\n",
    "print(\"\\nBC-time scoring (all leads, focal loss):\")\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n",
    "\n",
    "# 2. SC-time evaluation\n",
    "p_final = score_at_sc_keras(model_layer1, model_layer2, X_layer1[mask_sc], X_layer2[mask_sc])\n",
    "print(\"\\nSC-time re-scoring (only SC leads, focal loss):\")\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c176bd",
   "metadata": {},
   "source": [
    "ðŸ”Ž Interpretation\n",
    "\n",
    "focal loss is very useful when models collapse to predicting negatives everywhere.\n",
    "\n",
    "but your NN (even untuned) was already separating signal well enough, so focal loss may have been too aggressive and actually hurt ranking performance.\n",
    "\n",
    "with ~3% conversion, youâ€™re in a â€œrare but not ultra-rareâ€ regime â€” class weights or sample weights often do just as well, sometimes better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be526419",
   "metadata": {},
   "source": [
    "### Step back from Keras (focal loss) because no improvement. Class weights for first NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a0e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BC-time scoring (all leads, sklearn NN + weights):\n",
      "  Top 1% -> Recall=0.020, Lift=2.04\n",
      "  Top 5% -> Recall=0.078, Lift=1.56\n",
      "  Top 10% -> Recall=0.144, Lift=1.44\n",
      "\n",
      "SC-time re-scoring (only SC leads, sklearn NN + weights):\n",
      "  Top 1% -> Recall=0.037, Lift=3.76\n",
      "  Top 5% -> Recall=0.210, Lift=4.21\n",
      "  Top 10% -> Recall=0.351, Lift=3.51\n"
     ]
    }
   ],
   "source": [
    "# compute weights\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# for layer 1 (BCâ†’SC)\n",
    "sw_layer1 = compute_sample_weight(class_weight=\"balanced\", y=y_sc)\n",
    "\n",
    "# for layer 2 (SCâ†’Contract)\n",
    "mask_sc = (df['net_sc'] >= 1)\n",
    "sw_layer2 = compute_sample_weight(class_weight=\"balanced\", y=y_sign[mask_sc])\n",
    "\n",
    "#retrain NN with weights\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# layer 1 model\n",
    "nn_layer1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(hidden_layer_sizes=(128,),\n",
    "                         activation=\"relu\",\n",
    "                         early_stopping=True,\n",
    "                         max_iter=500,\n",
    "                         random_state=42))\n",
    "])\n",
    "nn_layer1.fit(X_layer1, y_sc, nn__sample_weight=sw_layer1)\n",
    "\n",
    "# layer 2 model\n",
    "nn_layer2 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nn\", MLPClassifier(hidden_layer_sizes=(64,32),\n",
    "                         activation=\"relu\",\n",
    "                         early_stopping=True,\n",
    "                         max_iter=500,\n",
    "                         random_state=42))\n",
    "])\n",
    "nn_layer2.fit(X_layer2[mask_sc], y_sign[mask_sc], nn__sample_weight=sw_layer2)\n",
    "\n",
    "#evaluate again\n",
    "\n",
    "# 1. BC-time\n",
    "avg_sign_given_sc = y_sign[mask_sc].mean()\n",
    "p_sc = nn_layer1.predict_proba(X_layer1)[:,1]\n",
    "p_sign_proxy = p_sc * avg_sign_given_sc\n",
    "\n",
    "print(\"\\nBC-time scoring (all leads, sklearn NN + weights):\")\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign, p_sign_proxy, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n",
    "\n",
    "# 2. SC-time\n",
    "p_sc = nn_layer1.predict_proba(X_layer1[mask_sc])[:,1]\n",
    "p_sign_given_sc = nn_layer2.predict_proba(X_layer2[mask_sc])[:,1]\n",
    "p_final = p_sc * p_sign_given_sc\n",
    "\n",
    "print(\"\\nSC-time re-scoring (only SC leads, sklearn NN + weights):\")\n",
    "for pct in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    lift = lift_at_percent(y_sign[mask_sc], p_final, top_percent=pct)\n",
    "    print(f\"  Top {int(pct*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266b796",
   "metadata": {},
   "source": [
    "At BC-time: class weights donâ€™t hurt or help much (features too weak).\n",
    "\n",
    "At SC-time: plain tuned NN is still your best performer so far.\n",
    "\n",
    "Focal loss â†’ too aggressive.\n",
    "\n",
    "Class weights â†’ overcorrected, lost ranking sharpness.\n",
    "\n",
    "Plain tuned NN (no imbalance tricks) gave the strongest lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0027e6",
   "metadata": {},
   "source": [
    "## XGBoost Cascade Pipeline (Comparing NN to a tree based model, 2nd benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2883bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:29:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\ElifYilmaz\\OneDrive - Enpal B.V\\Desktop\\New folder\\Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [10:29:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1% â†’ Recall=0.051, Lift=5.22\n",
      "Top 5% â†’ Recall=0.280, Lift=5.61\n",
      "Top 10% â†’ Recall=0.466, Lift=4.66\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ------------------\n",
    "# Layer 1: BC â†’ SC\n",
    "# ------------------\n",
    "scale_pos_weight_sc = (len(y_sc) - y_sc.sum()) / y_sc.sum()\n",
    "\n",
    "model_sc = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_pos_weight_sc,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "model_sc.fit(X_layer1.loc[X_train.index], (df.loc[X_train.index, 'net_sc'] >= 1).astype(int))\n",
    "\n",
    "# ------------------\n",
    "# Layer 2: SC â†’ Contract\n",
    "# ------------------\n",
    "mask_train_sc = (df.loc[X_train.index, 'net_sc'] >= 1)\n",
    "scale_pos_weight_sign = (mask_train_sc.sum() - y_train[mask_train_sc].sum()) / y_train[mask_train_sc].sum()\n",
    "\n",
    "model_sign = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_pos_weight_sign,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "model_sign.fit(X_layer2.loc[X_train.index][mask_train_sc], y_train[mask_train_sc])\n",
    "\n",
    "# ------------------\n",
    "# Evaluation on test set\n",
    "# ------------------\n",
    "# Layer 1 probs\n",
    "p_sc = model_sc.predict_proba(X_layer1.loc[X_test.index])[:, 1]\n",
    "\n",
    "# Layer 2 probs (only for test leads that actually had SC)\n",
    "mask_test_sc = (df.loc[X_test.index, 'net_sc'] >= 1)\n",
    "p_sign_cond_sc = np.zeros(len(X_test))\n",
    "p_sign_cond_sc[mask_test_sc] = model_sign.predict_proba(X_layer2.loc[X_test.index][mask_test_sc])[:, 1]\n",
    "\n",
    "# Final cascade probability = P(SC) * P(Sign|SC)\n",
    "p_final = p_sc * p_sign_cond_sc\n",
    "\n",
    "# ------------------\n",
    "# Metrics\n",
    "# ------------------\n",
    "for top in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_test, p_final, top)\n",
    "    lift = lift_at_percent(y_test, p_final, top)\n",
    "    print(f\"Top {int(top*100)}% â†’ Recall={rec:.3f}, Lift={lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2dae410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BC-time scoring (all leads):\n",
      "  Top 1% -> Recall=0.008, Lift=0.87\n",
      "  Top 5% -> Recall=0.051, Lift=1.02\n",
      "  Top 10% -> Recall=0.119, Lift=1.19\n",
      "\n",
      "SC-time re-scoring (only SC leads):\n",
      "  Top 1% -> Recall=0.051, Lift=5.25\n",
      "  Top 5% -> Recall=0.314, Lift=6.29\n",
      "  Top 10% -> Recall=0.415, Lift=4.17\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# BC-time scoring (all leads, proxy contract score)\n",
    "# ------------------\n",
    "# At BC we only know P(SC). We don't know P(Sign|SC) yet,\n",
    "# so a simple proxy is: P(contract) â‰ˆ P(SC) * avg(sign rate given SC in train)\n",
    "avg_sign_given_sc = y_train[df.loc[X_train.index, 'net_sc'] >= 1].mean()\n",
    "\n",
    "p_bc_proxy = p_sc * avg_sign_given_sc\n",
    "\n",
    "print(\"\\nBC-time scoring (all leads):\")\n",
    "for top in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_test, p_bc_proxy, top)\n",
    "    lift = lift_at_percent(y_test, p_bc_proxy, top)\n",
    "    print(f\"  Top {int(top*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n",
    "\n",
    "# ------------------\n",
    "# SC-time re-scoring (only leads that had >=1 SC)\n",
    "# ------------------\n",
    "mask_test_sc = (df.loc[X_test.index, 'net_sc'] >= 1)\n",
    "p_sc_only = model_sign.predict_proba(X_layer2.loc[X_test.index][mask_test_sc])[:, 1]\n",
    "\n",
    "print(\"\\nSC-time re-scoring (only SC leads):\")\n",
    "for top in [0.01, 0.05, 0.10]:\n",
    "    rec = recall_at_percent(y_test[mask_test_sc], p_sc_only, top)\n",
    "    lift = lift_at_percent(y_test[mask_test_sc], p_sc_only, top)\n",
    "    print(f\"  Top {int(top*100)}% -> Recall={rec:.3f}, Lift={lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b7499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a5972f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
