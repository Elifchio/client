{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd0fa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "287d505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests\n",
    "file_path = r\"C:\\Users\\ElifYilmaz\\Downloads\\project 2.0.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# qualitative \n",
    "\n",
    "file2 = r\"C:\\Users\\ElifYilmaz\\Downloads\\info client.csv\"\n",
    "\n",
    "df2 = pd.read_csv(file2)\n",
    "df2.columns = df2.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353766",
   "metadata": {},
   "source": [
    "#### Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d1690b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing appointment types\n",
    "\n",
    "type_mapping = {\n",
    "    'FU': 'FU',\n",
    "    'FUVD': 'FU', \n",
    "    'REMINDER': 'FU',\n",
    "    'VDFIELD': 'SC',\n",
    "    'VD': 'SC'\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "df['grouped_type'] = df['type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ad51c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition for net events (not cancelled and past)\n",
    "df['is_net_event'] = df['deletedat'].isna() & df['is_passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "05bbb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross counts (all events)\n",
    "gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "# Net counts (only events that actually happened)\n",
    "net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "net_counts.columns = [f'net_{col}' for col in net_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "511b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gross and net counts\n",
    "counts_df = gross_counts.join(net_counts, how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8011647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn self ipa event to bbinary \n",
    "df2['selfipa_done'] = df2['selfipaimportedat'].notnull().astype(int)\n",
    "df2.drop('selfipaimportedat', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9be656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert createdat to datetime\n",
    "df['createdat'] = pd.to_datetime(df['createdat'])\n",
    "\n",
    "# Step 2: Get first gross SC and first net FU timestamps\n",
    "first_gross_sc = df[df['grouped_type'] == 'SC'].groupby('requestid')['createdat'].min()\n",
    "first_net_fu = df[(df['grouped_type'] == 'FU') & df['is_net_event']].groupby('requestid')['createdat'].min()\n",
    "\n",
    "# Step 3: Calculate time difference in hours\n",
    "counts_df['time_first_sc_to_first_net_fu'] = (first_net_fu - first_gross_sc).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "88cecdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gross_SC\n",
       "1     15250\n",
       "2      3013\n",
       "3       700\n",
       "4       198\n",
       "5        48\n",
       "6        12\n",
       "7         6\n",
       "10        2\n",
       "23        1\n",
       "8         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop older requests that havent had a sc since march\n",
    "counts_df = counts_df[counts_df['gross_SC'] != 0]\n",
    "counts_df['gross_SC'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d420799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (19231, 5)\n",
      "No missing values: True\n"
     ]
    }
   ],
   "source": [
    "# handle missing value for customers who never made to fu\n",
    "counts_df['time_first_sc_to_first_net_fu'] = counts_df['time_first_sc_to_first_net_fu'].fillna(-1)\n",
    "\n",
    "# Check final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {counts_df.shape}\")\n",
    "print(f\"No missing values: {counts_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c23d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negative time differences (not NaN fills): 22\n",
      "Positive time differences: 7351\n",
      "NaN fills (-1): 11858\n",
      "\n",
      "Real negative values range: -4146.53 to -22.24 hours\n"
     ]
    }
   ],
   "source": [
    "# problematic entries with negative time difference: \n",
    "# Separate real negative times from NaN fill values\n",
    "real_negative = counts_df[\n",
    "    (counts_df['time_first_sc_to_first_net_fu'] < 0) & \n",
    "    (counts_df['time_first_sc_to_first_net_fu'] != -1)\n",
    "]\n",
    "\n",
    "actual_valid_times = counts_df[counts_df['time_first_sc_to_first_net_fu'] >= 0]\n",
    "\n",
    "print(f\"Actual negative time differences (not NaN fills): {len(real_negative)}\")\n",
    "print(f\"Positive time differences: {len(actual_valid_times)}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "if len(real_negative) > 0:\n",
    "    print(f\"\\nReal negative values range: {real_negative['time_first_sc_to_first_net_fu'].min():.2f} to {real_negative['time_first_sc_to_first_net_fu'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "06a370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle negative time difference\n",
    "# Remove outliers entirely (set to NaN, then fill with -1)\n",
    "counts_df.loc[counts_df['time_first_sc_to_first_net_fu'] < -1, 'time_first_sc_to_first_net_fu'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aec7cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing outliers:\n",
      "Negative values (excluding -1 fills): 0\n",
      "Valid positive time differences: 7351\n",
      "NaN fills (-1): 11880\n",
      "\n",
      "Final dataset shape: (19231, 5)\n",
      "Ready to join with target variable!\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Negative values (excluding -1 fills): {((counts_df['time_first_sc_to_first_net_fu'] < 0) & (counts_df['time_first_sc_to_first_net_fu'] != -1)).sum()}\")\n",
    "print(f\"Valid positive time differences: {(counts_df['time_first_sc_to_first_net_fu'] > 0).sum()}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {counts_df.shape}\")\n",
    "print(\"Ready to join with target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "98124e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2039</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3279</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.043056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3729</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.835556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3852</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.193889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requestid  gross_FU  gross_SC  net_FU  net_SC  \\\n",
       "0       2039         0         2     0.0     0.0   \n",
       "1       3279         0         1     0.0     1.0   \n",
       "2       3583         1         1     1.0     1.0   \n",
       "3       3729         8         1     8.0     1.0   \n",
       "4       3852         2         1     2.0     1.0   \n",
       "\n",
       "   time_first_sc_to_first_net_fu  \n",
       "0                      -1.000000  \n",
       "1                      -1.000000  \n",
       "2                     120.043056  \n",
       "3                      73.835556  \n",
       "4                      42.193889  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point requestid is the index. make it explicit\n",
    "\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b02d0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (19245, 14)\n",
      "Target variable distribution:\n",
      "netcontractsigned\n",
      "0.0    18809\n",
      "1.0      436\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['requestid', 'gross_FU', 'gross_SC', 'net_FU', 'net_SC', 'time_first_sc_to_first_net_fu', 'zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore', 'netcontractsigned', 'selfipa_done']\n"
     ]
    }
   ],
   "source": [
    "# Left join to add target variable\n",
    "#final_df1 = counts_df.merge(contract, on='requestid', how='left')\n",
    "# unnecessairy join because df2 has netcontractsigned info \n",
    "\n",
    "final_df = counts_df.merge(df2, on='requestid', how='left')\n",
    "\n",
    "\n",
    "# Fill non-matching requestids with 0 for the target\n",
    "final_df['netcontractsigned'] = final_df['netcontractsigned'].fillna(0)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(final_df['netcontractsigned'].value_counts())\n",
    "print(f\"\\nColumns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38cbb0",
   "metadata": {},
   "source": [
    "#### Encoding and Null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "452d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of bills have -1, 1 and 0\n",
    "\n",
    "# set wrong bill values to nan\n",
    "# Replace specific values with NaN\n",
    "final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea01f",
   "metadata": {},
   "source": [
    "Optional for bill handling \n",
    "def clean_billing_column(series, column_name):\n",
    "    \"\"\"Clean billing column: set negative values and extreme outliers to replacement value\"\"\"\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    series_clean = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Define reasonable bounds (adjust these based on your domain knowledge)\n",
    "    min_reasonable = 0  # Bills shouldn't be negative\n",
    "    max_reasonable = 2000  # Adjust based on your market (e.g., â‚¬1000/month seems high)\n",
    "    \n",
    "    # Count issues for reporting\n",
    "    negative_count = (series_clean < min_reasonable).sum()\n",
    "    outlier_count = (series_clean > max_reasonable).sum()\n",
    "    \n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  - Negative values: {negative_count}\")\n",
    "    print(f\"  - Values > {max_reasonable}: {outlier_count}\")\n",
    "    print(f\"  - Original NaN: {series.isna().sum()}\")\n",
    "    \n",
    "    # Replace problematic values\n",
    "    series_clean.loc[series_clean < min_reasonable] = np.nan  # or -1\n",
    "    series_clean.loc[series_clean > max_reasonable] = np.nan  # or -1\n",
    "    \n",
    "    return series_clean\n",
    "\n",
    "# Apply cleaning\n",
    "final_df['heatingbill'] = clean_billing_column(final_df['heatingbill'], 'heatingbill')\n",
    "final_df['electricitybill'] = clean_billing_column(final_df['electricitybill'], 'electricitybill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5f97be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicator correlations with target:\n",
      "electricitybill_missing          -0.047029\n",
      "desiredinstallationend_missing   -0.010914\n",
      "evaluationtime_missing           -0.010865\n",
      "mktgparamscore_missing           -0.003229\n",
      "zipregion_missing                 0.010433\n",
      "heatingbill_missing               0.018988\n",
      "Name: netcontractsigned, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# before encoding, mark the missingness inside the row;\n",
    "missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore']\n",
    "\n",
    "for col in missing_cols:\n",
    "    final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "\n",
    "# Check the predictive power of missing indicators\n",
    "print(\"Missing indicator correlations with target:\")\n",
    "missing_indicators = [f'{col}_missing' for col in missing_cols]\n",
    "missing_corrs = final_df[missing_indicators + ['netcontractsigned']].corr()['netcontractsigned'].drop('netcontractsigned')\n",
    "print(missing_corrs.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b111b4",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8f2ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# desiredinstallationend\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-4mesi': 'three_to_four_months', \n",
    "    '5+mesi': 'more_than_5_months',\n",
    "    '1-2mesi': 'one_to_two_months',\n",
    "    'Non lo so': 'dont_know',\n",
    "    'short': np.nan,\n",
    "    # Already mapped values (keep as-is)\n",
    "    'dont_know': 'dont_know',\n",
    "    'three_to_four_months': 'three_to_four_months',\n",
    "    'one_to_two_months': 'one_to_two_months', \n",
    "    'more_than_5_months': 'more_than_5_months',\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1889c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationtime\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-6 mesi': np.nan,\n",
    "    '<3 mesi': np.nan,\n",
    "    '>6 mesi': np.nan,\n",
    "    # Already mapped English values\n",
    "    'less_than_three_months': np.nan,\n",
    "    'more_than_six_months': np.nan,\n",
    "    # Other values that appear in your data\n",
    "    'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "    'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "    'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "    'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09d77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the old columns directly\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "\n",
    "# Drop the temporary columns\n",
    "final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b24a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan before encoding\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c19a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with ordinal encoding for time-based features\n",
    "\n",
    "# Ordinal encoding for desiredinstallationend (time-based order)\n",
    "time_order = [ 'one_to_two_months', 'three_to_four_months', 'more_than_5_months', 'dont_know', 'missing']\n",
    "final_df['desiredinstallationend_encoded'] = final_df['desiredinstallationend'].map({val: i for i, val in enumerate(time_order)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78461e67",
   "metadata": {},
   "source": [
    "##### Analysis to see how to handle marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2f7fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19245 entries, 0 to 19244\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19245 non-null  int64  \n",
      " 1   gross_FU                        19245 non-null  int64  \n",
      " 2   gross_SC                        19245 non-null  int64  \n",
      " 3   net_FU                          19245 non-null  float64\n",
      " 4   net_SC                          19245 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19245 non-null  float64\n",
      " 6   zipregion                       18637 non-null  object \n",
      " 7   evaluationtime                  19245 non-null  object \n",
      " 8   desiredinstallationend          19245 non-null  object \n",
      " 9   electricitybill                 12527 non-null  object \n",
      " 10  heatingbill                     4742 non-null   object \n",
      " 11  mktgparamscore                  18319 non-null  object \n",
      " 12  netcontractsigned               19245 non-null  float64\n",
      " 13  selfipa_done                    19245 non-null  int64  \n",
      " 14  zipregion_missing               19245 non-null  int64  \n",
      " 15  evaluationtime_missing          19245 non-null  int64  \n",
      " 16  desiredinstallationend_missing  19245 non-null  int64  \n",
      " 17  electricitybill_missing         19245 non-null  int64  \n",
      " 18  heatingbill_missing             19245 non-null  int64  \n",
      " 19  mktgparamscore_missing          19245 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  19245 non-null  int64  \n",
      "dtypes: float64(4), int64(11), object(6)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7d73e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rates by category:\n",
      "\n",
      "\n",
      "ZIPREGION:\n",
      "                       total_samples  conversions  conversion_rate\n",
      "zipregion                                                         \n",
      "Friuli-Venezia Giulia           1068         64.0         0.059925\n",
      "Piemonte                        1717         58.0         0.033780\n",
      "Valle D'Aosta                     72          2.0         0.027778\n",
      "Liguria                          469         13.0         0.027719\n",
      "Emilia-Romagna                  1669         44.0         0.026363\n",
      "Lombardia                       3083         81.0         0.026273\n",
      "Veneto                          1466         37.0         0.025239\n",
      "Toscana                         1293         29.0         0.022428\n",
      "Basilicata                       148          3.0         0.020270\n",
      "Marche                           392          7.0         0.017857\n",
      "Umbria                           280          5.0         0.017857\n",
      "Lazio                           1908         34.0         0.017820\n",
      "Trentino-Alto Adige              146          2.0         0.013699\n",
      "Sardegna                         571          6.0         0.010508\n",
      "Campania                         675          7.0         0.010370\n",
      "Molise                           109          1.0         0.009174\n",
      "Sicilia                         1137         10.0         0.008795\n",
      "Abruzzo                          379          3.0         0.007916\n",
      "Puglia                          1391          9.0         0.006470\n",
      "Calabria                         664          2.0         0.003012\n",
      "Overall variation: 0.0128\n",
      "\n",
      "MKTGPARAMSCORE:\n",
      "                total_samples  conversions  conversion_rate\n",
      "mktgparamscore                                             \n",
      "Referral                   49          8.0         0.163265\n",
      "form_classico              14          1.0         0.071429\n",
      "Organic                  1698         91.0         0.053592\n",
      "Affiliation                69          3.0         0.043478\n",
      "Other                     601         18.0         0.029950\n",
      "Google                   2248         61.0         0.027135\n",
      "Youtube                  2237         53.0         0.023692\n",
      "Mediago                   528         11.0         0.020833\n",
      "TikTok                   1421         27.0         0.019001\n",
      "Outbrain                 2987         47.0         0.015735\n",
      "Meta                     6351         97.0         0.015273\n",
      "Taboola                   114          0.0         0.000000\n",
      "d2d                         2          0.0         0.000000\n",
      "Overall variation: 0.0428\n"
     ]
    }
   ],
   "source": [
    "# For each categorical column, see conversion rates\n",
    "print(\"Conversion rates by category:\\n\")\n",
    "\n",
    "for col in ['zipregion', 'mktgparamscore']:  # replace with your actual column names\n",
    "    conversion_by_cat = final_df.groupby(col)['netcontractsigned'].agg(['count', 'sum', 'mean'])\n",
    "    conversion_by_cat.columns = ['total_samples', 'conversions', 'conversion_rate']\n",
    "    conversion_by_cat = conversion_by_cat.sort_values('conversion_rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(conversion_by_cat)\n",
    "    print(f\"Overall variation: {conversion_by_cat['conversion_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf1bfd",
   "metadata": {},
   "source": [
    "Marketing gave good variance, would like to maintain that information. Region is not that significant but still good. \n",
    "However given the * of unique values, in both columns I will opt for grouping instead of each value having its column. \n",
    "\n",
    "Instead of 30+ categorical features, you get ~6-8, keeping the predictive power but losing the noise.\n",
    "\n",
    "Downside; this grouping should occasionally double checked to see if it still makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163be87",
   "metadata": {},
   "source": [
    "##### one hot encoding for marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bcb8c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance groups\n",
    "marketing_groups = {\n",
    "    'High': ['Referral', 'form_classico', 'Organic', 'Affiliation'],\n",
    "    'Medium': ['Other', 'Google', 'Youtube', 'Mediago', 'TikTok'], \n",
    "    'Low': ['Outbrain', 'Meta', 'Taboola', 'd2d']\n",
    "}\n",
    "\n",
    "# Create mapping function\n",
    "def group_marketing(value):\n",
    "    for group, channels in marketing_groups.items():\n",
    "        if value in channels:\n",
    "            return group\n",
    "    return 'Low'  # fallback\n",
    "\n",
    "# Apply grouping\n",
    "final_df['mktg_grouped'] = final_df['mktgparamscore'].apply(group_marketing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9995467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group regions by performance\n",
    "def group_regions(region):\n",
    "    if region == 'Friuli-Venezia Giulia':\n",
    "        return 'High_Performer'\n",
    "    elif region in ['Piemonte', 'Lombardia', 'Emilia-Romagna', 'Veneto']:\n",
    "        return 'Large_Good'\n",
    "    elif region in ['Liguria', 'Toscana', 'Valle D\\'Aosta']:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "final_df['region_grouped'] = final_df['zipregion'].apply(group_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0c5957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the grouped categories\n",
    "final_df = pd.get_dummies(final_df, columns=['mktg_grouped', 'region_grouped'], prefix=['mktg', 'region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "be7d7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19245 entries, 0 to 19244\n",
      "Data columns (total 28 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       19245 non-null  int64  \n",
      " 1   gross_FU                        19245 non-null  int64  \n",
      " 2   gross_SC                        19245 non-null  int64  \n",
      " 3   net_FU                          19245 non-null  float64\n",
      " 4   net_SC                          19245 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   19245 non-null  float64\n",
      " 6   zipregion                       18637 non-null  object \n",
      " 7   evaluationtime                  19245 non-null  object \n",
      " 8   desiredinstallationend          19245 non-null  object \n",
      " 9   electricitybill                 12527 non-null  object \n",
      " 10  heatingbill                     4742 non-null   object \n",
      " 11  mktgparamscore                  18319 non-null  object \n",
      " 12  netcontractsigned               19245 non-null  float64\n",
      " 13  selfipa_done                    19245 non-null  int64  \n",
      " 14  zipregion_missing               19245 non-null  int64  \n",
      " 15  evaluationtime_missing          19245 non-null  int64  \n",
      " 16  desiredinstallationend_missing  19245 non-null  int64  \n",
      " 17  electricitybill_missing         19245 non-null  int64  \n",
      " 18  heatingbill_missing             19245 non-null  int64  \n",
      " 19  mktgparamscore_missing          19245 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  19245 non-null  int64  \n",
      " 21  mktg_High                       19245 non-null  bool   \n",
      " 22  mktg_Low                        19245 non-null  bool   \n",
      " 23  mktg_Medium                     19245 non-null  bool   \n",
      " 24  region_High_Performer           19245 non-null  bool   \n",
      " 25  region_Large_Good               19245 non-null  bool   \n",
      " 26  region_Medium                   19245 non-null  bool   \n",
      " 27  region_Other                    19245 non-null  bool   \n",
      "dtypes: bool(7), float64(4), int64(11), object(6)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18644c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values: False\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns and create final feature matrix\n",
    "columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'mktgparamscore']\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "print(f\"\\nNo missing values: {final_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215f64f",
   "metadata": {},
   "source": [
    "#### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "16e8505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n",
      "Dataset ready: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_34972\\1109642794.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_34972\\1109642794.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n"
     ]
    }
   ],
   "source": [
    "# Fix electricitybill missing values\n",
    "final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "\n",
    "# Fix desiredinstallationend_encoded - NaN means 'Unknown' which should be 0 --> check if needed\n",
    "#X_final['desiredinstallationend_encoded'] = X_final['desiredinstallationend_encoded'].fillna(0)\n",
    "\n",
    "# Verify all missing values are gone\n",
    "print(f\"Remaining missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Dataset ready: {final_df.isnull().sum().sum() == 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bc1b",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5df80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling when needed\n",
    "continuous_cols= [\n",
    "    'time_first_sc_to_first_net_fu',\n",
    "    'electricitybill', \n",
    "    'heatingbill'\n",
    "]\n",
    "\n",
    "# Scale only the continuous features\n",
    "scaler = StandardScaler()\n",
    "final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c19f5",
   "metadata": {},
   "source": [
    "Adding behavioral data from Booking Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a7cb496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(r\"processed_data/df_model.csv\")\n",
    "behaviour.columns = behaviour.columns.str.lower()\n",
    "merged_df = final_df.merge(behaviour, \n",
    "                          left_on='requestid', \n",
    "                          right_on='id', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5e5749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop requestid\n",
    "#merged_df = merged_df.drop(columns=['requestid', 'id', 'converted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da2ee9",
   "metadata": {},
   "source": [
    "#### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3a919aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column pairs: []\n",
      "Original shape: (19245, 47)\n",
      "After removing duplicates: (19245, 47)\n"
     ]
    }
   ],
   "source": [
    "# Check your actual dataset for duplicates\n",
    "duplicate_cols = []\n",
    "cols = merged_df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if merged_df.equals(merged_df[cols[j]]):\n",
    "            duplicate_cols.append((cols[i], cols[j]))\n",
    "\n",
    "print(\"Duplicate column pairs:\", duplicate_cols)\n",
    "print(f\"Original shape: {merged_df.shape}\")\n",
    "\n",
    "# Remove duplicates if any found\n",
    "X_final_clean = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "print(f\"After removing duplicates: {X_final_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "86b2c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19245 entries, 0 to 19244\n",
      "Data columns (total 47 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   requestid                         19245 non-null  int64  \n",
      " 1   gross_FU                          19245 non-null  int64  \n",
      " 2   gross_SC                          19245 non-null  int64  \n",
      " 3   net_FU                            19245 non-null  float64\n",
      " 4   net_SC                            19245 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu     19245 non-null  float64\n",
      " 6   electricitybill                   19245 non-null  float64\n",
      " 7   heatingbill                       19245 non-null  float64\n",
      " 8   netcontractsigned                 19245 non-null  float64\n",
      " 9   selfipa_done                      19245 non-null  int64  \n",
      " 10  zipregion_missing                 19245 non-null  int64  \n",
      " 11  evaluationtime_missing            19245 non-null  int64  \n",
      " 12  desiredinstallationend_missing    19245 non-null  int64  \n",
      " 13  electricitybill_missing           19245 non-null  int64  \n",
      " 14  heatingbill_missing               19245 non-null  int64  \n",
      " 15  mktgparamscore_missing            19245 non-null  int64  \n",
      " 16  desiredinstallationend_encoded    19245 non-null  int64  \n",
      " 17  mktg_High                         19245 non-null  bool   \n",
      " 18  mktg_Low                          19245 non-null  bool   \n",
      " 19  mktg_Medium                       19245 non-null  bool   \n",
      " 20  region_High_Performer             19245 non-null  bool   \n",
      " 21  region_Large_Good                 19245 non-null  bool   \n",
      " 22  region_Medium                     19245 non-null  bool   \n",
      " 23  region_Other                      19245 non-null  bool   \n",
      " 24  id                                18140 non-null  float64\n",
      " 25  converted                         18140 non-null  float64\n",
      " 26  total_bc_attempts                 18140 non-null  float64\n",
      " 27  total_bc_outcomes                 18140 non-null  float64\n",
      " 28  lead_to_first_bc_days             18140 non-null  float64\n",
      " 29  bc_duration_days                  18140 non-null  float64\n",
      " 30  lead_to_sc1_days                  18140 non-null  float64\n",
      " 31  sc1_schedule_to_appointment_days  18140 non-null  float64\n",
      " 32  bc_frequency                      18140 non-null  float64\n",
      " 33  positive_outcomes_count           18140 non-null  float64\n",
      " 34  negative_outcomes_count           18140 non-null  float64\n",
      " 35  noshow_outcomes_count             18140 non-null  float64\n",
      " 36  positive_outcome_ratio            18140 non-null  float64\n",
      " 37  negative_outcome_ratio            18140 non-null  float64\n",
      " 38  noshow_outcome_ratio              18140 non-null  float64\n",
      " 39  reachability_score                18140 non-null  float64\n",
      " 40  outcome_trend                     18140 non-null  float64\n",
      " 41  persistence_after_negative        18140 non-null  float64\n",
      " 42  showed_up_sc1                     18140 non-null  float64\n",
      " 43  engagement_score                  18140 non-null  float64\n",
      " 44  efficiency_score                  18140 non-null  float64\n",
      " 45  last_bc_outcome_encoded           18140 non-null  float64\n",
      " 46  first_bc_outcome_encoded          18140 non-null  float64\n",
      "dtypes: bool(7), float64(29), int64(11)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6ad7dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing call data: 1105\n",
      "\n",
      "Request IDs with missing call data:\n",
      "[2039, 3279, 3583, 3729, 3852, 4359, 4689, 5180, 5216, 5837, 6442, 6521, 6749, 6882, 7183, 7653, 7899, 8057, 9106, 9971, 10157, 10622, 10630, 11034, 11446, 12271, 12548, 12741, 13171, 13514, 13648, 13973, 14578, 14807, 14966, 15106, 15626, 15874, 16288, 16594, 16677, 17124, 17870, 17960, 18429, 18517, 18874, 19527, 19753, 20114, 20193, 20680, 20810, 21062, 21188, 21236, 21468, 21708, 22080, 22690, 22879, 23699, 24093, 24359, 25652, 26202, 26230, 27380, 27584, 27621, 27693, 27713, 27787, 27808, 27830, 28210, 30068, 30881, 30941, 31609, 31791, 31932, 32097, 32714, 32990, 33039, 33130, 33550, 33583, 33793, 34334, 34500, 35640, 35995, 37294, 37319, 38479, 39326, 39500, 40136, 40444, 40758, 40958, 41170, 41292, 41364, 41669, 41729, 42087, 42235, 42346, 42708, 42844, 42989, 43202, 43621, 44179, 44795, 45017, 45483, 45612, 46194, 47009, 47240, 47945, 48369, 48407, 48463, 48607, 49417, 49792, 49837, 49844, 50451, 50554, 51125, 51208, 51540, 51586, 51617, 51765, 52123, 52513, 52899, 53426, 53880, 53897, 54245, 55296, 55493, 55605, 55680, 55701, 55848, 55883, 56364, 56513, 56755, 56909, 57466, 57689, 57751, 57906, 59128, 59531, 59708, 59817, 60610, 60922, 60944, 61103, 61179, 61383, 61684, 61967, 62074, 62158, 62333, 62489, 62949, 63221, 63586, 63674, 63892, 64695, 65167, 65312, 65357, 65552, 65641, 65757, 65926, 65999, 66063, 66188, 66553, 66692, 67651, 67853, 68086, 68094, 68235, 68392, 68897, 69192, 69329, 69368, 69642, 69947, 70047, 70262, 70343, 70479, 70611, 71339, 71366, 71392, 71418, 71925, 72352, 72406, 72910, 72959, 73016, 73044, 73065, 73081, 73192, 73196, 73566, 73680, 73954, 74237, 74249, 74736, 75635, 75751, 76216, 76239, 76364, 76511, 76815, 76843, 76852, 77081, 77216, 77461, 77527, 77784, 77970, 78360, 78372, 78573, 78648, 78723, 79311, 79615, 79652, 79734, 79952, 80006, 80104, 80134, 80145, 80173, 80737, 80741, 80799, 80930, 81750, 81830, 82094, 82290, 82486, 82853, 83013, 83071, 83132, 83405, 83629, 83856, 83971, 84082, 84259, 84307, 84603, 84824, 85328, 85772, 85780, 85976, 86008, 86018, 86060, 86069, 86365, 86424, 86459, 86617, 86710, 86822, 86837, 86872, 86984, 87228, 87362, 87411, 87467, 87669, 87900, 87911, 87984, 87998, 88273, 88281, 88662, 88783, 88876, 88960, 89186, 89325, 89556, 89591, 90045, 90053, 90354, 90819, 91344, 91345, 91397, 91752, 92080, 92083, 92155, 92190, 92553, 92659, 92703, 92709, 92827, 93118, 93232, 93470, 93500, 93631, 93637, 93647, 93791, 93951, 94066, 94119, 94186, 94487, 94504, 94617, 94654, 94951, 94972, 95077, 95119, 95169, 95391, 95672, 95778, 95789, 96583, 97023, 97106, 97186, 97313, 97345, 97378, 97424, 97584, 97637, 97652, 97860, 97882, 97894, 98047, 98096, 98285, 98400, 98665, 98901, 99030, 99192, 99268, 99284, 99319, 99611, 99759, 99822, 99982, 100233, 100265, 100334, 100447, 100494, 100567, 101045, 101085, 101110, 101111, 101146, 101313, 101351, 101467, 101536, 101537, 101554, 101555, 101571, 101660, 101726, 101727, 101736, 101737, 101824, 101841, 101842, 101941, 101978, 101979, 102021, 102022, 102085, 102156, 102157, 102310, 102311, 102374, 102381, 102403, 102422, 102434, 102460, 102533, 102539, 102540, 102644, 102675, 102768, 102788, 102805, 102806, 102832, 102888, 102889, 102901, 102902, 102961, 102974, 102976, 102977, 103011, 103012, 103016, 103017, 103025, 103041, 103073, 103074, 103108, 103164, 103213, 103214, 103256, 103278, 103288, 103347, 103374, 103403, 110069, 110071, 110074, 110075, 110076, 110077, 110078, 110080, 110081, 110082, 110084, 110085, 110087, 110088, 110090, 110091, 110092, 110093, 110094, 110097, 110098, 110099, 110100, 110102, 110105, 110106, 110108, 110109, 110110, 110111, 110112, 110113, 110114, 110116, 110117, 110118, 110119, 110120, 110122, 110123, 110124, 110125, 110126, 110127, 110128, 110129, 110130, 110132, 110133, 110134, 110135, 110136, 110137, 110138, 110139, 110141, 110142, 110143, 110144, 110145, 110146, 110147, 110148, 110149, 110150, 110151, 110152, 110153, 110154, 110155, 110156, 110157, 110158, 110159, 110160, 110161, 110162, 110163, 110166, 110167, 110168, 110169, 110171, 110172, 110174, 110175, 110176, 110177, 110178, 110179, 110180, 110181, 110182, 110183, 110184, 110185, 110186, 110188, 110190, 110195, 110197, 110198, 110199, 110200, 110202, 110204, 110205, 110206, 110207, 110208, 110209, 110210, 110211, 110212, 110214, 110215, 110216, 110217, 110218, 110219, 110220, 110222, 110224, 110226, 110227, 110228, 110229, 110230, 110231, 110232, 110233, 110234, 110235, 110236, 110237, 110238, 110239, 110240, 110241, 110242, 110243, 110244, 110245, 110246, 110247, 110248, 110249, 110250, 110251, 110252, 110253, 110254, 110256, 110257, 110258, 110259, 110260, 110261, 110262, 110263, 110264, 110265, 110267, 110268, 110269, 110270, 110271, 110272, 110273, 110274, 110275, 110276, 110277, 110278, 110279, 110280, 110281, 110282, 110283, 110284, 110285, 110286, 110287, 110288, 110289, 110290, 110291, 110292, 110293, 110294, 110295, 110296, 110297, 110298, 110299, 110300, 110301, 110302, 110303, 110304, 110305, 110306, 110307, 110308, 110309, 110310, 110311, 110312, 110313, 110315, 110316, 110317, 110318, 110319, 110320, 110321, 110322, 110324, 110325, 110326, 110328, 110329, 110330, 110331, 110332, 110333, 110335, 110336, 110337, 110338, 110340, 110341, 110342, 110343, 110344, 110345, 110346, 110347, 110348, 110349, 110350, 110351, 110352, 110353, 110354, 110355, 110356, 110357, 110358, 110359, 110361, 110363, 110365, 110366, 110367, 110368, 110369, 110370, 110371, 110372, 110373, 110374, 110375, 110376, 110378, 110379, 110380, 110381, 110382, 110383, 110384, 110385, 110386, 110387, 110388, 110389, 110390, 110391, 110395, 110396, 110397, 110398, 110399, 110400, 110401, 110402, 110403, 110404, 110405, 110406, 110407, 110408, 110409, 110410, 110411, 110412, 110413, 110414, 110415, 110417, 110418, 110419, 110420, 110421, 110422, 110423, 110424, 110425, 110426, 110427, 110428, 110429, 110430, 110431, 110432, 110433, 110434, 110435, 110436, 110437, 110440, 110441, 110442, 110446, 110447, 110449, 110450, 110451, 110452, 110456, 110457, 110458, 110461, 110462, 110467, 110469, 110470, 110471, 110472, 110475, 110476, 110478, 110482, 110483, 110484, 110485, 110486, 110487, 110488, 110489, 110490, 110491, 110492, 110493, 110495, 110496, 110497, 110498, 110499, 110500, 110501, 110502, 110503, 110505, 110506, 110508, 110509, 110510, 110511, 110514, 110515, 110516, 110517, 110518, 110521, 110524, 110525, 110526, 110527, 110528, 110529, 110530, 110532, 110533, 110534, 110535, 110536, 110537, 110540, 110541, 110542, 110543, 110544, 110545, 110546, 110547, 110548, 110551, 110552, 110553, 110554, 110556, 110557, 110558, 110560, 110561, 110562, 110563, 110564, 110565, 110566, 110567, 110568, 110569, 110570, 110571, 110572, 110574, 110575, 110578, 110579, 110580, 110582, 110583, 110584, 110585, 110586, 110587, 110588, 110589, 110590, 110592, 110593, 110594, 110595, 110596, 110597, 110598, 110599, 110600, 110601, 110602, 110603, 110605, 110606, 110607, 110609, 110610, 110611, 110612, 110613, 110614, 110615, 110616, 110617, 110618, 110619, 110620, 110621, 110622, 110623, 110624, 110625, 110628, 110629, 110630, 110631, 110634, 110635, 110636, 110637, 110638, 110639, 110640, 110641, 110642, 110643, 110644, 110646, 110647, 110648, 110649, 110650, 110652, 110653, 110654, 110655, 110658, 110659, 110660, 110662, 110663, 110664, 110665, 110667, 110668, 110669, 110670, 110671, 110673, 110674, 110675, 110676, 110677, 110678, 110679, 110680, 110681, 110682, 110685, 110687, 110689, 110690, 110693, 110694, 110699, 110700, 110701, 110702, 110703, 110704, 110705, 110706, 110707, 110708, 110710, 110711, 110712, 110713, 110714, 110715, 110716, 110717, 110720, 110721, 110722, 110723, 110724, 110725, 110726, 110727, 110728, 110729, 110730, 110731, 110733, 110734, 110735, 110737, 110739, 110743, 110745, 110746, 110749, 110750, 110751, 110752, 110753, 110754, 110755, 110756, 110757, 110758, 110759, 110760, 110761, 110762, 110763, 110764, 110765, 110766, 110767, 110768, 110770, 110773, 110774, 110776, 110777, 110779, 110780, 110781, 110782, 110783, 110784, 110785, 110786, 110787, 110788, 110792, 110793, 110797, 110798, 110799, 110800, 110801, 110802, 110803, 110804, 110805, 110806, 110807, 110808, 110810, 110812, 110813, 110814, 110815, 110816, 110817, 110819, 110820, 110822, 110823, 110825, 110826, 110827, 110828, 110829, 110830, 110831, 110832, 110833, 110834, 110835, 110836, 110837, 110838, 110839, 110840, 110844, 110845, 125957]\n"
     ]
    }
   ],
   "source": [
    "#investigating missing data request ids\n",
    "# Find rows with missing call data\n",
    "missing_call_data = merged_df[merged_df['total_bc_attempts'].isnull()]\n",
    "\n",
    "print(f\"Number of rows with missing call data: {len(missing_call_data)}\")\n",
    "\n",
    "# Show the request IDs (assuming you have a request ID column)\n",
    "if 'requestid' in merged_df.columns:\n",
    "    print(\"\\nRequest IDs with missing call data:\")\n",
    "    print(missing_call_data['requestid'].tolist())\n",
    "else:\n",
    "    print(\"\\nColumns available to identify these rows:\")\n",
    "    print(merged_df.columns.tolist())\n",
    "    \n",
    "    # Show index positions as fallback\n",
    "    print(f\"\\nIndex positions of missing rows: {missing_call_data.index.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64a85535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversions in missing rows: 10.0\n",
      "Conversion rate in missing rows: 0.0090\n",
      "Conversion rate in complete rows: 0.0235\n"
     ]
    }
   ],
   "source": [
    "# Check conversions in missing call data rows\n",
    "missing_mask = merged_df['total_bc_attempts'].isnull()\n",
    "print(f\"Conversions in missing rows: {merged_df[missing_mask]['netcontractsigned'].sum()}\")\n",
    "print(f\"Conversion rate in missing rows: {merged_df[missing_mask]['netcontractsigned'].mean():.4f}\")\n",
    "print(f\"Conversion rate in complete rows: {merged_df[~missing_mask]['netcontractsigned'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these requests are very old and dont have propre registstration, some have some registration but essentially there are only 10 contract signed insode t1100, so essentially dropping as a first step of undersampling,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5c84629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (18140, 47)\n",
      "New conversion rate: 0.0235\n",
      "Total conversions kept: 426.0\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.dropna().copy()\n",
    "print(f\"New dataset shape: {merged_df.shape}\")\n",
    "print(f\"New conversion rate: {merged_df['netcontractsigned'].mean():.4f}\")\n",
    "print(f\"Total conversions kept: {merged_df['netcontractsigned'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4526a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create subfolder if it doesn't exist\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# Save your merged dataframe\n",
    "#merged_df.to_csv('processed_data/merged_df.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
