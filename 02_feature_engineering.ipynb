{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0fa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287d505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests\n",
    "file_path = r\"C:\\Users\\ElifYilmaz\\Downloads\\project 2.0.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# qualitative \n",
    "\n",
    "file2 = r\"C:\\Users\\ElifYilmaz\\Downloads\\info client.csv\"\n",
    "\n",
    "df2 = pd.read_csv(file2)\n",
    "df2.columns = df2.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353766",
   "metadata": {},
   "source": [
    "#### Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1690b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing appointment types\n",
    "\n",
    "type_mapping = {\n",
    "    'FU': 'FU',\n",
    "    'FUVD': 'FU', \n",
    "    'REMINDER': 'FU',\n",
    "    'VDFIELD': 'SC',\n",
    "    'VD': 'SC'\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "df['grouped_type'] = df['type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad51c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition for net events (not cancelled and past)\n",
    "df['is_net_event'] = df['deletedat'].isna() & df['is_passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bbb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross counts (all events)\n",
    "gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "# Net counts (only events that actually happened)\n",
    "net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "net_counts.columns = [f'net_{col}' for col in net_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gross and net counts\n",
    "counts_df = gross_counts.join(net_counts, how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8011647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn self ipa event to bbinary \n",
    "df2['selfipa_done'] = df2['selfipaimportedat'].notnull().astype(int)\n",
    "df2.drop('selfipaimportedat', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert createdat to datetime\n",
    "df['createdat'] = pd.to_datetime(df['createdat'])\n",
    "\n",
    "# Step 2: Get first gross SC and first net FU timestamps\n",
    "first_gross_sc = df[df['grouped_type'] == 'SC'].groupby('requestid')['createdat'].min()\n",
    "first_net_fu = df[(df['grouped_type'] == 'FU') & df['is_net_event']].groupby('requestid')['createdat'].min()\n",
    "\n",
    "# Step 3: Calculate time difference in hours\n",
    "counts_df['time_first_sc_to_first_net_fu'] = (first_net_fu - first_gross_sc).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88cecdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gross_SC\n",
       "1     15250\n",
       "2      3013\n",
       "3       700\n",
       "4       198\n",
       "5        48\n",
       "6        12\n",
       "7         6\n",
       "10        2\n",
       "23        1\n",
       "8         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop older requests that havent had a sc since march\n",
    "counts_df = counts_df[counts_df['gross_SC'] != 0]\n",
    "counts_df['gross_SC'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d420799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (19231, 5)\n",
      "No missing values: True\n"
     ]
    }
   ],
   "source": [
    "# handle missing value for customers who never made to fu\n",
    "counts_df['time_first_sc_to_first_net_fu'] = counts_df['time_first_sc_to_first_net_fu'].fillna(-1)\n",
    "\n",
    "# Check final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {counts_df.shape}\")\n",
    "print(f\"No missing values: {counts_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negative time differences (not NaN fills): 22\n",
      "Positive time differences: 7351\n",
      "NaN fills (-1): 11858\n",
      "\n",
      "Real negative values range: -4146.53 to -22.24 hours\n"
     ]
    }
   ],
   "source": [
    "# problematic entries with negative time difference: \n",
    "# Separate real negative times from NaN fill values\n",
    "real_negative = counts_df[\n",
    "    (counts_df['time_first_sc_to_first_net_fu'] < 0) & \n",
    "    (counts_df['time_first_sc_to_first_net_fu'] != -1)\n",
    "]\n",
    "\n",
    "actual_valid_times = counts_df[counts_df['time_first_sc_to_first_net_fu'] >= 0]\n",
    "\n",
    "print(f\"Actual negative time differences (not NaN fills): {len(real_negative)}\")\n",
    "print(f\"Positive time differences: {len(actual_valid_times)}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "if len(real_negative) > 0:\n",
    "    print(f\"\\nReal negative values range: {real_negative['time_first_sc_to_first_net_fu'].min():.2f} to {real_negative['time_first_sc_to_first_net_fu'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle negative time difference\n",
    "# Remove outliers entirely (set to NaN, then fill with -1)\n",
    "counts_df.loc[counts_df['time_first_sc_to_first_net_fu'] < -1, 'time_first_sc_to_first_net_fu'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec7cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing outliers:\n",
      "Negative values (excluding -1 fills): 0\n",
      "Valid positive time differences: 7351\n",
      "NaN fills (-1): 11880\n",
      "\n",
      "Final dataset shape: (19231, 5)\n",
      "Ready to join with target variable!\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Negative values (excluding -1 fills): {((counts_df['time_first_sc_to_first_net_fu'] < 0) & (counts_df['time_first_sc_to_first_net_fu'] != -1)).sum()}\")\n",
    "print(f\"Valid positive time differences: {(counts_df['time_first_sc_to_first_net_fu'] > 0).sum()}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {counts_df.shape}\")\n",
    "print(\"Ready to join with target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98124e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2039</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3279</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.043056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3729</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.835556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3852</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.193889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requestid  gross_FU  gross_SC  net_FU  net_SC  \\\n",
       "0       2039         0         2     0.0     0.0   \n",
       "1       3279         0         1     0.0     1.0   \n",
       "2       3583         1         1     1.0     1.0   \n",
       "3       3729         8         1     8.0     1.0   \n",
       "4       3852         2         1     2.0     1.0   \n",
       "\n",
       "   time_first_sc_to_first_net_fu  \n",
       "0                      -1.000000  \n",
       "1                      -1.000000  \n",
       "2                     120.043056  \n",
       "3                      73.835556  \n",
       "4                      42.193889  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point requestid is the index. make it explicit\n",
    "\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02d0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (19245, 14)\n",
      "Target variable distribution:\n",
      "netcontractsigned\n",
      "0.0    18809\n",
      "1.0      436\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['requestid', 'gross_FU', 'gross_SC', 'net_FU', 'net_SC', 'time_first_sc_to_first_net_fu', 'zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore', 'netcontractsigned', 'selfipa_done']\n"
     ]
    }
   ],
   "source": [
    "# Left join to add target variable\n",
    "#final_df1 = counts_df.merge(contract, on='requestid', how='left')\n",
    "# unnecessairy join because df2 has netcontractsigned info \n",
    "\n",
    "final_df = counts_df.merge(df2, on='requestid', how='left')\n",
    "\n",
    "\n",
    "# Fill non-matching requestids with 0 for the target\n",
    "final_df['netcontractsigned'] = final_df['netcontractsigned'].fillna(0)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(final_df['netcontractsigned'].value_counts())\n",
    "print(f\"\\nColumns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38cbb0",
   "metadata": {},
   "source": [
    "#### Encoding and Null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "452d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of bills have -1, 1 and 0\n",
    "\n",
    "# set wrong bill values to nan\n",
    "# Replace specific values with NaN\n",
    "final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea01f",
   "metadata": {},
   "source": [
    "Optional for bill handling \n",
    "def clean_billing_column(series, column_name):\n",
    "    \"\"\"Clean billing column: set negative values and extreme outliers to replacement value\"\"\"\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    series_clean = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Define reasonable bounds (adjust these based on your domain knowledge)\n",
    "    min_reasonable = 0  # Bills shouldn't be negative\n",
    "    max_reasonable = 2000  # Adjust based on your market (e.g., â‚¬1000/month seems high)\n",
    "    \n",
    "    # Count issues for reporting\n",
    "    negative_count = (series_clean < min_reasonable).sum()\n",
    "    outlier_count = (series_clean > max_reasonable).sum()\n",
    "    \n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  - Negative values: {negative_count}\")\n",
    "    print(f\"  - Values > {max_reasonable}: {outlier_count}\")\n",
    "    print(f\"  - Original NaN: {series.isna().sum()}\")\n",
    "    \n",
    "    # Replace problematic values\n",
    "    series_clean.loc[series_clean < min_reasonable] = np.nan  # or -1\n",
    "    series_clean.loc[series_clean > max_reasonable] = np.nan  # or -1\n",
    "    \n",
    "    return series_clean\n",
    "\n",
    "# Apply cleaning\n",
    "final_df['heatingbill'] = clean_billing_column(final_df['heatingbill'], 'heatingbill')\n",
    "final_df['electricitybill'] = clean_billing_column(final_df['electricitybill'], 'electricitybill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f97be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicator correlations with target:\n",
      "electricitybill_missing          -0.047029\n",
      "desiredinstallationend_missing   -0.010914\n",
      "evaluationtime_missing           -0.010865\n",
      "mktgparamscore_missing           -0.003229\n",
      "zipregion_missing                 0.010433\n",
      "heatingbill_missing               0.018988\n",
      "Name: netcontractsigned, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# before encoding, mark the missingness inside the row;\n",
    "missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore']\n",
    "\n",
    "for col in missing_cols:\n",
    "    final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "\n",
    "# Check the predictive power of missing indicators\n",
    "print(\"Missing indicator correlations with target:\")\n",
    "missing_indicators = [f'{col}_missing' for col in missing_cols]\n",
    "missing_corrs = final_df[missing_indicators + ['netcontractsigned']].corr()['netcontractsigned'].drop('netcontractsigned')\n",
    "print(missing_corrs.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f2ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# desiredinstallationend\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-4mesi': 'three_to_four_months', \n",
    "    '5+mesi': 'more_than_5_months',\n",
    "    '1-2mesi': 'one_to_two_months',\n",
    "    'Non lo so': 'dont_know',\n",
    "    'short': np.nan,\n",
    "    # Already mapped values (keep as-is)\n",
    "    'dont_know': 'dont_know',\n",
    "    'three_to_four_months': 'three_to_four_months',\n",
    "    'one_to_two_months': 'one_to_two_months', \n",
    "    'more_than_5_months': 'more_than_5_months',\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1889c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationtime\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-6 mesi': np.nan,\n",
    "    '<3 mesi': np.nan,\n",
    "    '>6 mesi': np.nan,\n",
    "    # Already mapped English values\n",
    "    'less_than_three_months': np.nan,\n",
    "    'more_than_six_months': np.nan,\n",
    "    # Other values that appear in your data\n",
    "    'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "    'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "    'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "    'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09d77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the old columns directly\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "\n",
    "# Drop the temporary columns\n",
    "final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b24a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan before encoding\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with ordinal encoding for time-based features\n",
    "\n",
    "# Ordinal encoding for desiredinstallationend (time-based order)\n",
    "time_order = [ 'one_to_two_months', 'three_to_four_months', 'more_than_5_months', 'dont_know', 'missing']\n",
    "final_df['desiredinstallationend_encoded'] = final_df['desiredinstallationend'].map({val: i for i, val in enumerate(time_order)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9995467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after encoding: (19245, 55)\n",
      "New dummy columns created: 33\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding for marketing channels (mktgparamscore)\n",
    "marketing_dummies = pd.get_dummies(final_df['mktgparamscore'], prefix='mktg')\n",
    "final_df = pd.concat([final_df, marketing_dummies], axis=1)\n",
    "\n",
    "# One-hot encoding for regions (zipregion) \n",
    "region_dummies = pd.get_dummies(final_df['zipregion'], prefix='region')\n",
    "final_df = pd.concat([final_df, region_dummies], axis=1)\n",
    "\n",
    "# Label encoding for evaluationtime (mixed ordinal/categorical)\n",
    "le = LabelEncoder()\n",
    "final_df['evaluationtime_encoded'] = le.fit_transform(final_df['evaluationtime'])\n",
    "\n",
    "print(f\"Dataset shape after encoding: {final_df.shape}\")\n",
    "print(f\"New dummy columns created: {len(marketing_dummies.columns) + len(region_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18644c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values: False\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns and create final feature matrix\n",
    "columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'mktgparamscore', 'netcontractsigned']\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "print(f\"\\nNo missing values: {final_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215f64f",
   "metadata": {},
   "source": [
    "#### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16e8505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n",
      "Dataset ready: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_14140\\1109642794.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_14140\\1109642794.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n"
     ]
    }
   ],
   "source": [
    "# Fix electricitybill missing values\n",
    "final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "\n",
    "# Fix desiredinstallationend_encoded - NaN means 'Unknown' which should be 0 --> check if needed\n",
    "#X_final['desiredinstallationend_encoded'] = X_final['desiredinstallationend_encoded'].fillna(0)\n",
    "\n",
    "# Verify all missing values are gone\n",
    "print(f\"Remaining missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Dataset ready: {final_df.isnull().sum().sum() == 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bc1b",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5df80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling when needed\n",
    "continuous_cols= [\n",
    "    'time_first_sc_to_first_net_fu',\n",
    "    'electricitybill', \n",
    "    'heatingbill'\n",
    "]\n",
    "\n",
    "# Scale only the continuous features\n",
    "scaler = StandardScaler()\n",
    "final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c19f5",
   "metadata": {},
   "source": [
    "Adding behavioral data from Booking Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7cb496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(r\"df_model.csv\")\n",
    "behaviour.columns = behaviour.columns.str.lower()\n",
    "merged_df = final_df.merge(behaviour, \n",
    "                          left_on='requestid', \n",
    "                          right_on='id', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e5749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop requestid\n",
    "merged_df = merged_df.drop(columns=['requestid', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da2ee9",
   "metadata": {},
   "source": [
    "#### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a919aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column pairs: []\n",
      "Original shape: (19245, 73)\n",
      "After removing duplicates: (19245, 73)\n"
     ]
    }
   ],
   "source": [
    "# Check your actual dataset for duplicates\n",
    "duplicate_cols = []\n",
    "cols = merged_df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if merged_df.equals(merged_df[cols[j]]):\n",
    "            duplicate_cols.append((cols[i], cols[j]))\n",
    "\n",
    "print(\"Duplicate column pairs:\", duplicate_cols)\n",
    "print(f\"Original shape: {merged_df.shape}\")\n",
    "\n",
    "# Remove duplicates if any found\n",
    "X_final_clean = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "print(f\"After removing duplicates: {X_final_clean.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
