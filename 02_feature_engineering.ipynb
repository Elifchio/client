{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0fa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287d505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests\n",
    "file_path = r\"C:\\Users\\ElifYilmaz\\Downloads\\project 2.0.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# qualitative \n",
    "\n",
    "file2 = r\"C:\\Users\\ElifYilmaz\\Downloads\\info client.csv\"\n",
    "\n",
    "df2 = pd.read_csv(file2)\n",
    "df2.columns = df2.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1648b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requestids with duplicates: 35\n",
      "Total duplicate rows: 35\n",
      "\n",
      "Dropped 8 absolute duplicate rows\n",
      "Remaining duplicate requestids before netcontractsigned cleanup: 27\n",
      "Dropped 27 rows where netcontractsigned = 0 from duplicate requestids\n",
      "\n",
      "==================================================\n",
      "FINAL DUPLICATE CHECK\n",
      "==================================================\n",
      "Number of requestids with duplicates: 0\n",
      "Total duplicate rows: 0\n",
      "✅ No remaining duplicates!\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates in df2,they were found in notebook 01\n",
    "# Step 1: Count requestids that have duplicates\n",
    "duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df2['requestid'].duplicated().sum()}\")\n",
    "\n",
    "# Step 2: Drop absolute duplicates (excluding leadid column)\n",
    "comparison_cols = [col for col in df2.columns if col != 'leadid']\n",
    "df2_before = len(df2)\n",
    "df2 = df2.drop_duplicates(subset=comparison_cols, keep='first')\n",
    "df2_after = len(df2)\n",
    "\n",
    "print(f\"\\nDropped {df2_before - df2_after} absolute duplicate rows\")\n",
    "\n",
    "# Step 3: From the remaining duplicate requestids, drop rows where netcontractsigned = 0\n",
    "remaining_duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "\n",
    "if len(remaining_duplicate_requestids) > 0:\n",
    "    print(f\"Remaining duplicate requestids before netcontractsigned cleanup: {len(remaining_duplicate_requestids)}\")\n",
    "    \n",
    "    # Create condition: either NOT a duplicate requestid, OR netcontractsigned != 0\n",
    "    condition = (~df2['requestid'].isin(remaining_duplicate_requestids)) | (df2['netcontractsigned'] != 0)\n",
    "    \n",
    "    df2_before_net_drop = len(df2)\n",
    "    df2 = df2[condition]\n",
    "    df2_after_net_drop = len(df2)\n",
    "    \n",
    "    print(f\"Dropped {df2_before_net_drop - df2_after_net_drop} rows where netcontractsigned = 0 from duplicate requestids\")\n",
    "\n",
    "# Step 4: Final duplicate check\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL DUPLICATE CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(final_duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df2['requestid'].duplicated().sum()}\")\n",
    "\n",
    "if len(final_duplicate_requestids) > 0:\n",
    "    # Check what columns still differ\n",
    "    all_differing_columns = set()\n",
    "    for requestid in final_duplicate_requestids:\n",
    "        group = df2[df2['requestid'] == requestid]\n",
    "        for col in df2.columns:\n",
    "            if col != 'requestid' and group[col].nunique() > 1:\n",
    "                all_differing_columns.add(col)\n",
    "    \n",
    "    print(f\"Differing columns: {sorted(all_differing_columns)}\")\n",
    "else:\n",
    "    print(f\"✅ No remaining duplicates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353766",
   "metadata": {},
   "source": [
    "#### Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1690b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing appointment types\n",
    "\n",
    "type_mapping = {\n",
    "    'FU': 'FU',\n",
    "    'FUVD': 'FU', \n",
    "    'REMINDER': 'FU',\n",
    "    'VDFIELD': 'SC',\n",
    "    'VD': 'SC'\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "df['grouped_type'] = df['type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad51c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition for net events (not cancelled and past)\n",
    "df['is_net_event'] = df['deletedat'].isna() & df['is_passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bbb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross counts (all events)\n",
    "gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "# Net counts (only events that actually happened)\n",
    "net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "net_counts.columns = [f'net_{col}' for col in net_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gross and net counts\n",
    "counts_df = gross_counts.join(net_counts, how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b102cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8011647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn self ipa event to bbinary \n",
    "df2['selfipa_done'] = df2['selfipaimportedat'].notnull().astype(int)\n",
    "df2.drop('selfipaimportedat', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert createdat to datetime\n",
    "df['createdat'] = pd.to_datetime(df['createdat'])\n",
    "\n",
    "# Step 2: Get first gross SC and first net FU timestamps\n",
    "first_gross_sc = df[df['grouped_type'] == 'SC'].groupby('requestid')['createdat'].min()\n",
    "first_net_fu = df[(df['grouped_type'] == 'FU') & df['is_net_event']].groupby('requestid')['createdat'].min()\n",
    "\n",
    "# Step 3: Calculate time difference in hours\n",
    "counts_df['time_first_sc_to_first_net_fu'] = (first_net_fu - first_gross_sc).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88cecdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gross_SC\n",
       "1     11241\n",
       "2      2185\n",
       "3       532\n",
       "4       148\n",
       "5        34\n",
       "6         8\n",
       "7         5\n",
       "10        2\n",
       "23        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop older requests that havent had a sc since march\n",
    "counts_df = counts_df[counts_df['gross_SC'] != 0]\n",
    "counts_df['gross_SC'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d420799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (14156, 5)\n",
      "No missing values: True\n"
     ]
    }
   ],
   "source": [
    "# handle missing value for customers who never made to fu\n",
    "counts_df['time_first_sc_to_first_net_fu'] = counts_df['time_first_sc_to_first_net_fu'].fillna(-1)\n",
    "\n",
    "# Check final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {counts_df.shape}\")\n",
    "print(f\"No missing values: {counts_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negative time differences (not NaN fills): 7\n",
      "Positive time differences: 5964\n",
      "NaN fills (-1): 8185\n",
      "\n",
      "Real negative values range: -905.06 to -23.08 hours\n"
     ]
    }
   ],
   "source": [
    "# problematic entries with negative time difference: \n",
    "# Separate real negative times from NaN fill values\n",
    "real_negative = counts_df[\n",
    "    (counts_df['time_first_sc_to_first_net_fu'] < 0) & \n",
    "    (counts_df['time_first_sc_to_first_net_fu'] != -1)\n",
    "]\n",
    "\n",
    "actual_valid_times = counts_df[counts_df['time_first_sc_to_first_net_fu'] >= 0]\n",
    "\n",
    "print(f\"Actual negative time differences (not NaN fills): {len(real_negative)}\")\n",
    "print(f\"Positive time differences: {len(actual_valid_times)}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "if len(real_negative) > 0:\n",
    "    print(f\"\\nReal negative values range: {real_negative['time_first_sc_to_first_net_fu'].min():.2f} to {real_negative['time_first_sc_to_first_net_fu'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle negative time difference\n",
    "# Remove outliers entirely (set to NaN, then fill with -1)\n",
    "counts_df.loc[counts_df['time_first_sc_to_first_net_fu'] < -1, 'time_first_sc_to_first_net_fu'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec7cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing outliers:\n",
      "Negative values (excluding -1 fills): 0\n",
      "Valid positive time differences: 5964\n",
      "NaN fills (-1): 8192\n",
      "\n",
      "Final dataset shape: (14156, 5)\n",
      "Ready to join with target variable!\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Negative values (excluding -1 fills): {((counts_df['time_first_sc_to_first_net_fu'] < 0) & (counts_df['time_first_sc_to_first_net_fu'] != -1)).sum()}\")\n",
    "print(f\"Valid positive time differences: {(counts_df['time_first_sc_to_first_net_fu'] > 0).sum()}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {counts_df.shape}\")\n",
    "print(\"Ready to join with target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98124e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3279</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3729</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.835556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3852</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.193889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4689</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>837.203889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requestid  gross_FU  gross_SC  net_FU  net_SC  \\\n",
       "0       3279         0         1     0.0     1.0   \n",
       "1       3729         8         1     8.0     1.0   \n",
       "2       3852         2         1     2.0     1.0   \n",
       "3       4359         0         1     0.0     0.0   \n",
       "4       4689         4         1     1.0     1.0   \n",
       "\n",
       "   time_first_sc_to_first_net_fu  \n",
       "0                      -1.000000  \n",
       "1                      73.835556  \n",
       "2                      42.193889  \n",
       "3                      -1.000000  \n",
       "4                     837.203889  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point requestid is the index. make it explicit\n",
    "\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd583cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# requestid duplicates? --> no \n",
    "# Show which requestid values appear more than once\n",
    "duplicate_requestids = counts_df[counts_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba7e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# also df2 has no duplicates\n",
    "duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02d0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (14156, 14)\n",
      "Target variable distribution:\n",
      "netcontractsigned\n",
      "0.0    13812\n",
      "1.0      344\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['requestid', 'gross_FU', 'gross_SC', 'net_FU', 'net_SC', 'time_first_sc_to_first_net_fu', 'zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore', 'netcontractsigned', 'selfipa_done']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_df = counts_df.merge(df2, on='requestid', how='left')\n",
    "\n",
    "\n",
    "# Fill non-matching requestids with 0 for the target\n",
    "final_df['netcontractsigned'] = final_df['netcontractsigned'].fillna(0)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(final_df['netcontractsigned'].value_counts())\n",
    "print(f\"\\nColumns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cde1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38cbb0",
   "metadata": {},
   "source": [
    "#### Encoding and Null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "452d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of bills have -1, 1 and 0\n",
    "\n",
    "# set wrong bill values to nan\n",
    "# Replace specific values with NaN\n",
    "final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea01f",
   "metadata": {},
   "source": [
    "Optional for bill handling \n",
    "def clean_billing_column(series, column_name):\n",
    "    \"\"\"Clean billing column: set negative values and extreme outliers to replacement value\"\"\"\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    series_clean = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Define reasonable bounds (adjust these based on your domain knowledge)\n",
    "    min_reasonable = 0  # Bills shouldn't be negative\n",
    "    max_reasonable = 2000  # Adjust based on your market (e.g., €1000/month seems high)\n",
    "    \n",
    "    # Count issues for reporting\n",
    "    negative_count = (series_clean < min_reasonable).sum()\n",
    "    outlier_count = (series_clean > max_reasonable).sum()\n",
    "    \n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  - Negative values: {negative_count}\")\n",
    "    print(f\"  - Values > {max_reasonable}: {outlier_count}\")\n",
    "    print(f\"  - Original NaN: {series.isna().sum()}\")\n",
    "    \n",
    "    # Replace problematic values\n",
    "    series_clean.loc[series_clean < min_reasonable] = np.nan  # or -1\n",
    "    series_clean.loc[series_clean > max_reasonable] = np.nan  # or -1\n",
    "    \n",
    "    return series_clean\n",
    "\n",
    "# Apply cleaning\n",
    "final_df['heatingbill'] = clean_billing_column(final_df['heatingbill'], 'heatingbill')\n",
    "final_df['electricitybill'] = clean_billing_column(final_df['electricitybill'], 'electricitybill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f97be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicator correlations with target:\n",
      "electricitybill_missing          -0.048352\n",
      "evaluationtime_missing           -0.013618\n",
      "desiredinstallationend_missing   -0.013618\n",
      "mktgparamscore_missing           -0.000055\n",
      "zipregion_missing                 0.015379\n",
      "heatingbill_missing               0.027626\n",
      "Name: netcontractsigned, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# before encoding, mark the missingness inside the row;\n",
    "missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore']\n",
    "\n",
    "for col in missing_cols:\n",
    "    final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "\n",
    "# Check the predictive power of missing indicators\n",
    "print(\"Missing indicator correlations with target:\")\n",
    "missing_indicators = [f'{col}_missing' for col in missing_cols]\n",
    "missing_corrs = final_df[missing_indicators + ['netcontractsigned']].corr()['netcontractsigned'].drop('netcontractsigned')\n",
    "print(missing_corrs.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b111b4",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# desiredinstallationend\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-4mesi': 'three_to_four_months', \n",
    "    '5+mesi': 'more_than_5_months',\n",
    "    '1-2mesi': 'one_to_two_months',\n",
    "    'Non lo so': 'dont_know',\n",
    "    'short': np.nan,\n",
    "    # Already mapped values (keep as-is)\n",
    "    'dont_know': 'dont_know',\n",
    "    'three_to_four_months': 'three_to_four_months',\n",
    "    'one_to_two_months': 'one_to_two_months', \n",
    "    'more_than_5_months': 'more_than_5_months',\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1889c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationtime\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-6 mesi': np.nan,\n",
    "    '<3 mesi': np.nan,\n",
    "    '>6 mesi': np.nan,\n",
    "    # Already mapped English values\n",
    "    'less_than_three_months': np.nan,\n",
    "    'more_than_six_months': np.nan,\n",
    "    # Other values that appear in your data\n",
    "    'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "    'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "    'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "    'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09d77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the old columns directly\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "\n",
    "# Drop the temporary columns\n",
    "final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b24a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan before encoding\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c19a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with ordinal encoding for time-based features\n",
    "\n",
    "# Ordinal encoding for desiredinstallationend (time-based order)\n",
    "time_order = [ 'one_to_two_months', 'three_to_four_months', 'more_than_5_months', 'dont_know', 'missing']\n",
    "final_df['desiredinstallationend_encoded'] = final_df['desiredinstallationend'].map({val: i for i, val in enumerate(time_order)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78461e67",
   "metadata": {},
   "source": [
    "##### Analysis to see how to handle marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f7fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14156 entries, 0 to 14155\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       14156 non-null  int64  \n",
      " 1   gross_FU                        14156 non-null  int64  \n",
      " 2   gross_SC                        14156 non-null  int64  \n",
      " 3   net_FU                          14156 non-null  float64\n",
      " 4   net_SC                          14156 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   14156 non-null  float64\n",
      " 6   zipregion                       13700 non-null  object \n",
      " 7   evaluationtime                  14156 non-null  object \n",
      " 8   desiredinstallationend          14156 non-null  object \n",
      " 9   electricitybill                 9026 non-null   object \n",
      " 10  heatingbill                     3080 non-null   object \n",
      " 11  mktgparamscore                  13373 non-null  object \n",
      " 12  netcontractsigned               14156 non-null  float64\n",
      " 13  selfipa_done                    14156 non-null  int64  \n",
      " 14  zipregion_missing               14156 non-null  int64  \n",
      " 15  evaluationtime_missing          14156 non-null  int64  \n",
      " 16  desiredinstallationend_missing  14156 non-null  int64  \n",
      " 17  electricitybill_missing         14156 non-null  int64  \n",
      " 18  heatingbill_missing             14156 non-null  int64  \n",
      " 19  mktgparamscore_missing          14156 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  14156 non-null  int64  \n",
      "dtypes: float64(4), int64(11), object(6)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d73e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rates by category:\n",
      "\n",
      "\n",
      "ZIPREGION:\n",
      "                       total_samples  conversions  conversion_rate\n",
      "zipregion                                                         \n",
      "Friuli-Venezia Giulia            776         55.0         0.070876\n",
      "Piemonte                        1179         48.0         0.040712\n",
      "Lombardia                       2152         64.0         0.029740\n",
      "Liguria                          349         10.0         0.028653\n",
      "Veneto                          1105         28.0         0.025339\n",
      "Emilia-Romagna                  1212         30.0         0.024752\n",
      "Marche                           260          6.0         0.023077\n",
      "Toscana                          919         20.0         0.021763\n",
      "Valle D'Aosta                     47          1.0         0.021277\n",
      "Lazio                           1296         25.0         0.019290\n",
      "Trentino-Alto Adige              106          2.0         0.018868\n",
      "Basilicata                       113          2.0         0.017699\n",
      "Umbria                           193          3.0         0.015544\n",
      "Campania                         520          6.0         0.011538\n",
      "Molise                            88          1.0         0.011364\n",
      "Sardegna                         457          5.0         0.010941\n",
      "Sicilia                          903          9.0         0.009967\n",
      "Puglia                          1164          8.0         0.006873\n",
      "Abruzzo                          313          2.0         0.006390\n",
      "Calabria                         548          2.0         0.003650\n",
      "Overall variation: 0.0149\n",
      "\n",
      "MKTGPARAMSCORE:\n",
      "                total_samples  conversions  conversion_rate\n",
      "mktgparamscore                                             \n",
      "Referral                   34          7.0         0.205882\n",
      "form_classico              14          1.0         0.071429\n",
      "Organic                  1146         67.0         0.058464\n",
      "Affiliation                64          3.0         0.046875\n",
      "Other                     470         17.0         0.036170\n",
      "Mediago                   266          9.0         0.033835\n",
      "Google                   1929         52.0         0.026957\n",
      "Youtube                  1688         43.0         0.025474\n",
      "Outbrain                 2374         44.0         0.018534\n",
      "TikTok                   1091         20.0         0.018332\n",
      "Meta                     4181         62.0         0.014829\n",
      "Taboola                   114          0.0         0.000000\n",
      "d2d                         2          0.0         0.000000\n",
      "Overall variation: 0.0532\n"
     ]
    }
   ],
   "source": [
    "# For each categorical column, see conversion rates\n",
    "print(\"Conversion rates by category:\\n\")\n",
    "\n",
    "for col in ['zipregion', 'mktgparamscore']:  # replace with your actual column names\n",
    "    conversion_by_cat = final_df.groupby(col)['netcontractsigned'].agg(['count', 'sum', 'mean'])\n",
    "    conversion_by_cat.columns = ['total_samples', 'conversions', 'conversion_rate']\n",
    "    conversion_by_cat = conversion_by_cat.sort_values('conversion_rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(conversion_by_cat)\n",
    "    print(f\"Overall variation: {conversion_by_cat['conversion_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf1bfd",
   "metadata": {},
   "source": [
    "Marketing gave good variance, would like to maintain that information. Region is not that significant but still good. \n",
    "However given the * of unique values, in both columns I will opt for grouping instead of each value having its column. \n",
    "\n",
    "Instead of 30+ categorical features, you get ~6-8, keeping the predictive power but losing the noise.\n",
    "\n",
    "Downside; this grouping should occasionally double checked to see if it still makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163be87",
   "metadata": {},
   "source": [
    "##### one hot encoding for marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb8c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance groups\n",
    "marketing_groups = {\n",
    "    'High': ['Referral', 'form_classico', 'Organic', 'Affiliation'],\n",
    "    'Medium': ['Other', 'Google', 'Youtube', 'Mediago', 'TikTok'], \n",
    "    'Low': ['Outbrain', 'Meta', 'Taboola', 'd2d']\n",
    "}\n",
    "\n",
    "# Create mapping function\n",
    "def group_marketing(value):\n",
    "    for group, channels in marketing_groups.items():\n",
    "        if value in channels:\n",
    "            return group\n",
    "    return 'Low'  # fallback\n",
    "\n",
    "# Apply grouping\n",
    "final_df['mktg_grouped'] = final_df['mktgparamscore'].apply(group_marketing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9995467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group regions by performance\n",
    "def group_regions(region):\n",
    "    if region == 'Friuli-Venezia Giulia':\n",
    "        return 'High_Performer'\n",
    "    elif region in ['Piemonte', 'Lombardia', 'Emilia-Romagna', 'Veneto']:\n",
    "        return 'Large_Good'\n",
    "    elif region in ['Liguria', 'Toscana', 'Valle D\\'Aosta']:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "final_df['region_grouped'] = final_df['zipregion'].apply(group_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c5957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the grouped categories\n",
    "final_df = pd.get_dummies(final_df, columns=['mktg_grouped', 'region_grouped'], prefix=['mktg', 'region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be7d7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777490e3",
   "metadata": {},
   "source": [
    "df is the log of all calendar_events so a direct join will make our dataset exponentially duplicated.\n",
    "\n",
    "I will first get the unique requestid --> leadid cpairs and then merge that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "937eadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestid_leadid_pairs = df[['requestid', 'leadid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4c8ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requestid_leadid_pairs[requestid_leadid_pairs['requestid'].duplicated(keep=False)]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c10a614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also add leadid to the final df for merging with behaviour data\n",
    "\n",
    "# Suppose the column you want from df1 is called \"col_from_df1\"\n",
    "final_df = final_df.merge(\n",
    "    requestid_leadid_pairs[['requestid', 'leadid']],  # keep only requestid + desired column\n",
    "    on='requestid',                      # join key\n",
    "    how='left'                           # keep all rows from df2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1713e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73b32aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any requestid with no leadid?\n",
    "final_df[final_df['leadid'].isnull()]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18644c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values: False\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns and create final feature matrix\n",
    "columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'mktgparamscore']\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "print(f\"\\nNo missing values: {final_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215f64f",
   "metadata": {},
   "source": [
    "#### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16e8505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n",
      "Dataset ready: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_5968\\1109642794.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_5968\\1109642794.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n"
     ]
    }
   ],
   "source": [
    "# Fix electricitybill missing values\n",
    "final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "\n",
    "# Fix desiredinstallationend_encoded - NaN means 'Unknown' which should be 0 --> check if needed\n",
    "#X_final['desiredinstallationend_encoded'] = X_final['desiredinstallationend_encoded'].fillna(0)\n",
    "\n",
    "# Verify all missing values are gone\n",
    "print(f\"Remaining missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Dataset ready: {final_df.isnull().sum().sum() == 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bc1b",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5df80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling when needed\n",
    "continuous_cols= [\n",
    "    'time_first_sc_to_first_net_fu',\n",
    "    'electricitybill', \n",
    "    'heatingbill'\n",
    "]\n",
    "\n",
    "# Scale only the continuous features\n",
    "scaler = StandardScaler()\n",
    "final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c19f5",
   "metadata": {},
   "source": [
    "Adding behavioral data from Booking Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7cb496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(r\"processed_data/df_model.csv\") # manualy saved leadtime info. hafter having gotten modified in older attempts/notebook\n",
    "behaviour.columns = behaviour.columns.str.lower()\n",
    "merged_df = final_df.merge(behaviour, \n",
    "                          left_on='leadid', \n",
    "                          right_on='id', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_requestids = behaviour[behaviour['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1efb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = merged_df[merged_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n",
    "\n",
    "# --> loooks ok "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da2ee9",
   "metadata": {},
   "source": [
    "#### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a919aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column pairs: []\n",
      "Original shape: (14156, 48)\n",
      "After removing duplicates: (14156, 48)\n"
     ]
    }
   ],
   "source": [
    "# Check your actual dataset for duplicates\n",
    "duplicate_cols = []\n",
    "cols = merged_df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if merged_df.equals(merged_df[cols[j]]):\n",
    "            duplicate_cols.append((cols[i], cols[j]))\n",
    "\n",
    "print(\"Duplicate column pairs:\", duplicate_cols)\n",
    "print(f\"Original shape: {merged_df.shape}\")\n",
    "\n",
    "# Remove duplicates if any found\n",
    "X_final_clean = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "print(f\"After removing duplicates: {X_final_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86b2c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13296 entries, 635 to 14155\n",
      "Data columns (total 48 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   requestid                         13296 non-null  int64  \n",
      " 1   gross_FU                          13296 non-null  int64  \n",
      " 2   gross_SC                          13296 non-null  int64  \n",
      " 3   net_FU                            13296 non-null  float64\n",
      " 4   net_SC                            13296 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu     13296 non-null  float64\n",
      " 6   electricitybill                   13296 non-null  float64\n",
      " 7   heatingbill                       13296 non-null  float64\n",
      " 8   netcontractsigned                 13296 non-null  float64\n",
      " 9   selfipa_done                      13296 non-null  int64  \n",
      " 10  zipregion_missing                 13296 non-null  int64  \n",
      " 11  evaluationtime_missing            13296 non-null  int64  \n",
      " 12  desiredinstallationend_missing    13296 non-null  int64  \n",
      " 13  electricitybill_missing           13296 non-null  int64  \n",
      " 14  heatingbill_missing               13296 non-null  int64  \n",
      " 15  mktgparamscore_missing            13296 non-null  int64  \n",
      " 16  desiredinstallationend_encoded    13296 non-null  int64  \n",
      " 17  mktg_High                         13296 non-null  bool   \n",
      " 18  mktg_Low                          13296 non-null  bool   \n",
      " 19  mktg_Medium                       13296 non-null  bool   \n",
      " 20  region_High_Performer             13296 non-null  bool   \n",
      " 21  region_Large_Good                 13296 non-null  bool   \n",
      " 22  region_Medium                     13296 non-null  bool   \n",
      " 23  region_Other                      13296 non-null  bool   \n",
      " 24  leadid                            13296 non-null  int64  \n",
      " 25  id                                13296 non-null  float64\n",
      " 26  converted                         13296 non-null  float64\n",
      " 27  total_bc_attempts                 13296 non-null  float64\n",
      " 28  total_bc_outcomes                 13296 non-null  float64\n",
      " 29  lead_to_first_bc_days             13296 non-null  float64\n",
      " 30  bc_duration_days                  13296 non-null  float64\n",
      " 31  lead_to_sc1_days                  13296 non-null  float64\n",
      " 32  sc1_schedule_to_appointment_days  13296 non-null  float64\n",
      " 33  bc_frequency                      13296 non-null  float64\n",
      " 34  positive_outcomes_count           13296 non-null  float64\n",
      " 35  negative_outcomes_count           13296 non-null  float64\n",
      " 36  noshow_outcomes_count             13296 non-null  float64\n",
      " 37  positive_outcome_ratio            13296 non-null  float64\n",
      " 38  negative_outcome_ratio            13296 non-null  float64\n",
      " 39  noshow_outcome_ratio              13296 non-null  float64\n",
      " 40  reachability_score                13296 non-null  float64\n",
      " 41  outcome_trend                     13296 non-null  float64\n",
      " 42  persistence_after_negative        13296 non-null  float64\n",
      " 43  showed_up_sc1                     13296 non-null  float64\n",
      " 44  engagement_score                  13296 non-null  float64\n",
      " 45  efficiency_score                  13296 non-null  float64\n",
      " 46  last_bc_outcome_encoded           13296 non-null  float64\n",
      " 47  first_bc_outcome_encoded          13296 non-null  float64\n",
      "dtypes: bool(7), float64(29), int64(12)\n",
      "memory usage: 4.3 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing call data: 860\n",
      "\n",
      "Request IDs with missing call data:\n",
      "[3279, 3729, 3852, 4359, 4689, 5180, 5216, 5837, 6442, 6521, 6882, 7183, 7653, 7899, 8057, 9106, 9971, 10157, 10622, 10630, 11034, 12548, 12741, 13171, 13514, 13648, 13973, 14578, 14807, 14966, 15874, 16288, 16594, 16677, 17124, 17870, 17960, 18429, 18517, 19527, 19753, 20193, 20810, 21062, 21188, 21236, 21468, 21708, 22690, 22879, 23699, 24093, 24359, 25652, 26230, 27380, 27584, 27621, 27693, 27713, 27787, 27808, 27830, 30068, 30881, 31609, 31791, 31932, 32097, 32714, 32990, 33039, 33130, 33550, 33583, 33793, 34334, 34500, 35640, 37319, 39326, 40136, 40444, 40758, 40958, 41170, 41292, 41364, 41729, 42235, 42346, 42989, 43202, 43621, 44179, 44795, 45017, 45483, 45612, 47009, 47240, 47945, 48369, 48407, 48463, 48607, 49417, 49792, 49837, 49844, 50451, 50554, 51125, 51208, 51540, 51586, 51617, 51765, 52123, 52513, 53426, 53880, 54245, 55493, 55701, 55883, 56364, 56513, 56755, 56909, 57466, 57689, 57751, 57906, 59128, 59531, 59708, 60610, 60944, 61103, 61179, 61383, 61684, 61967, 62074, 62158, 62333, 62489, 63221, 63674, 63892, 64695, 65167, 65312, 65357, 65552, 65757, 65926, 65999, 66063, 66188, 66553, 66692, 67651, 67853, 68094, 68235, 68897, 69192, 69329, 69368, 69947, 70047, 70343, 70479, 70611, 71339, 71392, 71925, 72352, 72406, 72910, 72959, 73044, 73065, 73081, 73192, 73196, 73566, 73680, 73954, 74237, 74249, 74736, 75635, 75751, 76216, 76239, 76364, 76511, 76815, 76843, 76852, 77216, 77461, 77527, 77784, 78360, 78372, 78573, 78648, 78723, 79311, 79615, 79652, 79734, 79952, 80006, 80104, 80134, 80145, 80173, 80737, 80741, 80799, 80930, 81750, 81830, 82094, 82290, 82486, 82853, 83013, 83071, 83132, 83629, 83856, 84082, 84307, 84824, 85328, 85772, 85780, 86008, 86018, 86060, 86069, 86365, 86424, 86617, 86710, 86822, 86837, 86872, 86984, 87362, 87411, 87467, 87669, 87900, 87911, 87998, 88273, 88281, 88662, 88783, 88960, 89186, 89325, 89556, 89591, 90045, 90053, 90354, 90819, 91344, 91345, 91397, 91752, 92080, 92083, 92155, 92190, 92659, 92703, 92709, 92827, 93118, 93232, 93470, 93500, 93631, 93647, 93951, 94487, 94504, 94617, 94654, 94951, 94972, 95077, 95119, 95169, 95391, 95778, 95789, 96583, 97023, 97106, 97186, 97313, 97637, 97652, 97860, 97882, 97894, 98047, 98096, 98285, 98400, 98901, 99030, 99192, 99268, 99319, 99611, 99759, 99822, 99982, 100233, 100334, 100447, 100494, 100567, 101045, 101085, 101110, 101111, 101146, 101313, 101351, 101467, 101536, 101537, 101554, 101555, 101571, 101660, 101726, 101727, 101736, 101737, 101824, 101841, 101842, 101941, 101978, 101979, 102021, 102022, 102085, 102156, 102157, 102310, 102311, 102374, 102381, 102422, 102460, 102533, 102644, 102675, 102768, 102788, 102805, 102806, 102832, 102888, 102889, 102901, 102902, 102961, 102974, 102976, 102977, 103016, 103017, 103025, 103041, 103073, 103074, 103108, 103164, 103213, 103214, 103256, 103278, 103288, 103347, 103374, 103403, 103809, 103823, 103826, 103953, 103954, 103955, 103961, 103962, 103985, 104044, 104066, 104095, 104098, 104115, 104192, 104193, 104194, 104206, 104214, 104388, 104446, 104469, 104563, 104564, 104579, 104705, 104746, 104797, 104798, 104810, 104863, 104879, 104906, 104907, 104952, 104953, 105067, 105068, 105103, 105115, 105116, 105155, 105180, 105230, 105294, 105314, 105315, 105323, 105324, 105397, 105422, 105423, 105463, 105464, 105494, 105495, 105506, 105545, 105546, 105558, 105595, 105601, 105602, 105606, 105607, 105608, 105615, 105616, 105634, 105639, 105644, 105686, 105699, 105704, 105714, 105730, 105734, 105754, 105755, 105782, 105783, 105786, 105835, 105841, 105870, 105886, 105955, 105967, 106124, 106125, 106144, 106198, 106199, 106212, 106213, 106219, 106234, 106235, 106246, 106247, 106262, 106275, 106298, 106299, 106305, 106306, 106311, 106312, 106315, 106316, 106323, 106324, 106360, 106361, 106387, 106399, 106405, 106406, 106407, 106409, 106410, 106416, 106417, 106429, 106437, 106461, 106490, 106527, 106534, 106535, 106543, 106544, 106545, 106570, 106571, 106586, 106587, 106588, 106589, 106596, 106600, 106601, 106614, 106615, 106626, 106627, 106630, 106631, 106633, 106639, 106652, 106653, 106657, 106672, 106673, 106678, 106686, 106691, 106692, 106703, 106711, 106712, 106729, 106732, 106733, 106738, 106751, 106761, 106762, 106764, 106765, 106776, 106783, 106784, 106797, 106798, 106799, 106800, 106801, 106802, 106803, 106804, 106805, 106806, 106807, 106810, 106811, 106812, 106813, 106816, 106817, 106819, 106820, 106821, 106822, 106826, 106828, 106829, 106830, 106831, 106832, 106833, 106834, 106835, 106836, 106837, 106838, 106839, 106840, 106841, 106843, 106846, 106847, 106850, 106852, 106853, 106854, 106855, 106856, 106857, 106858, 106859, 106860, 106861, 106862, 106864, 106865, 106866, 106867, 106868, 106869, 106870, 106871, 106872, 106874, 106875, 106876, 106882, 106926, 106927, 106932, 106933, 106979, 106980, 106981, 106982, 106983, 106984, 106987, 106991, 106992, 106993, 106994, 106995, 107000, 107001, 107002, 107005, 107007, 107018, 107019, 107021, 107022, 107098, 107099, 107103, 107122, 107164, 107191, 107192, 107196, 107197, 107206, 107207, 107213, 107217, 107261, 107287, 107288, 107289, 107290, 107304, 107305, 107312, 107313, 107314, 107317, 107322, 107324, 107325, 107333, 107340, 107341, 107351, 107352, 107368, 107369, 107370, 107372, 107373, 107378, 107379, 107385, 107389, 107444, 107445, 107448, 107449, 107456, 107457, 107458, 107465, 107466, 107467, 107474, 107486, 107487, 107525, 107526, 107528, 107533, 107592, 107593, 107608, 107609, 107613, 107614, 107628, 107704, 107740, 107743, 107744, 107843, 107883, 107910, 108021, 108022, 108031, 108032, 108058, 108162, 108163, 108301, 108380, 108422, 108500, 108617, 108650, 108752, 108856, 108872, 108876, 108877, 108887, 108929, 108964, 108970, 108989, 108994, 108997, 109027, 109029, 109078, 109098, 109125, 109126, 109152, 109237, 109262, 109266, 109286, 109480, 109499, 109583, 109632, 109654, 109675, 109682, 109685, 109722, 109732, 109733, 109745, 109747, 109819, 109841, 109850, 109853, 109864, 109865, 109870, 109875, 109910, 109959, 110037, 110041, 110046, 110047, 110080, 110102, 110136, 110138, 110158, 110163, 110198, 110199, 110224, 110371, 110417, 110418, 110446, 110486, 110498, 110609, 110634, 110644, 110646, 110660, 110673, 110681, 110700, 110819, 110966, 111007, 111033, 111108, 111175, 111181, 111189, 111421, 111423, 111424, 111427, 111446, 111519, 111544, 111548, 111587, 111588, 111596, 111611, 111706, 111707, 111946, 111951, 112541, 113431, 113434, 113438, 114304, 114434, 115173, 115346, 117039, 117110, 117111, 117475, 122317, 122318]\n"
     ]
    }
   ],
   "source": [
    "#investigating missing data request ids\n",
    "# Find rows with missing call data //| all calls before march. decide to include or not\n",
    "missing_call_data = merged_df[merged_df['total_bc_attempts'].isnull()]\n",
    "\n",
    "print(f\"Number of rows with missing call data: {len(missing_call_data)}\")\n",
    "\n",
    "# Show the request IDs (assuming you have a request ID column)\n",
    "if 'requestid' in merged_df.columns:\n",
    "    print(\"\\nRequest IDs with missing call data:\")\n",
    "    print(missing_call_data['requestid'].tolist())\n",
    "else:\n",
    "    print(\"\\nColumns available to identify these rows:\")\n",
    "    print(merged_df.columns.tolist())\n",
    "    \n",
    "    # Show index positions as fallback\n",
    "    print(f\"\\nIndex positions of missing rows: {missing_call_data.index.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639      106879\n",
       "640      106880\n",
       "645      106887\n",
       "647      106889\n",
       "651      106893\n",
       "          ...  \n",
       "14137    122721\n",
       "14138    122722\n",
       "14142    122726\n",
       "14152    122736\n",
       "14154    122738\n",
       "Name: requestid, Length: 4540, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see leads that didnt make it to sc --> lead couldve showed up to a different reuqesyts*s apointment because via backtrack there are sc1 appointment for them \n",
    "# ok, 4.5 k requests didnt have net sc, no duplicates.\n",
    "merged_df.loc[merged_df['showed_up_sc1'] == 0, 'requestid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "320cbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4540\n"
     ]
    }
   ],
   "source": [
    "# roughly 3.5 k distinct leads and 4.5 k distinct requests\n",
    "print(leadids.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e5749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop requestid ad other indicatiors\n",
    "#merged_df = merged_df.drop(columns=['requestid', 'id', 'converted', 'leadid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64a85535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversions in missing rows: 7.0\n",
      "Conversion rate in missing rows: 0.0081\n",
      "Conversion rate in complete rows: 0.0253\n"
     ]
    }
   ],
   "source": [
    "# Check conversions in missing call data rows\n",
    "missing_mask = merged_df['total_bc_attempts'].isnull()\n",
    "print(f\"Conversions in missing rows: {merged_df[missing_mask]['netcontractsigned'].sum()}\")\n",
    "print(f\"Conversion rate in missing rows: {merged_df[missing_mask]['netcontractsigned'].mean():.4f}\")\n",
    "print(f\"Conversion rate in complete rows: {merged_df[~missing_mask]['netcontractsigned'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0fc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these requests are very old and dont have propre registstration, some have some registration but essentially there are only 26 contract signed insode t1100, so essentially dropping as a first step of undersampling,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c84629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (13296, 48)\n",
      "New conversion rate: 0.0253\n",
      "Total conversions kept: 337.0\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.dropna().copy()\n",
    "print(f\"New dataset shape: {merged_df.shape}\")\n",
    "print(f\"New conversion rate: {merged_df['netcontractsigned'].mean():.4f}\")\n",
    "print(f\"Total conversions kept: {merged_df['netcontractsigned'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4526a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create subfolder if it doesn't exist\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# Save your merged dataframe\n",
    "merged_df.to_csv('processed_data/merged_df.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "840074b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts (including NaN):\n",
      " showed_up_sc1\n",
      "1.0    8756\n",
      "0.0    4540\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "value_counts_with_nan = merged_df['showed_up_sc1'].value_counts(dropna=False)\n",
    "print(\"Value counts (including NaN):\\n\", value_counts_with_nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
