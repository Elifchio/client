{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700454d-9017-45aa-aac7-634197fece98",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "USE DATABASE ENPAL_ITALY;\n",
    "USE SCHEMA RAW;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "\n",
    "select * from lead_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6674d99-36be-4a44-868e-b3e59ed1b7dc",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "  SELECT \n",
    "            r.id AS requestid,\n",
    "            zipregion,\n",
    "            r.evaluationtime,\n",
    "            r.desiredinstallationend,\n",
    "            electricitybill,\n",
    "            heatingbill,\n",
    "            channel__campaign,\n",
    "            l.channel,\n",
    "            a.grosscontractsigned,\n",
    "           selfipaimportedat\n",
    "        FROM request r\n",
    "        LEFT JOIN lead l \n",
    "            ON l.id = r.leadid\n",
    "        LEFT JOIN airtable_contracts a \n",
    "            ON a.requestid = r.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef142330-9517-4a8d-9c56-01c037536d8f",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "WITH all_events AS (\n",
    "  SELECT \n",
    "    createdat, \n",
    "    deletedat, \n",
    "    requestid, \n",
    "    tsstart, \n",
    "    type, \n",
    "    is_passed, \n",
    "    leadid\n",
    "  FROM calendar_event\n",
    "  WHERE type IN ('VD', 'VDFIELD')\n",
    "),\n",
    "latest_activity AS (\n",
    "  SELECT \n",
    "    requestid,\n",
    "    MAX(createdat) AS latest_date\n",
    "  FROM all_events\n",
    "  GROUP BY requestid\n",
    "),\n",
    "airtable_req AS (\n",
    "  SELECT DISTINCT requestid\n",
    "  FROM airtable_contracts\n",
    "  WHERE requestid IS NOT NULL\n",
    "),\n",
    "-- Group A: requests with NO recent activity in the last 3 months\n",
    "no_recent_activity AS (\n",
    "  SELECT \n",
    "    e.*\n",
    "  FROM all_events e\n",
    "  JOIN latest_activity la ON e.requestid = la.requestid\n",
    "  WHERE la.latest_date < CURRENT_DATE - INTERVAL '3 months'\n",
    "),\n",
    "-- Group B: requests that have a record in airtable_contracts\n",
    "with_contract AS (\n",
    "  SELECT \n",
    "    e.*\n",
    "  FROM all_events e\n",
    "  JOIN airtable_req a ON e.requestid = a.requestid\n",
    ")\n",
    "-- Combine and remove duplicates\n",
    "SELECT *\n",
    "FROM no_recent_activity\n",
    "UNION  -- not UNION ALL!\n",
    "SELECT *\n",
    "FROM with_contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3a074-11ac-4fea-86b6-0b6278a55b7e",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    " WITH c AS (\n",
    "  SELECT \n",
    "    \"Date\",\n",
    "    \"telephoneNumber\",\n",
    "    \"Wrap-up\",\n",
    "    RIGHT(REGEXP_REPLACE(\"telephoneNumber\", '[^0-9]', ''), 10) AS cleaned_phone\n",
    "  FROM genesys_db\n",
    "  WHERE \"Queue\" = 'SELLER'\n",
    "    AND \"Wrap-up\" IN ('FU_FATTO', 'FU_SENZA RISPOSTA')\n",
    "),\n",
    "s AS (\n",
    "  SELECT \n",
    "    phonenumber, \n",
    "    id,\n",
    "    RIGHT(REGEXP_REPLACE(phonenumber, '[^0-9]', ''), 10) AS cleaned_phone\n",
    "  FROM lead\n",
    "),\n",
    "joined_data AS (\n",
    "  SELECT \n",
    "    c.\"Date\", \n",
    "    c.\"Wrap-up\", \n",
    "    r.id AS requestid\n",
    "  FROM c\n",
    "  JOIN s ON c.cleaned_phone = s.cleaned_phone\n",
    "  JOIN request r ON r.leadid = s.id\n",
    "  WHERE s.cleaned_phone IS NOT NULL\n",
    "    AND s.id IS NOT NULL\n",
    "),\n",
    "latest_activity AS (\n",
    "  SELECT \n",
    "    requestid,\n",
    "    MAX(\"Date\") AS latest_date\n",
    "  FROM joined_data\n",
    "  GROUP BY requestid\n",
    "),\n",
    "airtable_req AS (\n",
    "  SELECT DISTINCT requestid\n",
    "  FROM airtable_contracts\n",
    "  WHERE requestid IS NOT NULL\n",
    "),\n",
    "-- Group A: No recent activity in last 3 months\n",
    "no_recent_activity AS (\n",
    "  SELECT \n",
    "    j.\"Date\",\n",
    "    j.\"Wrap-up\",\n",
    "    j.requestid,\n",
    "    'NO_RECENT_ACTIVITY' AS group_type\n",
    "  FROM joined_data j\n",
    "  JOIN latest_activity la ON j.requestid = la.requestid\n",
    "  WHERE la.latest_date < CURRENT_DATE - INTERVAL '3 months'\n",
    "),\n",
    "-- Group B: Has a signed contract\n",
    "with_contract AS (\n",
    "  SELECT \n",
    "    j.\"Date\",\n",
    "    j.\"Wrap-up\",\n",
    "    j.requestid,\n",
    "    'WITH_CONTRACT' AS group_type\n",
    "  FROM joined_data j\n",
    "  JOIN airtable_req a ON j.requestid = a.requestid\n",
    ")\n",
    "-- Union both groups\n",
    "SELECT   \"Date\",\n",
    "    \"Wrap-up\",\n",
    "    requestid,\n",
    "FROM no_recent_activity\n",
    "UNION \n",
    "SELECT  \"Date\",\n",
    "    \"Wrap-up\",\n",
    "    requestid,\n",
    "FROM with_contract;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fdf076-c74d-44d5-b7ba-cb2d235354ed",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "select distinct id as requestid, leadid from request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20bf23-0525-483e-b5ea-8a08f448ac32",
   "metadata": {
    "language": "sql",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "WITH first_dispatcher AS (\n",
    "    SELECT \n",
    "        leadid,\n",
    "        createdat AS first_dispatcher_createdat\n",
    "    FROM dispatcher_answers\n",
    "    WHERE outcome = 'VDTOPLAN'\n",
    "    QUALIFY ROW_NUMBER() OVER (\n",
    "        PARTITION BY leadid\n",
    "        ORDER BY createdat ASC\n",
    "    ) = 1\n",
    "),\n",
    "\n",
    "first_calendar AS (\n",
    "    SELECT\n",
    "        leadid,\n",
    "        tsstart AS first_calendar_createdat\n",
    "    FROM calendar_event\n",
    "    WHERE type IN ('VD', 'VDFIELD')  -- ðŸ‘ˆ replace with your actual list of types\n",
    "      AND deletedat IS NULL\n",
    "    QUALIFY ROW_NUMBER() OVER (\n",
    "        PARTITION BY leadid\n",
    "        ORDER BY createdat ASC\n",
    "    ) = 1\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    d.leadid,\n",
    "    ROUND(\n",
    "        DATEDIFF('second', d.first_dispatcher_createdat, c.first_calendar_createdat) / 3600.0,\n",
    "        2\n",
    "    ) AS bc_to_sc_hour\n",
    "FROM first_dispatcher d\n",
    "LEFT JOIN first_calendar c \n",
    "    ON d.leadid = c.leadid\n",
    "WHERE c.first_calendar_createdat > d.first_dispatcher_createdat;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a318b2d-11de-448f-9319-3bc43af3d887",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "#leadtime added bc to sc \n",
    "leadtime = cell2.to_pandas()\n",
    "lead_bc_to_df = cell19.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ce99f-f4e3-4795-9353-a7ee227b9d65",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "lead_bc_to_df.columns = lead_bc_to_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ff4d3-75f4-49a7-b9d2-15e9ae604fe1",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "leadtime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Then, we can use the python name to turn cell2 into a Pandas dataframe\n",
    "\n",
    "client_info = cell4.to_pandas()\n",
    "sc_df = cell5.to_pandas()\n",
    "fu_df = cell6.to_pandas()\n",
    "requestid_leadid_pairs = cell7.to_pandas()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea84db-b444-492f-8efe-f1b53c786a04",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "client_info.columns = client_info.columns.str.lower()\n",
    "sc_df.columns = sc_df.columns.str.lower()\n",
    "fu_df.columns = fu_df.columns.str.lower()\n",
    "requestid_leadid_pairs.columns = requestid_leadid_pairs.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471e2c9-9cad-4148-b774-df2db674a876",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "def customer_behaviour_leadtime(df):\n",
    "   \n",
    "        \n",
    "        # Create a copy to avoid modifying original data\n",
    "        df_features = df.copy()\n",
    "        \n",
    "        # Convert date columns to datetime\n",
    "        date_columns = ['LEAD_CREATION_DATE', 'SC1_SCHEDULED', 'SC1_APPOINTMENT', \n",
    "                       'LAST_FU', 'NEXT_FU', 'CONTRACT_SIGNED', 'WITHDRAWAL', \n",
    "                       'IPA_SCHEDULED', 'IPA_DAY']\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in df_features.columns:\n",
    "                df_features[col] = pd.to_datetime(df_features[col], errors='coerce')\n",
    "        \n",
    "        # BC date columns\n",
    "        bc_date_cols = [f'BC_{i}' for i in range(1, 19)]\n",
    "        for col in bc_date_cols:\n",
    "            if col in df_features.columns:\n",
    "                df_features[col] = pd.to_datetime(df_features[col], errors='coerce')\n",
    "        \n",
    "        # BC outcome columns\n",
    "        bc_outcome_cols = [f'BC_{i}_OUTCOME' for i in range(1, 19)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 1. BOOKING CALL EFFICIENCY FEATURES\n",
    "        # Count total BC attempts (non-null BC dates)\n",
    "        bc_dates = df_features[bc_date_cols].copy()\n",
    "        df_features['total_bc_attempts'] = bc_dates.notna().sum(axis=1)\n",
    "        \n",
    "        # Count total BC outcomes (non-null outcomes)\n",
    "        bc_outcomes = df_features[bc_outcome_cols].copy()\n",
    "        df_features['total_bc_outcomes'] = bc_outcomes.notna().sum(axis=1)\n",
    "        \n",
    "        # First and last BC attempt dates\n",
    "        df_features['first_bc_date'] = bc_dates.min(axis=1)\n",
    "        df_features['last_bc_date'] = bc_dates.max(axis=1)\n",
    "        \n",
    "   \n",
    "        \n",
    "        # 2. TEMPORAL FEATURES\n",
    "        # Days from lead creation to first BC\n",
    "        df_features['lead_to_first_bc_days'] = (\n",
    "            df_features['first_bc_date'] - df_features['LEAD_CREATION_DATE']\n",
    "        ).dt.days\n",
    "        \n",
    "        # Days from first BC to last BC (booking call duration)\n",
    "        df_features['bc_duration_days'] = (\n",
    "            df_features['last_bc_date'] - df_features['first_bc_date']\n",
    "        ).dt.days\n",
    "        \n",
    "        # Days from lead creation to SC1 scheduled\n",
    "        df_features['lead_to_sc1_days'] = (\n",
    "            df_features['SC1_SCHEDULED'] - df_features['LEAD_CREATION_DATE']\n",
    "        ).dt.days\n",
    "        \n",
    "        # Days from SC1 scheduled to appointment\n",
    "        df_features['sc1_schedule_to_appointment_days'] = (\n",
    "            df_features['SC1_APPOINTMENT'] - df_features['SC1_SCHEDULED']\n",
    "        ).dt.days\n",
    "        \n",
    "        # BC call frequency (attempts per day during BC period)\n",
    "        df_features['bc_frequency'] = np.where(\n",
    "            df_features['bc_duration_days'] > 0,\n",
    "            df_features['total_bc_attempts'] / (df_features['bc_duration_days'] + 1),\n",
    "            df_features['total_bc_attempts']  # If same day, frequency = total attempts\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # 3. ENGAGEMENT QUALITY FEATURES\n",
    "        # Define outcome categories\n",
    "        positive_outcomes = ['VDTOPLAN', 'REACHED', 'CONFIRMED', 'INTERESTED']\n",
    "        negative_outcomes = ['LOST', 'NOTINTERESTED', 'UNQUALIFIED']\n",
    "        neutral_outcomes = ['TOBERECALLED', 'RESCHEDULE']\n",
    "        unreachable_outcomes = ['NOTREACHED', 'NOANSWER', 'VOICEMAIL']\n",
    "        noshow_outcomes = ['NOSHOW', 'NOSHOWNOTREACHED', 'NOSHOWLOST', 'NOSHOWVDTOPLAN', 'NOSHOWTOBERECALLED']\n",
    "        \n",
    "        # Count outcomes by category\n",
    "        def count_outcome_category(row, category_list):\n",
    "            count = 0\n",
    "            for col in bc_outcome_cols:\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    if any(cat.upper() in str(row[col]).upper() for cat in category_list):\n",
    "                        count += 1\n",
    "            return count\n",
    "        \n",
    "        df_features['positive_outcomes_count'] = df_features.apply(\n",
    "            lambda row: count_outcome_category(row, positive_outcomes), axis=1\n",
    "        )\n",
    "        \n",
    "        df_features['negative_outcomes_count'] = df_features.apply(\n",
    "            lambda row: count_outcome_category(row, negative_outcomes), axis=1\n",
    "        )\n",
    "        \n",
    "        df_features['noshow_outcomes_count'] = df_features.apply(\n",
    "            lambda row: count_outcome_category(row, noshow_outcomes), axis=1\n",
    "        )\n",
    "        \n",
    "        df_features['unreachable_outcomes_count'] = df_features.apply(\n",
    "            lambda row: count_outcome_category(row, unreachable_outcomes), axis=1\n",
    "        )\n",
    "        \n",
    "        # Calculate ratios (avoid division by zero)\n",
    "        total_outcomes = df_features['total_bc_outcomes']\n",
    "        df_features['positive_outcome_ratio'] = np.where(\n",
    "            total_outcomes > 0, \n",
    "            df_features['positive_outcomes_count'] / total_outcomes, \n",
    "            0\n",
    "        )\n",
    "        \n",
    "        df_features['negative_outcome_ratio'] = np.where(\n",
    "            total_outcomes > 0,\n",
    "            df_features['negative_outcomes_count'] / total_outcomes,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        df_features['noshow_outcome_ratio'] = np.where(\n",
    "            total_outcomes > 0,\n",
    "            df_features['noshow_outcomes_count'] / total_outcomes,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        df_features['reachability_score'] = np.where(\n",
    "            total_outcomes > 0,\n",
    "            1 - (df_features['unreachable_outcomes_count'] / total_outcomes),\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 4. BEHAVIORAL PATTERN FEATURES\n",
    "        # Last BC outcome (most recent attempt result)\n",
    "        def get_last_bc_outcome(row):\n",
    "            for i in range(18, 0, -1):  # Check from BC_18 down to BC_1\n",
    "                col = f'BC_{i}_OUTCOME'\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    return row[col]\n",
    "            return 'NO_OUTCOME'\n",
    "        \n",
    "        df_features['last_bc_outcome'] = df_features.apply(get_last_bc_outcome, axis=1)\n",
    "        \n",
    "        # First BC outcome\n",
    "        def get_first_bc_outcome(row):\n",
    "            for i in range(1, 19):  # Check from BC_1 to BC_18\n",
    "                col = f'BC_{i}_OUTCOME'\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    return row[col]\n",
    "            return 'NO_OUTCOME'\n",
    "        \n",
    "        df_features['first_bc_outcome'] = df_features.apply(get_first_bc_outcome, axis=1)\n",
    "        \n",
    "        # Outcome improvement/deterioration pattern\n",
    "        def outcome_trend(row):\n",
    "            outcomes = []\n",
    "            for i in range(1, 19):\n",
    "                col = f'BC_{i}_OUTCOME'\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    # Score outcomes (higher = better)\n",
    "                    if any(pos.upper() in str(row[col]).upper() for pos in positive_outcomes):\n",
    "                        outcomes.append(3)\n",
    "                    elif any(neu.upper() in str(row[col]).upper() for neu in neutral_outcomes):\n",
    "                        outcomes.append(2)\n",
    "                    elif any(unr.upper() in str(row[col]).upper() for unr in unreachable_outcomes):\n",
    "                        outcomes.append(1)\n",
    "                    else:\n",
    "                        outcomes.append(0)  # negative outcomes\n",
    "            \n",
    "            if len(outcomes) < 2:\n",
    "                return 0  # No trend\n",
    "            \n",
    "            # Calculate trend (positive = improving, negative = deteriorating)\n",
    "            return outcomes[-1] - outcomes[0]\n",
    "        \n",
    "        df_features['outcome_trend'] = df_features.apply(outcome_trend, axis=1)\n",
    "        \n",
    "        # Persistence score (attempts after first negative outcome)\n",
    "        def persistence_score(row):\n",
    "            first_negative_pos = None\n",
    "            total_after_negative = 0\n",
    "            \n",
    "            for i in range(1, 19):\n",
    "                col = f'BC_{i}_OUTCOME'\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    if first_negative_pos is None and any(neg.upper() in str(row[col]).upper() for neg in negative_outcomes):\n",
    "                        first_negative_pos = i\n",
    "                    elif first_negative_pos is not None:\n",
    "                        total_after_negative += 1\n",
    "            \n",
    "            return total_after_negative\n",
    "        \n",
    "        df_features['persistence_after_negative'] = df_features.apply(persistence_score, axis=1)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # 5. CONVERSION-RELATED FEATURES\n",
    "        # Binary target\n",
    "        df_features['converted'] = df_features['CONTRACT_SIGNED'].notna().astype(int)\n",
    "        \n",
    "     \n",
    "        # Show-up for SC1 appointment\n",
    "        df_features['showed_up_sc1'] = df_features['SC1_APPOINTMENT'].notna().astype(int)\n",
    "        \n",
    "    \n",
    "        \n",
    "        # 6. COMPOSITE SCORES\n",
    "        # Overall engagement score (weighted combination)\n",
    "        df_features['engagement_score'] = (\n",
    "            0.3 * df_features['positive_outcome_ratio'] + \n",
    "            0.2 * df_features['reachability_score'] + \n",
    "            0.2 * (1 - df_features['negative_outcome_ratio']) +\n",
    "            0.1 * (1 - df_features['noshow_outcome_ratio']) +\n",
    "            0.1 * np.clip(df_features['bc_frequency'], 0, 5) / 5 +  # Cap frequency at 5\n",
    "            0.1 * np.clip(df_features['outcome_trend'], -3, 3) / 6  # Normalize trend\n",
    "        )\n",
    "        \n",
    "        # Efficiency score (quick to schedule with few attempts)\n",
    "        df_features['efficiency_score'] = np.where(\n",
    "            df_features['total_bc_attempts'] > 0,\n",
    "            1 / (1 + df_features['total_bc_attempts']),  # Inverse of attempts\n",
    "            0\n",
    "        )\n",
    "        \n",
    "     \n",
    "        \n",
    "        # Fill NaN values in temporal features\n",
    "        temporal_features = [\n",
    "            'lead_to_first_bc_days', 'bc_duration_days', 'lead_to_sc1_days',\n",
    "            'sc1_schedule_to_appointment_days', 'bc_frequency'\n",
    "        ]\n",
    "        \n",
    "        for feature in temporal_features:\n",
    "            if feature in df_features.columns:\n",
    "                df_features[feature] = df_features[feature].fillna(0)\n",
    "        \n",
    "      \n",
    "        \n",
    "        # Return only the engineered features (remove original columns for cleaner output)\n",
    "        feature_columns = [\n",
    "            'ID', 'converted',  # Keep ID and target\n",
    "            # Booking call efficiency\n",
    "            'total_bc_attempts', 'total_bc_outcomes',\n",
    "            # Temporal features\n",
    "            'lead_to_first_bc_days', 'bc_duration_days', 'lead_to_sc1_days',\n",
    "            'sc1_schedule_to_appointment_days', 'bc_frequency',\n",
    "            # Engagement quality\n",
    "            'positive_outcomes_count', 'negative_outcomes_count', 'noshow_outcomes_count',\n",
    "            'positive_outcome_ratio', 'negative_outcome_ratio', 'noshow_outcome_ratio', 'reachability_score',\n",
    "            # Behavioral patterns\n",
    "            'last_bc_outcome', 'first_bc_outcome', 'outcome_trend', 'persistence_after_negative',\n",
    "            # Conversion related\n",
    "            'showed_up_sc1',\n",
    "            # Composite scores\n",
    "            'engagement_score', 'efficiency_score'\n",
    "        ]\n",
    "        \n",
    "        df_features = df_features[feature_columns]\n",
    "    \n",
    "        #encoding categorical variables\n",
    "    \n",
    "        df_model = df_features.copy()\n",
    "    \n",
    "        # Label encode the categorical feature\n",
    "        le = LabelEncoder()\n",
    "        df_model['last_bc_outcome_encoded'] = le.fit_transform(df_model['last_bc_outcome'])\n",
    "    \n",
    "        # Drop the original categorical column\n",
    "        df_model = df_model.drop('last_bc_outcome', axis=1)\n",
    "    \n",
    "        # Encode first_bc_outcome\n",
    "        le_first = LabelEncoder()\n",
    "        df_model['first_bc_outcome_encoded'] = le_first.fit_transform(df_model['first_bc_outcome'])\n",
    "    \n",
    "        # Drop the original categorical column\n",
    "        df_model = df_model.drop('first_bc_outcome', axis=1)\n",
    "    \n",
    "        return df_model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5969c-1983-4677-8aff-4b65461546ac",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "def info_client_request(df3):\n",
    "            \n",
    "        def get_aggregated_value(row):\n",
    "            # Check Youtube first\n",
    "            if row.get('Youtube') == 1:\n",
    "                return \"Youtube\"\n",
    "            \n",
    "            # Check specific CHANNEL__CAMPAIGN values\n",
    "            channel_campaign = row.get('channel_campaign')\n",
    "            if channel_campaign == \"d2d\":\n",
    "                return \"d2d\"\n",
    "            elif channel_campaign == \"form_classico\":\n",
    "                return \"Form_FB\"\n",
    "            elif channel_campaign == \"ranger\":\n",
    "                return \"ranger\"\n",
    "            elif channel_campaign == \"referral_link\":\n",
    "                return \"Referral\"\n",
    "            elif channel_campaign == \"referral_manual\":\n",
    "                return \"Referral\"\n",
    "            \n",
    "            # Check Channel column (case insensitive)\n",
    "            channel = row.get('channel')\n",
    "            if channel and pd.notna(channel):\n",
    "                channel_lower = channel.lower()\n",
    "                \n",
    "                if channel_lower == \"affiliation\":\n",
    "                    return \"Affiliation\"\n",
    "                elif channel_lower == \"outbrain\":\n",
    "                    return \"Outbrain\"\n",
    "                elif channel_lower in [\"taboola\", \"taboola_it\"]:\n",
    "                    return \"Taboola\"\n",
    "                elif channel_lower in [\"facebook\", \"fb\", \"ig\", \"instagram\", \"facebook_marketplace\"]:\n",
    "                    return \"Meta\"\n",
    "                elif channel_lower in [\"search ads\", \"searchads\", \"google\", \"google ads\"]:\n",
    "                    return \"Google\"\n",
    "                elif channel_lower in [\"organic_brand\", \"organic\", \"direct\"]:\n",
    "                    return \"Organic\"\n",
    "                elif channel_lower == \"seo\":\n",
    "                    return \"SEO\"\n",
    "                elif channel_lower == \"criteo\":\n",
    "                    return \"Criteo\"\n",
    "                elif channel_lower == \"mediago\":\n",
    "                    return \"MediaGo\"\n",
    "                elif channel_lower == \"tiktok\":\n",
    "                    return \"Tik Tok\"\n",
    "            \n",
    "            return \"Other\"\n",
    "        \n",
    "        df3['aggregated'] = df3.apply(get_aggregated_value, axis=1)\n",
    "        df3.drop('channel', axis=1, inplace=True)\n",
    "        df3.drop('channel__campaign', axis=1, inplace=True)\n",
    "        df3['selfipa_done'] = df3['selfipaimportedat'].notnull().astype(int)\n",
    "        df3.drop('selfipaimportedat', axis=1, inplace=True) \n",
    "        return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdb2ce-d291-475d-8d15-f183e7a51a31",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "def transform_sc(df_fu, df_sc):\n",
    "\n",
    "       # transforming sc and fu records\n",
    "         # start with fu records\n",
    "        \n",
    "        df_fu['grouped_type'] = 'FU'\n",
    "        df_fu.columns = df_fu.columns.str.lower()\n",
    "\n",
    "        # net event calculation\n",
    "        df_fu['is_net_event'] = 0\n",
    "        df_fu.loc[df_fu['wrap-up'] == 'FU_FATTO', 'is_net_event']= 1 \n",
    "\n",
    "        #drop wrap up column after calculation\n",
    "        df_fu = df_fu.drop(columns = 'wrap-up')\n",
    "\n",
    "        # fix date colum\n",
    "        df_fu['date'] = pd.to_datetime(df_fu['date'])\n",
    "    \n",
    "\n",
    "        # transform sc\n",
    "        #assign type\n",
    "       \n",
    "        df_sc.columns = df_sc.columns.str.lower()\n",
    "        df_sc['grouped_type'] = 'SC'\n",
    "\n",
    "        #net event calculation\n",
    "        df_sc['is_net_event'] = 0\n",
    "        df_sc.loc[df_sc['deletedat'].isna() & df_sc['is_passed'], 'is_net_event'] = 1\n",
    "        #delete unneeded columns\n",
    "        df_sc = df_sc.drop(columns = ['is_passed', 'deletedat', 'createdat'])\n",
    "\n",
    "        #rename tsstart to date to macth the rest of the df\n",
    "        df_sc.rename(columns ={\n",
    "        'tsstart': 'date'\n",
    "        }, inplace=True)\n",
    "\n",
    "        #fix datetime\n",
    "        df_sc['date']= pd.to_datetime(df_sc['date'])\n",
    "\n",
    "        #put fu and sc together \n",
    "        df = pd.concat([df_sc, df_fu], ignore_index=True)\n",
    "\n",
    "        #set is net to boolean for calculations\n",
    "        df['is_net_event'] = df['is_net_event'].astype(bool)\n",
    "\n",
    "       \n",
    "                # Gross counts (all events)\n",
    "        gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "        gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "        # Net counts (only events that actually happened)\n",
    "        net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "        net_counts.columns = [f'net_{col}' for col in net_counts.columns]\n",
    "\n",
    "        # Combine gross and net counts_\n",
    "        counts_df = gross_counts.join(net_counts, how='outer').fillna(0)\n",
    "\n",
    "        # at this point requestid is the index. make it explicit\n",
    "        counts_df = counts_df.reset_index()\n",
    "    \n",
    "        return counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d22625-308d-4fa7-8290-2a413f831036",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "def join_dfs(leadtime, sc_df, client_info, requestid_leadid_pairs):\n",
    "             #merge client info and sc records \n",
    "        final_df = sc_df.merge(client_info, on='requestid', how='left')\n",
    "\n",
    "        leadtime.columns = leadtime.columns.str.lower()\n",
    "\n",
    "    \n",
    "        # dont know why but lets impute missing values here \n",
    "        final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "        final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)\n",
    "    \n",
    "        # also encoding why not\n",
    "    \n",
    "        # mark the missingness inside the row;\n",
    "        missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'aggregated']\n",
    "    \n",
    "        for col in missing_cols:\n",
    "            final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "    \n",
    "        # desiredinstallationend\n",
    "        type_mapping = {\n",
    "            # Original Italian values\n",
    "            '3-4mesi': 'three_to_four_months', \n",
    "            '5+mesi': 'more_than_5_months',\n",
    "            '1-2mesi': 'one_to_two_months',\n",
    "            'Non lo so': 'dont_know',\n",
    "            'short': np.nan,\n",
    "            # Already mapped values (keep as-is)\n",
    "            'dont_know': 'dont_know',\n",
    "            'three_to_four_months': 'three_to_four_months',\n",
    "            'one_to_two_months': 'one_to_two_months', \n",
    "            'more_than_5_months': 'more_than_5_months',\n",
    "            # Handle string 'nan'\n",
    "            'nan': np.nan\n",
    "        }\n",
    "    \n",
    "        final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)\n",
    "    \n",
    "            # evaluationtime\n",
    "        type_mapping = {\n",
    "            # Original Italian values\n",
    "            '3-6 mesi': np.nan,\n",
    "            '<3 mesi': np.nan,\n",
    "            '>6 mesi': np.nan,\n",
    "            # Already mapped English values\n",
    "            'less_than_three_months': np.nan,\n",
    "            'more_than_six_months': np.nan,\n",
    "            # Other values that appear in your data\n",
    "            'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "            'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "            'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "            'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "            # Handle string 'nan'\n",
    "            'nan': np.nan\n",
    "        }\n",
    "    \n",
    "        # Create grouped_type column\n",
    "        final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n",
    "    \n",
    "        # Replace the old columns directly\n",
    "        final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "        final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "    \n",
    "        # Drop the temporary columns\n",
    "        final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)\n",
    "    \n",
    "        # handle nan before encoding\n",
    "        final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "        final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n",
    "    \n",
    "        \n",
    "    \n",
    "        # for marketing and region. nb this will need to be analyzed and updated every once in a while\n",
    "        aggregated_groups = {\n",
    "        'High': ['Affiliation', 'Taboola'],           # >25% conversion\n",
    "        'Medium': ['Google', 'Criteo', 'Organic'],    # 15-20% conversion  \n",
    "        'Low': ['Outbrain', 'Meta', 'Other', 'MediaGo', 'Tik Tok']  # <12% conversion\n",
    "    }\n",
    "        def group_aggregated(value):\n",
    "            if value == 'SEO':  # Handle the unreliable outlier\n",
    "                return 'Medium'  # Conservative assignment\n",
    "            \n",
    "            for group, channels in aggregated_groups.items():\n",
    "                if value in channels:\n",
    "                    return group\n",
    "            return 'Low'  # fallback\n",
    "    \n",
    "    # Apply grouping\n",
    "        final_df['mktg_grouped'] = final_df['aggregated'].apply(group_aggregated)\n",
    "    \n",
    "        # Group regions by performance\n",
    "        def group_regions(region):\n",
    "            # Top performers with good sample sizes\n",
    "            if region in ['Trentino-Alto Adige', 'Friuli-Venezia Giulia', 'Liguria']:\n",
    "                return 'High_Performer'  # 18-24% conversion\n",
    "            \n",
    "            # Large regions with solid performance  \n",
    "            elif region in ['Lombardia', 'Piemonte', 'Veneto', 'Toscana', 'Emilia-Romagna']:\n",
    "                return 'Large_Solid'     # 11-14% conversion\n",
    "            \n",
    "            # Smaller regions with decent samples\n",
    "            elif region in ['Umbria', 'Lazio', 'Marche']:\n",
    "                return 'Medium'          # 10-13% conversion\n",
    "            \n",
    "            # Lower performing regions\n",
    "            else:\n",
    "                return 'Lower'   \n",
    "    \n",
    "        final_df['region_grouped'] = final_df['zipregion'].apply(group_regions)\n",
    "        final_df = pd.get_dummies(final_df, columns=['mktg_grouped', 'region_grouped'], prefix=['mktg', 'region'])\n",
    "    \n",
    "   \n",
    "    \n",
    "        # also add leadid to the final df for merging with behaviour data\n",
    "        # probably there is a cleaner way to do this\n",
    "    \n",
    "        # Suppose the column you want from df1 is called \"col_from_df1\"\n",
    "        final_df = final_df.merge(\n",
    "            requestid_leadid_pairs,  # keep only requestid + desired column\n",
    "            on='requestid',                      # join key\n",
    "            how='left'                           # keep all rows from df2\n",
    "        )\n",
    "        #drop columns we encoded\n",
    "        columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'aggregated']\n",
    "        final_df = final_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "        #imputing\n",
    "        # Fix electricitybill missing values\n",
    "        final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "        final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "    \n",
    "        #scaling \n",
    "        continuous_cols= [\n",
    "           \n",
    "            'electricitybill', \n",
    "            'heatingbill'\n",
    "        ]\n",
    "    \n",
    "        # Scale only the continuous features\n",
    "        scaler = StandardScaler()\n",
    "        final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])\n",
    "    \n",
    "    \n",
    "        #finally merge with leadtime\n",
    "    \n",
    "        merged_df = final_df.merge(leadtime, \n",
    "                              left_on='leadid', \n",
    "                              right_on='id', \n",
    "                              how='left')\n",
    "        \n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8839c3-23f0-4509-a75c-fa8f199270c3",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e982e17-8de7-4f23-9f5b-9e243690e825",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "\n",
    "leadtime = customer_behaviour_leadtime(leadtime)\n",
    "leadtime.columns = leadtime.columns.str.lower()\n",
    "leadtime= pd.merge(leadtime, lead_bc_to_df, left_on='id', right_on='leadid', how='left').drop(columns='leadid')\n",
    "sc_df = transform_sc(fu_df, sc_df)\n",
    "client_info = info_client_request(client_info)\n",
    "final_df = join_dfs(leadtime, sc_df, client_info, requestid_leadid_pairs)\n",
    "final_df.columns = final_df.columns.str.lower()\n",
    "final_df['grosscontractsigned'] = final_df['grosscontractsigned'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c67e5-53ad-4d0b-aa9a-7bcfcbe28454",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b36d8a-0c19-4367-8146-7a2cc4f7b9bf",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": [
    "Analysis of final df\n",
    "\n",
    "Data analysis and feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e69dc-a881-4b6f-a89e-36401b1d7a4c",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "df = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650fdd91-eb5d-4d69-8419-d9e0b8736ad1",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "df['bc_to_sc_hour'].var(ddof=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8b80e-d060-4804-a126-2731ee3a9aaa",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2531a1-ffa7-45b5-886b-ccd99ee556bc",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# variance and ditribution\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(df.select_dtypes(include=['int64', 'float64']))\n",
    "low_var_cols = df.select_dtypes(include=['int64', 'float64']).columns[~selector.get_support()]\n",
    "print(low_var_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d420c5-0212-4787-aff2-65a9f63654cb",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df['gross_fu'].hist(bins=50)\n",
    "print(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b818bfa-c587-4e44-b471-bc78d20d9c08",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "corr['converted'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2ed1d-0ed0-47f9-a136-6e061cb0c704",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b63cf-1dca-48d4-a1b9-81952388fa2b",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "X = df.drop(columns=['converted', 'requestid', 'id', 'leadid', 'grosscontractsigned'])\n",
    "y = df['grosscontractsigned']\n",
    "model.fit(X, y)\n",
    "# Create feature importance DataFrame\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance.head(20), x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 20 Feature Importances (XGBoost)', fontsize=14)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39764247-069e-4dea-af88-7765f79a4b28",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "#take x because its dropped id and stuff \n",
    "variances = X.var(numeric_only=True)\n",
    "variances.sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53873c85-ba38-4e79-9e5c-17d18ded5e38",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "variances.sort_values().head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903ca6c-4a16-4b17-8b5b-20a0a3ed7ef9",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": [
    "df[['lead_to_sc1_days','bc_duration_days','lead_to_first_bc_days']].hist(bins=50, figsize=(10,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ba35d-fc8c-4d7b-bcd0-e4ad66b1a758",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "\n",
    "ids = final_df[\"requestid\"]\n",
    "df = final_df.drop(columns=[\"requestid\",\"id\",\"converted\",\"leadid\"])\n",
    "y_sc = (df['net_sc'] >= 1).astype(int)  # Stage 1: Did they reach SC?\n",
    "y_sign = df['grosscontractsigned']       # Stage 2: Did they sign?\n",
    "\n",
    "# Drop target from features\n",
    "X = df.drop('grosscontractsigned', axis=1)\n",
    "\n",
    "# Split with stratification on y_sc (since that's our first model)\n",
    "X_train, X_test, y_train_sc, y_test_sc, y_train_sign, y_test_sign = train_test_split(\n",
    "X, y_sc, y_sign, test_size=0.2, stratify=y_sc, random_state=42\n",
    ")\n",
    "X_layer1 = X.drop(columns=['gross_fu','gross_sc','net_fu','net_sc',\n",
    "                            'selfipa_done',\n",
    "                           'showed_up_sc1','sc1_schedule_to_appointment_days','lead_to_sc1_days'])\n",
    "X_layer2 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a0b6a-a82d-41a3-aace-0f495fc379dd",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL_SC (stage 1)\n",
    "\n",
    "# Calculate class weights on TRAINING data only\n",
    "scale_pos_weight_sc = (len(y_train_sc) - y_train_sc.sum()) / y_train_sc.sum()\n",
    "\n",
    "model_sc = xgb.XGBClassifier(\n",
    "    n_estimators=100, max_depth=4, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight_sc, random_state=42,\n",
    "    use_label_encoder=False, eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# Fit on training data with layer1 features\n",
    "model_sc.fit(X_layer1.loc[X_train.index], y_train_sc)\n",
    "\n",
    "# TRAIN MODEL_SIGN (stage 2)\n",
    "\n",
    "# Mask: only train on samples that reached SC stage\n",
    "mask_train_sc = (df.loc[X_train.index, 'net_sc'] >= 1)\n",
    "\n",
    "# Calculate class weights on the subset that reached SC\n",
    "scale_pos_weight_sign = (mask_train_sc.sum() - y_train_sign[mask_train_sc].sum()) / y_train_sign[mask_train_sc].sum()\n",
    "\n",
    "model_sign = xgb.XGBClassifier(\n",
    "    n_estimators=100, max_depth=4, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight_sign, random_state=42,\n",
    "    use_label_encoder=False, eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# Fit on training samples that reached SC\n",
    "model_sign.fit(X_layer2.loc[X_train.index][mask_train_sc], y_train_sign[mask_train_sc])\n",
    "\n",
    "# Serialize and store both models\n",
    "model_bytes_sc = pickle.dumps(model_sc)\n",
    "model_bytes_sign = pickle.dumps(model_sign)\n",
    "\n",
    "model_entry = pd.DataFrame({\n",
    "    \"TRAIN_DATE\": [pd.Timestamp.now()],\n",
    "    \"MODEL_SC\": [model_bytes_sc],\n",
    "    \"MODEL_SIGN\": [model_bytes_sign]\n",
    "})\n",
    "\n",
    "session.write_pandas(model_entry, \"ML_CASCADE_MODELS\", auto_create_table=True, overwrite=True)\n",
    "return \"âœ… Training completed and models stored in ML_CASCADE_MODELS\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "Elif.Yilmaz@enpal.de",
   "authorId": "6026428956118",
   "authorName": "ELIF.YILMAZ@ENPAL.DE",
   "lastEditTime": 1761658317092,
   "notebookId": "vb7nhruuyi2aqtvqo34r",
   "sessionId": "13f617a6-d8a0-4fd2-ab66-6de4462a9143"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
