{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0fa823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests\n",
    "file_path = r\"C:\\Users\\ElifYilmaz\\Downloads\\project 2.0.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "#older contracts\n",
    "path = r\"C:\\Users\\ElifYilmaz\\Downloads\\project 2.0_contracts.csv\"\n",
    "df2 = pd.read_csv(path)\n",
    "df2.columns = df.columns.str.lower()\n",
    "\n",
    "#concat\n",
    "combined_df = pd.concat([df, df2], ignore_index=True)\n",
    "df = combined_df.copy()\n",
    "\n",
    "# qualitative \n",
    "\n",
    "file2 = r\"C:\\Users\\ElifYilmaz\\Downloads\\info client.csv\"\n",
    "\n",
    "df2 = pd.read_csv(file2)\n",
    "df2.columns = df2.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1648b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of requestids with duplicates: 35\n",
      "Total duplicate rows: 35\n",
      "\n",
      "Dropped 8 absolute duplicate rows\n",
      "Remaining duplicate requestids before netcontractsigned cleanup: 27\n",
      "Dropped 27 rows where netcontractsigned = 0 from duplicate requestids\n",
      "\n",
      "==================================================\n",
      "FINAL DUPLICATE CHECK\n",
      "==================================================\n",
      "Number of requestids with duplicates: 0\n",
      "Total duplicate rows: 0\n",
      "✅ No remaining duplicates!\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates in df2,they were found in notebook 01\n",
    "# Step 1: Count requestids that have duplicates\n",
    "duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df2['requestid'].duplicated().sum()}\")\n",
    "\n",
    "# Step 2: Drop absolute duplicates (excluding leadid column)\n",
    "comparison_cols = [col for col in df2.columns if col != 'leadid']\n",
    "df2_before = len(df2)\n",
    "df2 = df2.drop_duplicates(subset=comparison_cols, keep='first')\n",
    "df2_after = len(df2)\n",
    "\n",
    "print(f\"\\nDropped {df2_before - df2_after} absolute duplicate rows\")\n",
    "\n",
    "# Step 3: From the remaining duplicate requestids, drop rows where netcontractsigned = 0\n",
    "remaining_duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "\n",
    "if len(remaining_duplicate_requestids) > 0:\n",
    "    print(f\"Remaining duplicate requestids before netcontractsigned cleanup: {len(remaining_duplicate_requestids)}\")\n",
    "    \n",
    "    # Create condition: either NOT a duplicate requestid, OR netcontractsigned != 0\n",
    "    condition = (~df2['requestid'].isin(remaining_duplicate_requestids)) | (df2['netcontractsigned'] != 0)\n",
    "    \n",
    "    df2_before_net_drop = len(df2)\n",
    "    df2 = df2[condition]\n",
    "    df2_after_net_drop = len(df2)\n",
    "    \n",
    "    print(f\"Dropped {df2_before_net_drop - df2_after_net_drop} rows where netcontractsigned = 0 from duplicate requestids\")\n",
    "\n",
    "# Step 4: Final duplicate check\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL DUPLICATE CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Number of requestids with duplicates: {len(final_duplicate_requestids)}\")\n",
    "print(f\"Total duplicate rows: {df2['requestid'].duplicated().sum()}\")\n",
    "\n",
    "if len(final_duplicate_requestids) > 0:\n",
    "    # Check what columns still differ\n",
    "    all_differing_columns = set()\n",
    "    for requestid in final_duplicate_requestids:\n",
    "        group = df2[df2['requestid'] == requestid]\n",
    "        for col in df2.columns:\n",
    "            if col != 'requestid' and group[col].nunique() > 1:\n",
    "                all_differing_columns.add(col)\n",
    "    \n",
    "    print(f\"Differing columns: {sorted(all_differing_columns)}\")\n",
    "else:\n",
    "    print(f\"✅ No remaining duplicates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353766",
   "metadata": {},
   "source": [
    "#### Create Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1690b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing appointment types\n",
    "\n",
    "type_mapping = {\n",
    "    'FU': 'FU',\n",
    "    'FUVD': 'FU', \n",
    "    'REMINDER': 'FU',\n",
    "    'VDFIELD': 'SC',\n",
    "    'VD': 'SC'\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "df['grouped_type'] = df['type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad51c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition for net events (not cancelled and past)\n",
    "df['is_net_event'] = df['deletedat'].isna() & df['is_passed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bbb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross counts (all events)\n",
    "gross_counts = df.groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "gross_counts.columns = [f'gross_{col}' for col in gross_counts.columns]\n",
    "\n",
    "# Net counts (only events that actually happened)\n",
    "net_counts = df[df['is_net_event']].groupby(['requestid', 'grouped_type']).size().unstack(fill_value=0)\n",
    "net_counts.columns = [f'net_{col}' for col in net_counts.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gross and net counts\n",
    "counts_df = gross_counts.join(net_counts, how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b102cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8011647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn self ipa event to bbinary \n",
    "df2['selfipa_done'] = df2['selfipaimportedat'].notnull().astype(int)\n",
    "df2.drop('selfipaimportedat', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert createdat to datetime\n",
    "df['createdat'] = pd.to_datetime(df['createdat'])\n",
    "\n",
    "# Step 2: Get first gross SC and first net FU timestamps\n",
    "first_gross_sc = df[df['grouped_type'] == 'SC'].groupby('requestid')['createdat'].min()\n",
    "first_net_fu = df[(df['grouped_type'] == 'FU') & df['is_net_event']].groupby('requestid')['createdat'].min()\n",
    "\n",
    "# Step 3: Calculate time difference in hours\n",
    "counts_df['time_first_sc_to_first_net_fu'] = (first_net_fu - first_gross_sc).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88cecdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gross_SC\n",
       "1     11241\n",
       "2      2185\n",
       "3       532\n",
       "4       148\n",
       "5        34\n",
       "6         8\n",
       "7         5\n",
       "10        2\n",
       "23        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop older requests that havent had a sc since march\n",
    "counts_df = counts_df[counts_df['gross_SC'] != 0]\n",
    "counts_df['gross_SC'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d420799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset info:\n",
      "Shape: (14156, 5)\n",
      "No missing values: True\n"
     ]
    }
   ],
   "source": [
    "# handle missing value for customers who never made to fu\n",
    "counts_df['time_first_sc_to_first_net_fu'] = counts_df['time_first_sc_to_first_net_fu'].fillna(-1)\n",
    "\n",
    "# Check final dataset\n",
    "print(\"Final dataset info:\")\n",
    "print(f\"Shape: {counts_df.shape}\")\n",
    "print(f\"No missing values: {counts_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negative time differences (not NaN fills): 7\n",
      "Positive time differences: 5964\n",
      "NaN fills (-1): 8185\n",
      "\n",
      "Real negative values range: -905.06 to -23.08 hours\n"
     ]
    }
   ],
   "source": [
    "# problematic entries with negative time difference: \n",
    "# Separate real negative times from NaN fill values\n",
    "real_negative = counts_df[\n",
    "    (counts_df['time_first_sc_to_first_net_fu'] < 0) & \n",
    "    (counts_df['time_first_sc_to_first_net_fu'] != -1)\n",
    "]\n",
    "\n",
    "actual_valid_times = counts_df[counts_df['time_first_sc_to_first_net_fu'] >= 0]\n",
    "\n",
    "print(f\"Actual negative time differences (not NaN fills): {len(real_negative)}\")\n",
    "print(f\"Positive time differences: {len(actual_valid_times)}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "if len(real_negative) > 0:\n",
    "    print(f\"\\nReal negative values range: {real_negative['time_first_sc_to_first_net_fu'].min():.2f} to {real_negative['time_first_sc_to_first_net_fu'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle negative time difference\n",
    "# Remove outliers entirely (set to NaN, then fill with -1)\n",
    "counts_df.loc[counts_df['time_first_sc_to_first_net_fu'] < -1, 'time_first_sc_to_first_net_fu'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec7cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing outliers:\n",
      "Negative values (excluding -1 fills): 0\n",
      "Valid positive time differences: 5964\n",
      "NaN fills (-1): 8192\n",
      "\n",
      "Final dataset shape: (14156, 5)\n",
      "Ready to join with target variable!\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Negative values (excluding -1 fills): {((counts_df['time_first_sc_to_first_net_fu'] < 0) & (counts_df['time_first_sc_to_first_net_fu'] != -1)).sum()}\")\n",
    "print(f\"Valid positive time differences: {(counts_df['time_first_sc_to_first_net_fu'] > 0).sum()}\")\n",
    "print(f\"NaN fills (-1): {(counts_df['time_first_sc_to_first_net_fu'] == -1).sum()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset shape: {counts_df.shape}\")\n",
    "print(\"Ready to join with target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98124e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestid</th>\n",
       "      <th>gross_FU</th>\n",
       "      <th>gross_SC</th>\n",
       "      <th>net_FU</th>\n",
       "      <th>net_SC</th>\n",
       "      <th>time_first_sc_to_first_net_fu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3279</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3729</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.835556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3852</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.193889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4689</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>837.203889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requestid  gross_FU  gross_SC  net_FU  net_SC  \\\n",
       "0       3279         0         1     0.0     1.0   \n",
       "1       3729         8         1     8.0     1.0   \n",
       "2       3852         2         1     2.0     1.0   \n",
       "3       4359         0         1     0.0     0.0   \n",
       "4       4689         4         1     1.0     1.0   \n",
       "\n",
       "   time_first_sc_to_first_net_fu  \n",
       "0                      -1.000000  \n",
       "1                      73.835556  \n",
       "2                      42.193889  \n",
       "3                      -1.000000  \n",
       "4                     837.203889  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at this point requestid is the index. make it explicit\n",
    "\n",
    "\n",
    "counts_df = counts_df.reset_index()\n",
    "counts_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd583cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# requestid duplicates? --> no \n",
    "# Show which requestid values appear more than once\n",
    "duplicate_requestids = counts_df[counts_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba7e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "# also df2 has no duplicates\n",
    "duplicate_requestids = df2[df2['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02d0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (14156, 14)\n",
      "Target variable distribution:\n",
      "netcontractsigned\n",
      "0.0    13812\n",
      "1.0      344\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns: ['requestid', 'gross_FU', 'gross_SC', 'net_FU', 'net_SC', 'time_first_sc_to_first_net_fu', 'zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore', 'netcontractsigned', 'selfipa_done']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_df = counts_df.merge(df2, on='requestid', how='left')\n",
    "\n",
    "\n",
    "# Fill non-matching requestids with 0 for the target\n",
    "final_df['netcontractsigned'] = final_df['netcontractsigned'].fillna(0)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Target variable distribution:\")\n",
    "print(final_df['netcontractsigned'].value_counts())\n",
    "print(f\"\\nColumns: {list(final_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cde1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38cbb0",
   "metadata": {},
   "source": [
    "#### Encoding and Null handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "452d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of bills have -1, 1 and 0\n",
    "\n",
    "# set wrong bill values to nan\n",
    "# Replace specific values with NaN\n",
    "final_df['electricitybill'] = final_df['electricitybill'].replace([0, 1, -1], pd.NA)\n",
    "final_df['heatingbill'] = final_df['heatingbill'].replace([0, 1, -1], pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ea01f",
   "metadata": {},
   "source": [
    "Optional for bill handling \n",
    "def clean_billing_column(series, column_name):\n",
    "    \"\"\"Clean billing column: set negative values and extreme outliers to replacement value\"\"\"\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    series_clean = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Define reasonable bounds (adjust these based on your domain knowledge)\n",
    "    min_reasonable = 0  # Bills shouldn't be negative\n",
    "    max_reasonable = 2000  # Adjust based on your market (e.g., €1000/month seems high)\n",
    "    \n",
    "    # Count issues for reporting\n",
    "    negative_count = (series_clean < min_reasonable).sum()\n",
    "    outlier_count = (series_clean > max_reasonable).sum()\n",
    "    \n",
    "    print(f\"{column_name}:\")\n",
    "    print(f\"  - Negative values: {negative_count}\")\n",
    "    print(f\"  - Values > {max_reasonable}: {outlier_count}\")\n",
    "    print(f\"  - Original NaN: {series.isna().sum()}\")\n",
    "    \n",
    "    # Replace problematic values\n",
    "    series_clean.loc[series_clean < min_reasonable] = np.nan  # or -1\n",
    "    series_clean.loc[series_clean > max_reasonable] = np.nan  # or -1\n",
    "    \n",
    "    return series_clean\n",
    "\n",
    "# Apply cleaning\n",
    "final_df['heatingbill'] = clean_billing_column(final_df['heatingbill'], 'heatingbill')\n",
    "final_df['electricitybill'] = clean_billing_column(final_df['electricitybill'], 'electricitybill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f97be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indicator correlations with target:\n",
      "electricitybill_missing          -0.048352\n",
      "evaluationtime_missing           -0.013618\n",
      "desiredinstallationend_missing   -0.013618\n",
      "mktgparamscore_missing           -0.000055\n",
      "zipregion_missing                 0.015379\n",
      "heatingbill_missing               0.027626\n",
      "Name: netcontractsigned, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# before encoding, mark the missingness inside the row;\n",
    "missing_cols = ['zipregion', 'evaluationtime', 'desiredinstallationend', 'electricitybill', 'heatingbill', 'mktgparamscore']\n",
    "\n",
    "for col in missing_cols:\n",
    "    final_df[f'{col}_missing'] = final_df[col].isnull().astype(int)\n",
    "\n",
    "# Check the predictive power of missing indicators\n",
    "print(\"Missing indicator correlations with target:\")\n",
    "missing_indicators = [f'{col}_missing' for col in missing_cols]\n",
    "missing_corrs = final_df[missing_indicators + ['netcontractsigned']].corr()['netcontractsigned'].drop('netcontractsigned')\n",
    "print(missing_corrs.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b111b4",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2ff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# desiredinstallationend\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-4mesi': 'three_to_four_months', \n",
    "    '5+mesi': 'more_than_5_months',\n",
    "    '1-2mesi': 'one_to_two_months',\n",
    "    'Non lo so': 'dont_know',\n",
    "    'short': np.nan,\n",
    "    # Already mapped values (keep as-is)\n",
    "    'dont_know': 'dont_know',\n",
    "    'three_to_four_months': 'three_to_four_months',\n",
    "    'one_to_two_months': 'one_to_two_months', \n",
    "    'more_than_5_months': 'more_than_5_months',\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "final_df['desiredinstallationend1'] = final_df['desiredinstallationend'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1889c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluationtime\n",
    "type_mapping = {\n",
    "    # Original Italian values\n",
    "    '3-6 mesi': np.nan,\n",
    "    '<3 mesi': np.nan,\n",
    "    '>6 mesi': np.nan,\n",
    "    # Already mapped English values\n",
    "    'less_than_three_months': np.nan,\n",
    "    'more_than_six_months': np.nan,\n",
    "    # Other values that appear in your data\n",
    "    'understand_need': 'understand_need',  # or map to np.nan if you don't want these\n",
    "    'understand_purchase': 'understand_purchase',  # or map to np.nan if you don't want these\n",
    "    'evaluation': 'evaluation',  # or map to np.nan if you don't want these\n",
    "    'curious': 'curious',  # or map to np.nan if you don't want these\n",
    "    # Handle string 'nan'\n",
    "    'nan': np.nan\n",
    "}\n",
    "\n",
    "# Create grouped_type column\n",
    "final_df['evaluationtime1'] = final_df['evaluationtime'].map(type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09d77850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the old columns directly\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend1']\n",
    "final_df['evaluationtime'] = final_df['evaluationtime1']\n",
    "\n",
    "# Drop the temporary columns\n",
    "final_df = final_df.drop(['desiredinstallationend1', 'evaluationtime1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b24a7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan before encoding\n",
    "final_df['desiredinstallationend'] = final_df['desiredinstallationend'].fillna('missing')\n",
    "final_df['evaluationtime'] = final_df['evaluationtime'].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c19a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with ordinal encoding for time-based features\n",
    "\n",
    "# Ordinal encoding for desiredinstallationend (time-based order)\n",
    "time_order = [ 'one_to_two_months', 'three_to_four_months', 'more_than_5_months', 'dont_know', 'missing']\n",
    "final_df['desiredinstallationend_encoded'] = final_df['desiredinstallationend'].map({val: i for i, val in enumerate(time_order)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78461e67",
   "metadata": {},
   "source": [
    "##### Analysis to see how to handle marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f7fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14156 entries, 0 to 14155\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       14156 non-null  int64  \n",
      " 1   gross_FU                        14156 non-null  int64  \n",
      " 2   gross_SC                        14156 non-null  int64  \n",
      " 3   net_FU                          14156 non-null  float64\n",
      " 4   net_SC                          14156 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   14156 non-null  float64\n",
      " 6   zipregion                       13700 non-null  object \n",
      " 7   evaluationtime                  14156 non-null  object \n",
      " 8   desiredinstallationend          14156 non-null  object \n",
      " 9   electricitybill                 9026 non-null   object \n",
      " 10  heatingbill                     3080 non-null   object \n",
      " 11  mktgparamscore                  13373 non-null  object \n",
      " 12  netcontractsigned               14156 non-null  float64\n",
      " 13  selfipa_done                    14156 non-null  int64  \n",
      " 14  zipregion_missing               14156 non-null  int64  \n",
      " 15  evaluationtime_missing          14156 non-null  int64  \n",
      " 16  desiredinstallationend_missing  14156 non-null  int64  \n",
      " 17  electricitybill_missing         14156 non-null  int64  \n",
      " 18  heatingbill_missing             14156 non-null  int64  \n",
      " 19  mktgparamscore_missing          14156 non-null  int64  \n",
      " 20  desiredinstallationend_encoded  14156 non-null  int64  \n",
      "dtypes: float64(4), int64(11), object(6)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d73e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rates by category:\n",
      "\n",
      "\n",
      "ZIPREGION:\n",
      "                       total_samples  conversions  conversion_rate\n",
      "zipregion                                                         \n",
      "Friuli-Venezia Giulia            776         55.0         0.070876\n",
      "Piemonte                        1179         48.0         0.040712\n",
      "Lombardia                       2152         64.0         0.029740\n",
      "Liguria                          349         10.0         0.028653\n",
      "Veneto                          1105         28.0         0.025339\n",
      "Emilia-Romagna                  1212         30.0         0.024752\n",
      "Marche                           260          6.0         0.023077\n",
      "Toscana                          919         20.0         0.021763\n",
      "Valle D'Aosta                     47          1.0         0.021277\n",
      "Lazio                           1296         25.0         0.019290\n",
      "Trentino-Alto Adige              106          2.0         0.018868\n",
      "Basilicata                       113          2.0         0.017699\n",
      "Umbria                           193          3.0         0.015544\n",
      "Campania                         520          6.0         0.011538\n",
      "Molise                            88          1.0         0.011364\n",
      "Sardegna                         457          5.0         0.010941\n",
      "Sicilia                          903          9.0         0.009967\n",
      "Puglia                          1164          8.0         0.006873\n",
      "Abruzzo                          313          2.0         0.006390\n",
      "Calabria                         548          2.0         0.003650\n",
      "Overall variation: 0.0149\n",
      "\n",
      "MKTGPARAMSCORE:\n",
      "                total_samples  conversions  conversion_rate\n",
      "mktgparamscore                                             \n",
      "Referral                   34          7.0         0.205882\n",
      "form_classico              14          1.0         0.071429\n",
      "Organic                  1146         67.0         0.058464\n",
      "Affiliation                64          3.0         0.046875\n",
      "Other                     470         17.0         0.036170\n",
      "Mediago                   266          9.0         0.033835\n",
      "Google                   1929         52.0         0.026957\n",
      "Youtube                  1688         43.0         0.025474\n",
      "Outbrain                 2374         44.0         0.018534\n",
      "TikTok                   1091         20.0         0.018332\n",
      "Meta                     4181         62.0         0.014829\n",
      "Taboola                   114          0.0         0.000000\n",
      "d2d                         2          0.0         0.000000\n",
      "Overall variation: 0.0532\n"
     ]
    }
   ],
   "source": [
    "# For each categorical column, see conversion rates\n",
    "print(\"Conversion rates by category:\\n\")\n",
    "\n",
    "for col in ['zipregion', 'mktgparamscore']:  # replace with your actual column names\n",
    "    conversion_by_cat = final_df.groupby(col)['netcontractsigned'].agg(['count', 'sum', 'mean'])\n",
    "    conversion_by_cat.columns = ['total_samples', 'conversions', 'conversion_rate']\n",
    "    conversion_by_cat = conversion_by_cat.sort_values('conversion_rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(conversion_by_cat)\n",
    "    print(f\"Overall variation: {conversion_by_cat['conversion_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf1bfd",
   "metadata": {},
   "source": [
    "Marketing gave good variance, would like to maintain that information. Region is not that significant but still good. \n",
    "However given the * of unique values, in both columns I will opt for grouping instead of each value having its column. \n",
    "\n",
    "Instead of 30+ categorical features, you get ~6-8, keeping the predictive power but losing the noise.\n",
    "\n",
    "Downside; this grouping should occasionally double checked to see if it still makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163be87",
   "metadata": {},
   "source": [
    "##### one hot encoding for marketing and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb8c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance groups\n",
    "marketing_groups = {\n",
    "    'High': ['Referral', 'form_classico', 'Organic', 'Affiliation'],\n",
    "    'Medium': ['Other', 'Google', 'Youtube', 'Mediago', 'TikTok'], \n",
    "    'Low': ['Outbrain', 'Meta', 'Taboola', 'd2d']\n",
    "}\n",
    "\n",
    "# Create mapping function\n",
    "def group_marketing(value):\n",
    "    for group, channels in marketing_groups.items():\n",
    "        if value in channels:\n",
    "            return group\n",
    "    return 'Low'  # fallback\n",
    "\n",
    "# Apply grouping\n",
    "final_df['mktg_grouped'] = final_df['mktgparamscore'].apply(group_marketing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9995467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group regions by performance\n",
    "def group_regions(region):\n",
    "    if region == 'Friuli-Venezia Giulia':\n",
    "        return 'High_Performer'\n",
    "    elif region in ['Piemonte', 'Lombardia', 'Emilia-Romagna', 'Veneto']:\n",
    "        return 'Large_Good'\n",
    "    elif region in ['Liguria', 'Toscana', 'Valle D\\'Aosta']:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "final_df['region_grouped'] = final_df['zipregion'].apply(group_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c5957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the grouped categories\n",
    "final_df = pd.get_dummies(final_df, columns=['mktg_grouped', 'region_grouped'], prefix=['mktg', 'region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be7d7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777490e3",
   "metadata": {},
   "source": [
    "df is the log of all calendar_events so a direct join will make our dataset exponentially duplicated.\n",
    "\n",
    "I will first get the unique requestid --> leadid cpairs and then merge that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "937eadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "requestid_leadid_pairs = df[['requestid', 'leadid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4c8ef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requestid_leadid_pairs[requestid_leadid_pairs['requestid'].duplicated(keep=False)]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c10a614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also add leadid to the final df for merging with behaviour data\n",
    "\n",
    "# Suppose the column you want from df1 is called \"col_from_df1\"\n",
    "final_df = final_df.merge(\n",
    "    requestid_leadid_pairs[['requestid', 'leadid']],  # keep only requestid + desired column\n",
    "    on='requestid',                      # join key\n",
    "    how='left'                           # keep all rows from df2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1713e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = final_df[final_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73b32aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any requestid with no leadid?\n",
    "final_df[final_df['leadid'].isnull()]['requestid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18644c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No missing values: False\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns and create final feature matrix\n",
    "columns_to_drop = [ 'zipregion', 'evaluationtime', 'desiredinstallationend', 'mktgparamscore']\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "print(f\"\\nNo missing values: {final_df.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215f64f",
   "metadata": {},
   "source": [
    "#### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16e8505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 0\n",
      "Dataset ready: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_20316\\1109642794.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
      "C:\\Users\\ElifYilmaz\\AppData\\Local\\Temp\\ipykernel_20316\\1109642794.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n"
     ]
    }
   ],
   "source": [
    "# Fix electricitybill missing values\n",
    "final_df['electricitybill'] = final_df['electricitybill'].fillna(final_df['electricitybill'].median())\n",
    "final_df['heatingbill'] = final_df['heatingbill'].fillna(final_df['heatingbill'].median())\n",
    "\n",
    "# Fix desiredinstallationend_encoded - NaN means 'Unknown' which should be 0 --> check if needed\n",
    "#X_final['desiredinstallationend_encoded'] = X_final['desiredinstallationend_encoded'].fillna(0)\n",
    "\n",
    "# Verify all missing values are gone\n",
    "print(f\"Remaining missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Dataset ready: {final_df.isnull().sum().sum() == 0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bc1b",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5df80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling when needed\n",
    "continuous_cols= [\n",
    "    'time_first_sc_to_first_net_fu',\n",
    "    'electricitybill', \n",
    "    'heatingbill'\n",
    "]\n",
    "\n",
    "# Scale only the continuous features\n",
    "scaler = StandardScaler()\n",
    "final_df[continuous_cols] = scaler.fit_transform(final_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1189176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(r\"processed_data/df_model.csv\") # manualy saved leadtime info. hafter having gotten modified in older attempts/notebook\n",
    "behaviour = behaviour.drop(['converted', 'lead_to_sc1_days', 'sc1_schedule_to_appointment_days', 'showed_up_sc1' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca844c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38367 entries, 0 to 38366\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          38367 non-null  int64  \n",
      " 1   total_bc_attempts           38367 non-null  int64  \n",
      " 2   total_bc_outcomes           38367 non-null  int64  \n",
      " 3   lead_to_first_bc_days       38367 non-null  float64\n",
      " 4   bc_duration_days            38367 non-null  float64\n",
      " 5   bc_frequency                38367 non-null  float64\n",
      " 6   positive_outcomes_count     38367 non-null  int64  \n",
      " 7   negative_outcomes_count     38367 non-null  int64  \n",
      " 8   noshow_outcomes_count       38367 non-null  int64  \n",
      " 9   positive_outcome_ratio      38367 non-null  float64\n",
      " 10  negative_outcome_ratio      38367 non-null  float64\n",
      " 11  noshow_outcome_ratio        38367 non-null  float64\n",
      " 12  reachability_score          38367 non-null  float64\n",
      " 13  outcome_trend               38367 non-null  int64  \n",
      " 14  persistence_after_negative  38367 non-null  int64  \n",
      " 15  engagement_score            38367 non-null  float64\n",
      " 16  efficiency_score            38367 non-null  float64\n",
      " 17  last_bc_outcome_encoded     38367 non-null  int64  \n",
      " 18  first_bc_outcome_encoded    38367 non-null  int64  \n",
      "dtypes: float64(9), int64(10)\n",
      "memory usage: 5.6 MB\n"
     ]
    }
   ],
   "source": [
    "behaviour.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c19f5",
   "metadata": {},
   "source": [
    "Adding behavioral data from Booking Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7cb496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour.columns = behaviour.columns.str.lower()\n",
    "merged_df = final_df.merge(behaviour, \n",
    "                          left_on='leadid', \n",
    "                          right_on='id', \n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc1efb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate requestid values: []\n"
     ]
    }
   ],
   "source": [
    "duplicate_requestids = merged_df[merged_df['requestid'].duplicated(keep=False)]['requestid'].unique()\n",
    "print(f\"Duplicate requestid values: {duplicate_requestids}\")\n",
    "\n",
    "# --> loooks ok "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da2ee9",
   "metadata": {},
   "source": [
    "#### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a919aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column pairs: []\n",
      "Original shape: (14156, 44)\n",
      "After removing duplicates: (14156, 44)\n"
     ]
    }
   ],
   "source": [
    "# Check your actual dataset for duplicates\n",
    "duplicate_cols = []\n",
    "cols = merged_df.columns.tolist()\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if merged_df.equals(merged_df[cols[j]]):\n",
    "            duplicate_cols.append((cols[i], cols[j]))\n",
    "\n",
    "print(\"Duplicate column pairs:\", duplicate_cols)\n",
    "print(f\"Original shape: {merged_df.shape}\")\n",
    "\n",
    "# Remove duplicates if any found\n",
    "X_final_clean = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "print(f\"After removing duplicates: {X_final_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "947b90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame sizes:\n",
      "final_df: 14156 rows\n",
      "behaviour: 38367 rows\n",
      "merged_df: 14156 rows\n",
      "\n",
      "==================================================\n",
      "Rows with matches: 13296 (93.9%)\n",
      "Rows without matches: 860 (6.1%)\n",
      "\n",
      "==================================================\n",
      "Checking for duplicates in join keys:\n",
      "Duplicates in final_df['leadid']: 3225\n",
      "Duplicates in behaviour['id']: 0\n",
      "\n",
      "==================================================\n",
      "Unique leadids in final_df: 10931\n",
      "Unique ids in behaviour: 38367\n",
      "Common IDs: 10240\n",
      "IDs only in final_df: 691\n",
      "IDs only in behaviour: 28127\n"
     ]
    }
   ],
   "source": [
    "# Check the original sizes\n",
    "print(\"Original DataFrame sizes:\")\n",
    "print(f\"final_df: {len(final_df)} rows\")\n",
    "print(f\"behaviour: {len(behaviour)} rows\")\n",
    "print(f\"merged_df: {len(merged_df)} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check how many matches we got\n",
    "matches = merged_df['id'].notna().sum()\n",
    "no_matches = merged_df['id'].isna().sum()\n",
    "\n",
    "print(f\"Rows with matches: {matches} ({matches/len(merged_df)*100:.1f}%)\")\n",
    "print(f\"Rows without matches: {no_matches} ({no_matches/len(merged_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for duplicates in the key columns before merge\n",
    "print(\"Checking for duplicates in join keys:\")\n",
    "print(f\"Duplicates in final_df['leadid']: {final_df['leadid'].duplicated().sum()}\")\n",
    "print(f\"Duplicates in behaviour['id']: {behaviour['id'].duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check the overlap between the two key columns\n",
    "final_leadids = set(final_df['leadid'])\n",
    "behaviour_ids = set(behaviour['id'])\n",
    "\n",
    "print(f\"Unique leadids in final_df: {len(final_leadids)}\")\n",
    "print(f\"Unique ids in behaviour: {len(behaviour_ids)}\")\n",
    "print(f\"Common IDs: {len(final_leadids & behaviour_ids)}\")\n",
    "print(f\"IDs only in final_df: {len(final_leadids - behaviour_ids)}\")\n",
    "print(f\"IDs only in behaviour: {len(behaviour_ids - final_leadids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab8a4935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of 860 unmatched rows:\n",
      "==================================================\n",
      "Unmatched entries conversion rate: 0.008 (0.8%)\n",
      "Total conversions among unmatched: 7.0\n",
      "\n",
      "==================================================\n",
      "Matched entries conversion rate: 0.025 (2.5%)\n",
      "Total conversions among matched: 337.0\n",
      "\n",
      "==================================================\n",
      "COMPARISON:\n",
      "Unmatched: 0.008 (0.8%)\n",
      "Matched:   0.025 (2.5%)\n",
      "Difference: 1.7 percentage points\n",
      "\n",
      "Statistical test p-value: 0.002201\n",
      "Significant difference: Yes\n"
     ]
    }
   ],
   "source": [
    "# checking if they are converters or not\n",
    "# Get the rows that didn't match (have NaN in the 'id' column from behaviour)\n",
    "unmatched_rows = merged_df[merged_df['id'].isna()]\n",
    "\n",
    "print(f\"Analysis of {len(unmatched_rows)} unmatched rows:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check conversion rate for unmatched entries\n",
    "conversion_rate_unmatched = unmatched_rows['netcontractsigned'].mean()\n",
    "total_conversions_unmatched = unmatched_rows['netcontractsigned'].sum()\n",
    "\n",
    "print(f\"Unmatched entries conversion rate: {conversion_rate_unmatched:.3f} ({conversion_rate_unmatched*100:.1f}%)\")\n",
    "print(f\"Total conversions among unmatched: {total_conversions_unmatched}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Compare with matched entries\n",
    "matched_rows = merged_df[merged_df['id'].notna()]\n",
    "conversion_rate_matched = matched_rows['netcontractsigned'].mean()\n",
    "total_conversions_matched = matched_rows['netcontractsigned'].sum()\n",
    "\n",
    "print(f\"Matched entries conversion rate: {conversion_rate_matched:.3f} ({conversion_rate_matched*100:.1f}%)\")\n",
    "print(f\"Total conversions among matched: {total_conversions_matched}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Overall comparison\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"Unmatched: {conversion_rate_unmatched:.3f} ({conversion_rate_unmatched*100:.1f}%)\")\n",
    "print(f\"Matched:   {conversion_rate_matched:.3f} ({conversion_rate_matched*100:.1f}%)\")\n",
    "print(f\"Difference: {(conversion_rate_matched - conversion_rate_unmatched)*100:.1f} percentage points\")\n",
    "\n",
    "# Statistical significance test (optional)\n",
    "from scipy import stats\n",
    "if len(unmatched_rows) > 0 and len(matched_rows) > 0:\n",
    "    stat, p_value = stats.chi2_contingency([[\n",
    "        unmatched_rows['netcontractsigned'].sum(), \n",
    "        len(unmatched_rows) - unmatched_rows['netcontractsigned'].sum()\n",
    "    ], [\n",
    "        matched_rows['netcontractsigned'].sum(), \n",
    "        len(matched_rows) - matched_rows['netcontractsigned'].sum()\n",
    "    ]])[:2]\n",
    "    \n",
    "    print(f\"\\nStatistical test p-value: {p_value:.6f}\")\n",
    "    print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86b2c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14156 entries, 0 to 14155\n",
      "Data columns (total 44 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   requestid                       14156 non-null  int64  \n",
      " 1   gross_FU                        14156 non-null  int64  \n",
      " 2   gross_SC                        14156 non-null  int64  \n",
      " 3   net_FU                          14156 non-null  float64\n",
      " 4   net_SC                          14156 non-null  float64\n",
      " 5   time_first_sc_to_first_net_fu   14156 non-null  float64\n",
      " 6   electricitybill                 14156 non-null  float64\n",
      " 7   heatingbill                     14156 non-null  float64\n",
      " 8   netcontractsigned               14156 non-null  float64\n",
      " 9   selfipa_done                    14156 non-null  int64  \n",
      " 10  zipregion_missing               14156 non-null  int64  \n",
      " 11  evaluationtime_missing          14156 non-null  int64  \n",
      " 12  desiredinstallationend_missing  14156 non-null  int64  \n",
      " 13  electricitybill_missing         14156 non-null  int64  \n",
      " 14  heatingbill_missing             14156 non-null  int64  \n",
      " 15  mktgparamscore_missing          14156 non-null  int64  \n",
      " 16  desiredinstallationend_encoded  14156 non-null  int64  \n",
      " 17  mktg_High                       14156 non-null  bool   \n",
      " 18  mktg_Low                        14156 non-null  bool   \n",
      " 19  mktg_Medium                     14156 non-null  bool   \n",
      " 20  region_High_Performer           14156 non-null  bool   \n",
      " 21  region_Large_Good               14156 non-null  bool   \n",
      " 22  region_Medium                   14156 non-null  bool   \n",
      " 23  region_Other                    14156 non-null  bool   \n",
      " 24  leadid                          14156 non-null  int64  \n",
      " 25  id                              13296 non-null  float64\n",
      " 26  total_bc_attempts               13296 non-null  float64\n",
      " 27  total_bc_outcomes               13296 non-null  float64\n",
      " 28  lead_to_first_bc_days           13296 non-null  float64\n",
      " 29  bc_duration_days                13296 non-null  float64\n",
      " 30  bc_frequency                    13296 non-null  float64\n",
      " 31  positive_outcomes_count         13296 non-null  float64\n",
      " 32  negative_outcomes_count         13296 non-null  float64\n",
      " 33  noshow_outcomes_count           13296 non-null  float64\n",
      " 34  positive_outcome_ratio          13296 non-null  float64\n",
      " 35  negative_outcome_ratio          13296 non-null  float64\n",
      " 36  noshow_outcome_ratio            13296 non-null  float64\n",
      " 37  reachability_score              13296 non-null  float64\n",
      " 38  outcome_trend                   13296 non-null  float64\n",
      " 39  persistence_after_negative      13296 non-null  float64\n",
      " 40  engagement_score                13296 non-null  float64\n",
      " 41  efficiency_score                13296 non-null  float64\n",
      " 42  last_bc_outcome_encoded         13296 non-null  float64\n",
      " 43  first_bc_outcome_encoded        13296 non-null  float64\n",
      "dtypes: bool(7), float64(25), int64(12)\n",
      "memory usage: 4.1 MB\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM ONE \n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f607eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original unmatched rows: 860\n",
      "Conversions among unmatched: 7.0\n",
      "Non-conversions among unmatched: 853\n",
      "\n",
      "After filtering unmatched rows:\n",
      "Keeping only unmatched conversions: 7\n",
      "\n",
      "Final merged_df:\n",
      "Matched rows (with behavior): 13296\n",
      "Unmatched conversions (without behavior): 7\n",
      "Total rows: 13303\n",
      "All remaining unmatched rows are conversions: True\n"
     ]
    }
   ],
   "source": [
    "# drop non converters with no behavioral data. keep the 7 that converted and recover their data manually\n",
    "# Get the unmatched rows (the 860 entries without behavior data)\n",
    "# Get the unmatched rows (the 860 entries without behavior data)\n",
    "unmatched_rows = merged_df[merged_df['id'].isna()]\n",
    "\n",
    "print(f\"Original unmatched rows: {len(unmatched_rows)}\")\n",
    "print(f\"Conversions among unmatched: {unmatched_rows['netcontractsigned'].sum()}\")\n",
    "print(f\"Non-conversions among unmatched: {(unmatched_rows['netcontractsigned'] == 0).sum()}\")\n",
    "\n",
    "# Filter unmatched rows to keep only conversions (netcontractsigned == 1)\n",
    "unmatched_conversions = unmatched_rows[unmatched_rows['netcontractsigned'] == 1]\n",
    "\n",
    "print(f\"\\nAfter filtering unmatched rows:\")\n",
    "print(f\"Keeping only unmatched conversions: {len(unmatched_conversions)}\")\n",
    "\n",
    "# Get the matched rows (the 13,296 entries with behavior data)\n",
    "matched_rows = merged_df[merged_df['id'].notna()]\n",
    "\n",
    "# Combine and replace merged_df\n",
    "merged_df = pd.concat([matched_rows, unmatched_conversions], ignore_index=True)\n",
    "\n",
    "print(f\"\\nFinal merged_df:\")\n",
    "print(f\"Matched rows (with behavior): {len(matched_rows)}\")\n",
    "print(f\"Unmatched conversions (without behavior): {len(unmatched_conversions)}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "print(f\"All remaining unmatched rows are conversions: {(merged_df[merged_df['id'].isna()]['netcontractsigned'] == 1).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8d6121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5        106886\n",
       "7        106888\n",
       "20       106902\n",
       "21       106903\n",
       "23       106905\n",
       "          ...  \n",
       "13284    122728\n",
       "13285    122729\n",
       "13289    122733\n",
       "13290    122734\n",
       "13291    122735\n",
       "Name: requestid, Length: 5517, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM THREE\n",
    "# see leads that didnt make it to sc --> le\n",
    "merged_df.loc[merged_df['net_SC'] == 0, 'requestid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7518fbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5186     113414\n",
       "8975     117775\n",
       "9454     118332\n",
       "13017    122408\n",
       "13083    122485\n",
       "Name: requestid, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any of them have contract signed?  yes but ofc after the cutoff time of the dataset\n",
    "merged_df.loc[\n",
    "    (merged_df['net_SC'] == 0) & (merged_df['netcontractsigned'] == 1),\n",
    "    'requestid'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc568ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop the ones who havent made to net sc\n",
    "# Keep rows where net_SC is NOT equal to 0\n",
    "merged_df = merged_df[merged_df['net_SC'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e5749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop requestid ad other indicatiors\n",
    "merged_df = merged_df.drop(columns=['requestid', 'id', 'leadid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64a85535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversions in missing rows: 7.0\n",
      "Conversion rate in missing rows: 1.0000\n",
      "Conversion rate in complete rows: 0.0427\n"
     ]
    }
   ],
   "source": [
    "# Check conversions in missing call data rows\n",
    "missing_mask = merged_df['total_bc_attempts'].isnull()\n",
    "print(f\"Conversions in missing rows: {merged_df[missing_mask]['netcontractsigned'].sum()}\")\n",
    "print(f\"Conversion rate in missing rows: {merged_df[missing_mask]['netcontractsigned'].mean():.4f}\")\n",
    "print(f\"Conversion rate in complete rows: {merged_df[~missing_mask]['netcontractsigned'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0fc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of these requests are very old and dont have propre registstration, some have some registration but essentially there are only 7 contract signed insode 860, so essentially dropping as a first step of undersampling.\n",
    "# ill keep 7 positives and recover their data, drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c84629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (7786, 41)\n",
      "New conversion rate: 0.0435\n",
      "Total conversions kept: 339.0\n"
     ]
    }
   ],
   "source": [
    "# this would drop all entries with any missing values, so lets hold our horses here. \n",
    "#merged_df = merged_df.dropna().copy()\n",
    "print(f\"New dataset shape: {merged_df.shape}\")\n",
    "print(f\"New conversion rate: {merged_df['netcontractsigned'].mean():.4f}\")\n",
    "print(f\"Total conversions kept: {merged_df['netcontractsigned'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4526a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create subfolder if it doesn't exist\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# Save your merged dataframe\n",
    "merged_df.to_csv('processed_data/merged_df.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
